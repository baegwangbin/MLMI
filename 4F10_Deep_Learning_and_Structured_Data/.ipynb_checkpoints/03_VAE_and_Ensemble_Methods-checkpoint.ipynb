{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Variational Auto Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Latent Variable Models\n",
    "\n",
    "* **Discrete LV**\n",
    "\n",
    ">$$p(\\mathbf{x}) = \\sum^M_{m=1} P(c_m) p(\\mathbf{x}|c_m)$$\n",
    "\n",
    "* **Continuous LV**\n",
    "\n",
    ">$$p(\\mathbf{x}) = \\int p(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z})d\\mathbf{z}$$\n",
    "\n",
    ">* $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z};\\mathbf{0},\\mathbf{I})$\n",
    ">* $p(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mathbf{x};\\mathbf{f}(\\mathbf{z}),\\sigma^2 \\mathbf{I})$\n",
    "\n",
    "* **Factor Analysis**, $\\mathbf{f}(\\mathbf{z}) = \\mathbf{Az}$\n",
    "\n",
    ">* **Auxiliary function:**\n",
    "\n",
    ">$$\\mathcal{Q}(\\boldsymbol{\\lambda}, \\tilde{\\boldsymbol{\\lambda}}) = \\int p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda}) \\log (p(\\mathbf{x}|\\mathbf{z};\\boldsymbol{\\lambda})) d\\mathbf{z}$$\n",
    "\n",
    ">* **General mapping using NN**\n",
    "\n",
    ">$$p(\\mathbf{x}|\\mathbf{z};\\boldsymbol{\\lambda}) = \\mathcal{N}(\\mathbf{x};\\mathbf{f}(\\mathbf{z};\\boldsymbol{\\lambda}),\\sigma^2 \\mathbf{I})$$\n",
    "\n",
    ">* No simple closed-form solution $\\rightarrow$ adopt **variational** approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. KL Divergence\n",
    "\n",
    "* **Definition**\n",
    "\n",
    ">$$\\mathcal{KL}(p(\\mathbf{x})||q(\\mathbf{x})) = \\int p(\\mathbf{x}) \\log \\left( \\frac{p(\\mathbf{x})}{q(\\mathbf{x})} \\right) d\\mathbf{x} = - \\int p(\\mathbf{x}) \\log \\left( \\frac{q(\\mathbf{x})}{p(\\mathbf{x})} \\right) d\\mathbf{x}$$\n",
    "\n",
    "* **Inequality** (use $\\log y \\leq y-1$)\n",
    "\n",
    ">$$\\int p(\\mathbf{x}) \\left( \\frac{q(\\mathbf{x})}{p(\\mathbf{x})} \\right) d\\mathbf{x} \\leq\n",
    "\\int p(\\mathbf{x}) \\left( \\frac{q(\\mathbf{x})}{p(\\mathbf{x})} - 1 \\right) d\\mathbf{x} = 0$$\n",
    "\n",
    "* **KL Divergence for Gaussians**\n",
    "\n",
    ">$$\\mathcal{KL}(p(\\mathbf{x})||q(\\mathbf{x})) = \\frac{1}{2} \\left( \\text{tr} (\\boldsymbol{\\Sigma}^{-1}_2 \\boldsymbol{\\Sigma}_1 - \\mathbf{I}) + (\\mu_1 - \\mu_2)^T \\boldsymbol{\\Sigma}^{-1}_2 (\\mu_1 - \\mu_2) + \\log \\left( \\frac{|\\boldsymbol{\\Sigma}_2|}{|\\boldsymbol{\\Sigma}_1|} \\right) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Variational EM\n",
    "\n",
    "* **EM: Auxiliary Function Maximization**\n",
    "\n",
    ">$$\\boldsymbol{\\lambda}^{(k+1)} = \\text{argmax}_{\\boldsymbol{\\lambda}} \\left( \\mathcal{Q}(\\boldsymbol{\\lambda}^{(k)},\\boldsymbol{\\lambda}) \\right) = \\text{argmax}_{\\boldsymbol{\\lambda}} \\left( \\int p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda}^{(k)}) \\log (p(\\mathbf{x},\\mathbf{z};\\boldsymbol{\\lambda})) d\\mathbf{z} \\right)$$\n",
    "\n",
    ">$=$ **expected value of the log joint distribution**\n",
    "\n",
    ">\\begin{align}\n",
    "\\log (p(\\mathbf{x},\\mathbf{z};\\boldsymbol{\\lambda})) &= \\log (p(\\mathbf{x}|\\mathbf{z};\\boldsymbol{\\lambda})) + \\log (p(\\mathbf{z})) \\\\\n",
    "&= \\log (\\mathcal{N} (\\mathbf{x}; \\mathbf{Az}, \\boldsymbol{\\Sigma}_{\\text{diag}})) + \\log (\\mathcal{N}(\\mathbf{z};\\mathbf{0},\\mathbf{I})) \\\\\n",
    "&= -\\frac{1}{2} (\\mathbf{z}^T \\mathbf{A}^T \\boldsymbol{\\Sigma}^{-1}_{\\text{diag}} \\mathbf{Az} - 2\\mathbf{z}^T \\mathbf{A}^T \\boldsymbol{\\Sigma}^{-1}_{\\text{diag}} \\mathbf{x} + \\mathbf{x}^T \\boldsymbol{\\Sigma}^{-1}_{\\text{diag}} \\mathbf{x}) - \\frac{1}{2} \\log (|\\boldsymbol{\\Sigma}_{\\text{diag}}|) + C\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "* **Estimate $\\boldsymbol{\\lambda}$** by maximizing the **Log-likelihood** $\\mathcal{L}(\\boldsymbol{\\lambda})$\n",
    "\n",
    ">\\begin{align}\n",
    "\\mathcal{L}(\\boldsymbol{\\lambda}) &= \\log (p(\\mathbf{x};\\boldsymbol{\\lambda})) \\\\\n",
    "&= \\int p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda}) \\log (p(\\mathbf{x};\\boldsymbol{\\lambda})) d\\mathbf{z} \\\\\n",
    "&= \\int p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda}) \\log \\left( \\frac{p(\\mathbf{x};\\boldsymbol{\\lambda})p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda})}{p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda})} \\right) d\\mathbf{z} \\\\\n",
    "&= \\bigg\\langle \\log \\left( \\frac{p(\\mathbf{x},\\mathbf{z};\\boldsymbol{\\lambda})}{p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda})} \\right) \\bigg\\rangle_{p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda})}\n",
    "\\end{align}\n",
    "\n",
    ">* **Need to know** $p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda})$ $\\rightarrow$ use any valid distribution $q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})$\n",
    "\n",
    ">\\begin{align}\n",
    "\\mathcal{L}(\\boldsymbol{\\lambda}) &= \\bigg\\langle \\log \\left( \\frac{p(\\mathbf{x},\\mathbf{z};\\boldsymbol{\\lambda})}{p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda})} \\right) \\bigg\\rangle_{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})} \\\\\n",
    "&= \\bigg\\langle \\log \\left( \\frac{p(\\mathbf{x},\\mathbf{z};\\boldsymbol{\\lambda})}{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})} \\right) \\bigg\\rangle_{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})}\n",
    "+ \\bigg\\langle \\log \\left( \\frac{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})}{p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda})} \\right) \\bigg\\rangle_{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})} \\\\\n",
    "&\\geq \\bigg\\langle \\log \\left( \\frac{p(\\mathbf{x},\\mathbf{z};\\boldsymbol{\\lambda})}{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})} \\right) \\bigg\\rangle_{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})}\n",
    "=\\mathcal{F} (q(\\mathbf{z};\\boldsymbol{\\lambda}),\\boldsymbol{\\lambda})\n",
    "\\end{align}\n",
    "\n",
    "* **EM Revisited**\n",
    "\n",
    ">* **Set** $q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k)}) = p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda}^{(k)})$\n",
    "\n",
    ">$$\\mathcal{L}(\\boldsymbol{\\lambda}^{(k)}) = \\mathcal{F} (q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k)}),\\boldsymbol{\\lambda}^{(k)})$$\n",
    "\n",
    ">* **Maximize $\\mathcal{F} (q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k)}),\\boldsymbol{\\lambda})$ to find $\\boldsymbol{\\lambda}^{(k+1)}$**\n",
    "\n",
    ">$$\\boldsymbol{\\lambda}^{(k+1)} = \\text{argmax}_{\\boldsymbol{\\lambda}} \\Bigg\\{ \\bigg\\langle \\log \\left( \\frac{p(\\mathbf{x},\\mathbf{z};\\boldsymbol{\\lambda})}{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k)})} \\right) \\bigg\\rangle_{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k)})} \\Bigg\\}$$\n",
    "\n",
    ">* **Minimize KL divergence for variational approximation**\n",
    "\n",
    ">$$q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k+1)}) = \\text{argmin}_{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})} \n",
    "\\Bigg\\{ \\bigg\\langle\n",
    "\\log \\left(\n",
    "\\frac{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})}{p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda}^{(k+1)})}\n",
    "\\right)\n",
    "\\bigg\\rangle_{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})}\\Bigg\\}$$\n",
    "\n",
    ">* **Which occurs at $q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k+1)}) = p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda}^{(k+1)})$**\n",
    "\n",
    "* **EM Guarantees:**\n",
    "\n",
    ">$$\\mathcal{L}(\\boldsymbol{\\lambda}^{(k)}) = \\mathcal{F} \\left( q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k)}), \\boldsymbol{\\lambda}^{(k)} \\right)\n",
    "\\leq \\mathcal{F} \\left( q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k)}), \\boldsymbol{\\lambda}^{(k+1)} \\right)\n",
    "\\leq \\mathcal{L}(\\boldsymbol{\\lambda}^{(k+1)})$$\n",
    "\n",
    ">* provided $q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k)}) = p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda}^{(k)})$\n",
    "\n",
    "\n",
    "* **Variational EM** (no guarantees)\n",
    "\n",
    ">$$\\mathcal{L}(\\boldsymbol{\\lambda}^{(k)}) \\geq \\mathcal{F} \\left( q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k)}), \\boldsymbol{\\lambda}^{(k)} \\right)\n",
    "\\leq \\mathcal{F} \\left( q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k)}), \\boldsymbol{\\lambda}^{(k+1)} \\right)\n",
    "\\leq \\mathcal{L}(\\boldsymbol{\\lambda}^{(k+1)})$$\n",
    "\n",
    ">* Allows any form of $q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})$ / e.g. mean-field approximation, $q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}) = \\prod^n_{i=1} q_i (z_i ; \\tilde{\\boldsymbol{\\lambda}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Variational Auto Encoder\n",
    "\n",
    "* **Variational Auto Encoder**\n",
    "\n",
    ">$$p(\\mathbf{x};\\boldsymbol{\\lambda}) = \\int p(\\mathbf{x}|\\mathbf{z};\\boldsymbol{\\lambda})p(\\mathbf{z})d\\mathbf{z}\n",
    "= \\int \\mathcal{N} (\\mathbf{x};\\mathbf{f}(\\mathbf{z};\\boldsymbol{\\lambda}), \\sigma^2 \\mathbf{I}) p(\\mathbf{z}) \n",
    "d\\mathbf{z}$$\n",
    "\n",
    ">* $\\mathbf{f}(\\mathbf{z};\\boldsymbol{\\lambda})$: cannot compute integral\n",
    ">* VEM (normally) not possible for DNN\n",
    "\n",
    "* **(Stochastic) Gradient Descent** - approximate gradient by lower-bound gradient\n",
    "\n",
    ">\\begin{align}\n",
    "\\nabla \\mathcal{L}(\\boldsymbol{\\lambda}) &\\approx \\nabla \\left( \\bigg\\langle\n",
    "\\log \\left( \\frac{p(\\mathbf{x},\\mathbf{z};\\boldsymbol{\\lambda})}{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})}\n",
    "\\right)\n",
    "\\bigg \\rangle_{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})} \\right) \\\\\n",
    "&= \\nabla \\left(\n",
    "\\langle \\log (p(\\mathbf{x}|\\mathbf{z};\\boldsymbol{\\lambda})) \\rangle_{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})}\n",
    "+ \\bigg\\langle \\log \\left( \\frac{p(\\mathbf{z})}{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})} \\right)\n",
    "\\bigg\\rangle_{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})}\n",
    "\\right)\n",
    "\\end{align}\n",
    "\n",
    ">* $p(\\mathbf{x}|\\mathbf{z};\\boldsymbol{\\lambda}) = \\mathcal{N}(\\mathbf{x};\\mathbf{f}(\\mathbf{z};\\boldsymbol{\\lambda}),\\sigma^2 \\mathbf{I})$\n",
    ">* $p(\\mathbf{z})$: also Gaussian\n",
    ">* **Iterative Optimization**\n",
    ">  * Optimize $\\boldsymbol{\\lambda}$ (model parameters)\n",
    ">  * Optimize $\\tilde{\\boldsymbol{\\lambda}}$ (variational approximation)\n",
    "\n",
    "* **Variational Form**\n",
    "\n",
    ">* Introduce dependence on $\\mathbf{x}$\n",
    "\n",
    ">$$q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}) \\rightarrow q(\\mathbf{z}|\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}}) = \\mathcal{N}(\\mathbf{z};\\mathbf{f}_\\boldsymbol{\\mu} (\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}}), \\mathbf{f}_\\boldsymbol{\\Sigma} (\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}}))$$\n",
    "\n",
    ">* Everything is Gaussian $\\rightarrow$ rewrite $\\mathcal{L}(\\boldsymbol{\\lambda})$\n",
    "\n",
    ">$$\\mathcal{L}(\\boldsymbol{\\lambda}) = \n",
    "\\Bigg\\langle \\log \\left( \n",
    "\\frac{q(\\mathbf{z}|\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})}\n",
    "{p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda})} \\right)\n",
    "+ \\log (p(\\mathbf{x}|\\mathbf{z};\\boldsymbol{\\lambda})) \n",
    "+ \\log \\left( \\frac{p(\\mathbf{z})}\n",
    "{q(\\mathbf{z}|\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})} \n",
    "\\right) \\Bigg\\rangle_{q(\\mathbf{z}|\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})}$$\n",
    "\n",
    ">* **1st Term: error**\n",
    ">  * Often neglected to yield the gradient lower-bound\n",
    ">* **2nd Term: decoding** (compute probability given $\\mathbf{z}$)\n",
    ">  * Difficult if $\\mathbf{f}(\\mathbf{z})$ is non-linear\n",
    ">* **3rd Term: encoding** (encode information about $\\mathbf{x}$ into $\\mathbf{z}$)\n",
    ">  * (-) **KL Divergence between Gaussians** $\\rightarrow$ closed-form solution\n",
    "\n",
    "* **Monte-Carlo Approximation** and **Reparameterization Trick**\n",
    "\n",
    ">\\begin{align}\n",
    "\\langle \\log (p(\\mathbf{x}|\\mathbf{z};\\boldsymbol{\\lambda})) \\rangle_{q(\\mathbf{z}|\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})}\n",
    "&\\approx \\frac{1}{K} \\sum^K_{i=1} \\log (p(\\mathbf{x}|\\mathbf{z}^{(i)};\\boldsymbol{\\lambda})) \\\\\n",
    "q(\\mathbf{z}|\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}}) &= \n",
    "\\mathcal{N}(\\mathbf{z};\\mathbf{f}_\\mathbf{\\mu} (\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}}), \n",
    "\\mathbf{f}_\\mathbf{\\Sigma} (\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})) \\\\\n",
    "\\mathbf{z}^{(i)} &= \\mathbf{f}_{\\boldsymbol{\\mu}} (\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})\n",
    "+ \\mathbf{f}_{\\boldsymbol{\\Sigma}} (\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})^{1/2} \\boldsymbol{\\epsilon}^{(i)}\n",
    "\\;\\;\\;,\\;\\;\\; \\boldsymbol{\\epsilon}^{(i)} \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I})\n",
    "\\end{align}\n",
    "\n",
    ">* **As a result,**\n",
    "\n",
    ">$$\\mathcal{L}(\\boldsymbol{\\lambda}) \\geq\n",
    "\\langle \\log (p(\\mathbf{x}|\n",
    "\\mathbf{f}_{\\boldsymbol{\\mu}} (\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})\n",
    "+ \\mathbf{f}_{\\boldsymbol{\\Sigma}} (\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})^{1/2} \\boldsymbol{\\epsilon}\n",
    ";\\boldsymbol{\\lambda})) \\rangle_{\\mathcal{N}(\\mathbf{0},\\mathbf{I})}\n",
    "+ \\Bigg\\langle \\log \\left( \\frac{p(\\mathbf{z})}\n",
    "{q(\\mathbf{z}|\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})} \n",
    "\\right) \\Bigg\\rangle_{q(\\mathbf{z}|\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Introduction\n",
    "\n",
    "* **Majority Voting**\n",
    "\n",
    ">$$P(\\text{error}) = \\sum^N_{i=\\frac{N}{2}} \\left( \\begin{matrix} N \\\\ i \\end{matrix} \\right)\n",
    "p^i_e (1-p_e)^{N-i}$$\n",
    "\n",
    "* **Bayesian Approaches** and **Monte-Carlo Approximation**\n",
    "\n",
    ">* Consider an ensemble of **discriminative classifiers**\n",
    "\n",
    ">\\begin{align}\n",
    "\\hat{\\omega} &= \\text{argmax}_{{\\omega}} \\left\\{ \n",
    "\\sum_{\\mathcal{M}} \\int P({\\omega}|\\mathbf{x}^\\star;\\boldsymbol{\\theta},\\mathcal{M})\n",
    "p(\\boldsymbol{\\theta}|\\mathcal{M},\\mathcal{D})\n",
    "P(\\mathcal{M}|\\mathcal{D})d\\boldsymbol{\\theta} \\right\\} \\\\\n",
    "&\\approx \\text{argmax}_{{\\omega}} \\left\\{ \n",
    "\\frac{1}{N} \\sum^N_{j=1} P({\\omega}|\\mathbf{x}^\\star;\\mathcal{M}^{(j)})\n",
    "\\right\\} \\;\\;\\;,\\;\\;\\;\n",
    "\\mathcal{M}^{(i)} \\sim p(\\boldsymbol{\\theta},\\mathcal{M}|\\mathcal{D})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Ensemble Generation\n",
    "\n",
    "* **Random Network Initialization** (each $\\mathcal{M}{(i)}$ at local optimum)\n",
    "\n",
    ">\\begin{align}\n",
    "\\tilde{\\mathcal{M}}^{(i)} &\\sim p(\\boldsymbol{\\theta}|\\mathcal{M}) \n",
    "\\;\\;\\;\\leftarrow\\;\\;\\; \\text{prior over parameters}\\\\\n",
    "\\mathcal{M}^{(i)} &= \\text{argmax}_{\\boldsymbol{\\theta}} \\left\\{ \n",
    "\\sum^n_{j=1} \\log \\left( P \\left( y_j | \\mathbf{x}_j ; \\boldsymbol{\\theta}, \\tilde{\\mathcal{M}}^{(i)} \\right)\n",
    "\\right)\n",
    "\\right\\}\n",
    "\\end{align}\n",
    "\n",
    "* **Bagging** - train each model on a random subset of data $\\tilde{\\mathcal{D}}$\n",
    "* **Monte Carlo Dropout** - de-activate random nodes\n",
    "* **Adhoc Models** - number/size of layers, activation fn., cost function, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Model Compression: Teacher-Student Training\n",
    "\n",
    "* **Cross Entropy Training Criterion** \n",
    "\n",
    ">$$\\mathcal{D}=\\{\\mathbf{x}_{1:n}, y_{1:n}\\} \\;\\;\\;,\\;\\;\\; y_i \\in \\{\\omega_1,...,\\omega_K\\}$$\n",
    "\n",
    ">$$\\mathcal{F}_{ce} = - \\sum^n_{i=1} \\log P(y_i|\\mathbf{x}_i;\\mathcal{M}_S)\n",
    "= - \\sum^n_{i=1} \\sum_{\\omega} \\delta(y_i,\\omega) \\log P(\\omega|\\mathbf{x}_i;\\mathcal{M}_S)$$\n",
    "\n",
    ">* $\\delta(y_i,\\omega)$: Kronecker delta function, sum over all classes\n",
    "\n",
    "* **Modify Targets based on a Teacher Network** (soft targets)\n",
    "\n",
    ">$$\\mathcal{F}_{ts} = - \\sum^n_{i=1} \\sum_{\\omega} \n",
    "P(\\omega|\\mathbf{x}_i;\\mathcal{M}_T)\n",
    "\\log P(\\omega|\\mathbf{x}_i;\\mathcal{M}_S)$$\n",
    "\n",
    "* **Replace the Teacher by an Ensemble**\n",
    "\n",
    ">$$\\mathcal{F}_{ts} = - \\sum^n_{i=1} \\sum_{\\omega} \n",
    "\\left[\n",
    "\\frac{1}{N} \\sum^N_{j=1} P \\left( \\omega|\\mathbf{x}_i;\\mathcal{M}^{(j)} \\right)\n",
    "\\right]\n",
    "\\log P(\\omega|\\mathbf{x}_i;\\mathcal{M}_S)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
