{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Variational Auto Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Latent Variable Models\n",
    "\n",
    "* **Discrete LV**\n",
    "\n",
    ">$$p(\\mathbf{x}) = \\sum^M_{m=1} P(c_m) p(\\mathbf{x}|c_m)$$\n",
    "\n",
    "* **Continuous LV**\n",
    "\n",
    ">$$p(\\mathbf{x}) = \\int p(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z})d\\mathbf{z}$$\n",
    "\n",
    ">* $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z};\\mathbf{0},\\mathbf{I})$\n",
    ">* $p(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mathbf{x};\\mathbf{f}(\\mathbf{z}),\\sigma^2 \\mathbf{I})$\n",
    "\n",
    "* **Factor Analysis**, $\\mathbf{f}(\\mathbf{z}) = \\mathbf{Az}$\n",
    "\n",
    ">* **Auxiliary function:**\n",
    "\n",
    ">$$\\mathcal{Q}(\\boldsymbol{\\lambda}, \\tilde{\\boldsymbol{\\lambda}}) = \\int p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda}) \\log (p(\\mathbf{x}|\\mathbf{z};\\boldsymbol{\\lambda})) d\\mathbf{z}$$\n",
    "\n",
    ">* **General mapping using NN**\n",
    "\n",
    ">$$p(\\mathbf{x}|\\mathbf{z};\\boldsymbol{\\lambda}) = \\mathcal{N}(\\mathbf{x};\\mathbf{f}(\\mathbf{z};\\boldsymbol{\\lambda}),\\sigma^2 \\mathbf{I})$$\n",
    "\n",
    ">* No simple closed-form solution $\\rightarrow$ adopt **variational** approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. KL Divergence\n",
    "\n",
    "* **Definition**\n",
    "\n",
    ">$$\\mathcal{KL}(p(\\mathbf{x})||q(\\mathbf{x})) = \\int p(\\mathbf{x}) \\log \\left( \\frac{p(\\mathbf{x})}{q(\\mathbf{x})} \\right) d\\mathbf{x} = - \\int p(\\mathbf{x}) \\log \\left( \\frac{q(\\mathbf{x})}{p(\\mathbf{x})} \\right) d\\mathbf{x}$$\n",
    "\n",
    "* **Inequality** (use $\\log y \\leq y-1$)\n",
    "\n",
    ">$$\\int p(\\mathbf{x}) \\left( \\frac{q(\\mathbf{x})}{p(\\mathbf{x})} \\right) d\\mathbf{x} \\leq\n",
    "\\int p(\\mathbf{x}) \\left( \\frac{q(\\mathbf{x})}{p(\\mathbf{x})} - 1 \\right) d\\mathbf{x} = 0$$\n",
    "\n",
    "* **KL Divergence for Gaussians**\n",
    "\n",
    ">$$\\mathcal{KL}(p(\\mathbf{x})||q(\\mathbf{x})) = \\frac{1}{2} \\left( \\text{tr} (\\boldsymbol{\\Sigma}^{-1}_2 \\boldsymbol{\\Sigma}_1 - \\mathbf{I}) + (\\mu_1 - \\mu_2)^T \\boldsymbol{\\Sigma}^{-1}_2 (\\mu_1 - \\mu_2) + \\log \\left( \\frac{|\\boldsymbol{\\Sigma}_2|}{|\\boldsymbol{\\Sigma}_1|} \\right) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Variational EM\n",
    "\n",
    "* **EM: Auxiliary Function Maximization**\n",
    "\n",
    ">$$\\boldsymbol{\\lambda}^{(k+1)} = \\text{argmax}_{\\boldsymbol{\\lambda}} \\left( \\mathcal{Q}(\\boldsymbol{\\lambda}^{(k)},\\boldsymbol{\\lambda}) \\right) = \\text{argmax}_{\\boldsymbol{\\lambda}} \\left( \\int p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda}^{(k)}) \\log (p(\\mathbf{x},\\mathbf{z};\\boldsymbol{\\lambda})) d\\mathbf{z} \\right)$$\n",
    "\n",
    ">$=$ **expected value of the log joint distribution**\n",
    "\n",
    ">\\begin{align}\n",
    "\\log (p(\\mathbf{x},\\mathbf{z};\\boldsymbol{\\lambda})) &= \\log (p(\\mathbf{x}|\\mathbf{z};\\boldsymbol{\\lambda})) + \\log (p(\\mathbf{z})) \\\\\n",
    "&= \\log (\\mathcal{N} (\\mathbf{x}; \\mathbf{Az}, \\boldsymbol{\\Sigma}_{\\text{diag}})) + \\log (\\mathcal{N}(\\mathbf{z};\\mathbf{0},\\mathbf{I})) \\\\\n",
    "&= -\\frac{1}{2} (\\mathbf{z}^T \\mathbf{A}^T \\boldsymbol{\\Sigma}^{-1}_{\\text{diag}} \\mathbf{Az} - 2\\mathbf{z}^T \\mathbf{A}^T \\boldsymbol{\\Sigma}^{-1}_{\\text{diag}} \\mathbf{x} + \\mathbf{x}^T \\boldsymbol{\\Sigma}^{-1}_{\\text{diag}} \\mathbf{x}) - \\frac{1}{2} \\log (|\\boldsymbol{\\Sigma}_{\\text{diag}}|) + C\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "* **Estimate $\\boldsymbol{\\lambda}$** by maximizing the **Log-likelihood** $\\mathcal{L}(\\boldsymbol{\\lambda})$\n",
    "\n",
    ">\\begin{align}\n",
    "\\mathcal{L}(\\boldsymbol{\\lambda}) &= \\log (p(\\mathbf{x};\\boldsymbol{\\lambda})) \\\\\n",
    "&= \\int p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda}) \\log (p(\\mathbf{x};\\boldsymbol{\\lambda})) d\\mathbf{z} \\\\\n",
    "&= \\int p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda}) \\log \\left( \\frac{p(\\mathbf{x};\\boldsymbol{\\lambda})p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda})}{p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda})} \\right) d\\mathbf{z} \\\\\n",
    "&= \\bigg\\langle \\log \\left( \\frac{p(\\mathbf{x},\\mathbf{z};\\boldsymbol{\\lambda})}{p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda})} \\right) \\bigg\\rangle_{p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda})}\n",
    "\\end{align}\n",
    "\n",
    ">* **Need to know** $p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda})$ $\\rightarrow$ use any valid distribution $q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})$\n",
    "\n",
    ">\\begin{align}\n",
    "\\mathcal{L}(\\boldsymbol{\\lambda}) &= \\bigg\\langle \\log \\left( \\frac{p(\\mathbf{x},\\mathbf{z};\\boldsymbol{\\lambda})}{p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda})} \\right) \\bigg\\rangle_{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})} \\\\\n",
    "&= \\bigg\\langle \\log \\left( \\frac{p(\\mathbf{x},\\mathbf{z};\\boldsymbol{\\lambda})}{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})} \\right) \\bigg\\rangle_{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})}\n",
    "+ \\bigg\\langle \\log \\left( \\frac{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})}{p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda})} \\right) \\bigg\\rangle_{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})} \\\\\n",
    "&\\geq \\bigg\\langle \\log \\left( \\frac{p(\\mathbf{x},\\mathbf{z};\\boldsymbol{\\lambda})}{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})} \\right) \\bigg\\rangle_{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})}\n",
    "=\\mathcal{F} (q(\\mathbf{z};\\boldsymbol{\\lambda}),\\boldsymbol{\\lambda})\n",
    "\\end{align}\n",
    "\n",
    "* **EM Revisited**\n",
    "\n",
    ">* **Set** $q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k)}) = p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda}^{(k)})$\n",
    "\n",
    ">$$\\mathcal{L}(\\boldsymbol{\\lambda}^{(k)}) = \\mathcal{F} (q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k)}),\\boldsymbol{\\lambda}^{(k)})$$\n",
    "\n",
    ">* **Maximize $\\mathcal{F} (q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k)}),\\boldsymbol{\\lambda})$ to find $\\boldsymbol{\\lambda}^{(k+1)}$**\n",
    "\n",
    ">$$\\boldsymbol{\\lambda}^{(k+1)} = \\text{argmax}_{\\boldsymbol{\\lambda}} \\Bigg\\{ \\bigg\\langle \\log \\left( \\frac{p(\\mathbf{x},\\mathbf{z};\\boldsymbol{\\lambda})}{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k)})} \\right) \\bigg\\rangle_{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k)})} \\Bigg\\}$$\n",
    "\n",
    ">* **Minimize KL divergence for variational approximation**\n",
    "\n",
    ">$$q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k+1)}) = \\text{argmin}_{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})} \n",
    "\\Bigg\\{ \\bigg\\langle\n",
    "\\log \\left(\n",
    "\\frac{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})}{p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda}^{(k+1)})}\n",
    "\\right)\n",
    "\\bigg\\rangle_{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})}\\Bigg\\}$$\n",
    "\n",
    ">* **Which occurs at $q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k+1)}) = p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda}^{(k+1)})$**\n",
    "\n",
    "* **EM Guarantees:**\n",
    "\n",
    ">$$\\mathcal{L}(\\boldsymbol{\\lambda}^{(k)}) = \\mathcal{F} \\left( q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k)}), \\boldsymbol{\\lambda}^{(k)} \\right)\n",
    "\\leq \\mathcal{F} \\left( q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k)}), \\boldsymbol{\\lambda}^{(k+1)} \\right)\n",
    "\\leq \\mathcal{L}(\\boldsymbol{\\lambda}^{(k+1)})$$\n",
    "\n",
    ">* provided $q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k)}) = p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda}^{(k)})$\n",
    "\n",
    "\n",
    "* **Variational EM** (no guarantees)\n",
    "\n",
    ">$$\\mathcal{L}(\\boldsymbol{\\lambda}^{(k)}) \\geq \\mathcal{F} \\left( q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k)}), \\boldsymbol{\\lambda}^{(k)} \\right)\n",
    "\\leq \\mathcal{F} \\left( q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}^{(k)}), \\boldsymbol{\\lambda}^{(k+1)} \\right)\n",
    "\\leq \\mathcal{L}(\\boldsymbol{\\lambda}^{(k+1)})$$\n",
    "\n",
    ">* Allows any form of $q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})$ / e.g. mean-field approximation, $q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}) = \\prod^n_{i=1} q_i (z_i ; \\tilde{\\boldsymbol{\\lambda}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Variational Auto Encoder\n",
    "\n",
    "* **Variational Auto Encoder**\n",
    "\n",
    ">$$p(\\mathbf{x};\\boldsymbol{\\lambda}) = \\int p(\\mathbf{x}|\\mathbf{z};\\boldsymbol{\\lambda})p(\\mathbf{z})d\\mathbf{z}\n",
    "= \\int \\mathcal{N} (\\mathbf{x};\\mathbf{f}(\\mathbf{z};\\boldsymbol{\\lambda}), \\sigma^2 \\mathbf{I}) p(\\mathbf{z}) \n",
    "d\\mathbf{z}$$\n",
    "\n",
    ">* $\\mathbf{f}(\\mathbf{z};\\boldsymbol{\\lambda})$: cannot compute integral\n",
    ">* VEM (normally) not possible for DNN\n",
    "\n",
    "* **(Stochastic) Gradient Descent** - approximate gradient by lower-bound gradient\n",
    "\n",
    ">\\begin{align}\n",
    "\\nabla \\mathcal{L}(\\boldsymbol{\\lambda}) &\\approx \\nabla \\left( \\bigg\\langle\n",
    "\\log \\left( \\frac{p(\\mathbf{x},\\mathbf{z};\\boldsymbol{\\lambda})}{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})}\n",
    "\\right)\n",
    "\\bigg \\rangle_{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})} \\right) \\\\\n",
    "&= \\nabla \\left(\n",
    "\\langle \\log (p(\\mathbf{x}|\\mathbf{z};\\boldsymbol{\\lambda})) \\rangle_{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})}\n",
    "+ \\bigg\\langle \\log \\left( \\frac{p(\\mathbf{z})}{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})} \\right)\n",
    "\\bigg\\rangle_{q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}})}\n",
    "\\right)\n",
    "\\end{align}\n",
    "\n",
    ">* $p(\\mathbf{x}|\\mathbf{z};\\boldsymbol{\\lambda}) = \\mathcal{N}(\\mathbf{x};\\mathbf{f}(\\mathbf{z};\\boldsymbol{\\lambda}),\\sigma^2 \\mathbf{I})$\n",
    ">* $p(\\mathbf{z})$: also Gaussian\n",
    ">* **Iterative Optimization**\n",
    ">  * Optimize $\\boldsymbol{\\lambda}$ (model parameters)\n",
    ">  * Optimize $\\tilde{\\boldsymbol{\\lambda}}$ (variational approximation)\n",
    "\n",
    "* **Variational Form**\n",
    "\n",
    ">* Introduce dependence on $\\mathbf{x}$\n",
    "\n",
    ">$$q(\\mathbf{z};\\tilde{\\boldsymbol{\\lambda}}) \\rightarrow q(\\mathbf{z}|\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}}) = \\mathcal{N}(\\mathbf{z};\\mathbf{f}_\\boldsymbol{\\mu} (\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}}), \\mathbf{f}_\\boldsymbol{\\Sigma} (\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}}))$$\n",
    "\n",
    ">* Everything is Gaussian $\\rightarrow$ rewrite $\\mathcal{L}(\\boldsymbol{\\lambda})$\n",
    "\n",
    ">$$\\mathcal{L}(\\boldsymbol{\\lambda}) = \n",
    "\\Bigg\\langle \\log \\left( \n",
    "\\frac{q(\\mathbf{z}|\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})}\n",
    "{p(\\mathbf{z}|\\mathbf{x};\\boldsymbol{\\lambda})} \\right)\n",
    "+ \\log (p(\\mathbf{x}|\\mathbf{z};\\boldsymbol{\\lambda})) \n",
    "+ \\log \\left( \\frac{p(\\mathbf{z})}\n",
    "{q(\\mathbf{z}|\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})} \n",
    "\\right) \\Bigg\\rangle_{q(\\mathbf{z}|\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})}$$\n",
    "\n",
    ">* **1st Term: error**\n",
    ">  * Often neglected to yield the gradient lower-bound\n",
    ">* **2nd Term: decoding** (compute probability given $\\mathbf{z}$)\n",
    ">  * Difficult if $\\mathbf{f}(\\mathbf{z})$ is non-linear\n",
    ">* **3rd Term: encoding** (encode information about $\\mathbf{x}$ into $\\mathbf{z}$)\n",
    ">  * (-) **KL Divergence between Gaussians** $\\rightarrow$ closed-form solution\n",
    "\n",
    "* **Monte-Carlo Approximation** and **Reparameterization Trick**\n",
    "\n",
    ">\\begin{align}\n",
    "\\langle \\log (p(\\mathbf{x}|\\mathbf{z};\\boldsymbol{\\lambda})) \\rangle_{q(\\mathbf{z}|\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})}\n",
    "&\\approx \\frac{1}{K} \\sum^K_{i=1} \\log (p(\\mathbf{x}|\\mathbf{z}^{(i)};\\boldsymbol{\\lambda})) \\\\\n",
    "q(\\mathbf{z}|\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}}) &= \n",
    "\\mathcal{N}(\\mathbf{z};\\mathbf{f}_\\mathbf{\\mu} (\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}}), \n",
    "\\mathbf{f}_\\mathbf{\\Sigma} (\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})) \\\\\n",
    "\\mathbf{z}^{(i)} &= \\mathbf{f}_{\\boldsymbol{\\mu}} (\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})\n",
    "+ \\mathbf{f}_{\\boldsymbol{\\Sigma}} (\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})^{1/2} \\boldsymbol{\\epsilon}^{(i)}\n",
    "\\;\\;\\;,\\;\\;\\; \\boldsymbol{\\epsilon}^{(i)} \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I})\n",
    "\\end{align}\n",
    "\n",
    ">* **As a result,**\n",
    "\n",
    ">$$\\mathcal{L}(\\boldsymbol{\\lambda}) \\geq\n",
    "\\langle \\log (p(\\mathbf{x}|\n",
    "\\mathbf{f}_{\\boldsymbol{\\mu}} (\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})\n",
    "+ \\mathbf{f}_{\\boldsymbol{\\Sigma}} (\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})^{1/2} \\boldsymbol{\\epsilon}\n",
    ";\\boldsymbol{\\lambda})) \\rangle_{\\mathcal{N}(\\mathbf{0},\\mathbf{I})}\n",
    "+ \\Bigg\\langle \\log \\left( \\frac{p(\\mathbf{z})}\n",
    "{q(\\mathbf{z}|\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})} \n",
    "\\right) \\Bigg\\rangle_{q(\\mathbf{z}|\\mathbf{x};\\tilde{\\boldsymbol{\\lambda}})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Introduction\n",
    "\n",
    "* **Majority Voting**\n",
    "\n",
    ">$$P(\\text{error}) = \\sum^N_{i=\\frac{N}{2}} \\left( \\begin{matrix} N \\\\ i \\end{matrix} \\right)\n",
    "p^i_e (1-p_e)^{N-i}$$\n",
    "\n",
    "* **Bayesian Approaches** and **Monte-Carlo Approximation**\n",
    "\n",
    ">* Consider an ensemble of **discriminative classifiers**\n",
    "\n",
    ">\\begin{align}\n",
    "\\hat{\\omega} &= \\text{argmax}_{{\\omega}} \\left\\{ \n",
    "\\sum_{\\mathcal{M}} \\int P({\\omega}|\\mathbf{x}^\\star;\\boldsymbol{\\theta},\\mathcal{M})\n",
    "p(\\boldsymbol{\\theta}|\\mathcal{M},\\mathcal{D})\n",
    "P(\\mathcal{M}|\\mathcal{D})d\\boldsymbol{\\theta} \\right\\} \\\\\n",
    "&\\approx \\text{argmax}_{{\\omega}} \\left\\{ \n",
    "\\frac{1}{N} \\sum^N_{j=1} P({\\omega}|\\mathbf{x}^\\star;\\mathcal{M}^{(j)})\n",
    "\\right\\} \\;\\;\\;,\\;\\;\\;\n",
    "\\mathcal{M}^{(i)} \\sim p(\\boldsymbol{\\theta},\\mathcal{M}|\\mathcal{D})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Ensemble Generation\n",
    "\n",
    "* **Random Network Initialization** (each $\\mathcal{M}{(i)}$ at local optimum)\n",
    "\n",
    ">\\begin{align}\n",
    "\\tilde{\\mathcal{M}}^{(i)} &\\sim p(\\boldsymbol{\\theta}|\\mathcal{M}) \n",
    "\\;\\;\\;\\leftarrow\\;\\;\\; \\text{prior over parameters}\\\\\n",
    "\\mathcal{M}^{(i)} &= \\text{argmax}_{\\boldsymbol{\\theta}} \\left\\{ \n",
    "\\sum^n_{j=1} \\log \\left( P \\left( y_j | \\mathbf{x}_j ; \\boldsymbol{\\theta}, \\tilde{\\mathcal{M}}^{(i)} \\right)\n",
    "\\right)\n",
    "\\right\\}\n",
    "\\end{align}\n",
    "\n",
    "* **Bagging** - train each model on a random subset of data $\\tilde{\\mathcal{D}}$\n",
    "* **Monte Carlo Dropout** - de-activate random nodes\n",
    "* **Adhoc Models** - number/size of layers, activation fn., cost function, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Model Compression: Teacher-Student Training\n",
    "\n",
    "* **Cross Entropy Training Criterion** \n",
    "\n",
    ">$$\\mathcal{D}=\\{\\mathbf{x}_{1:n}, y_{1:n}\\} \\;\\;\\;,\\;\\;\\; y_i \\in \\{\\omega_1,...,\\omega_K\\}$$\n",
    "\n",
    ">$$\\mathcal{F}_{ce} = - \\sum^n_{i=1} \\log P(y_i|\\mathbf{x}_i;\\mathcal{M}_S)\n",
    "= - \\sum^n_{i=1} \\sum_{\\omega} \\delta(y_i,\\omega) \\log P(\\omega|\\mathbf{x}_i;\\mathcal{M}_S)$$\n",
    "\n",
    ">* $\\delta(y_i,\\omega)$: Kronecker delta function, sum over all classes\n",
    "\n",
    "* **Modify Targets based on a Teacher Network** (soft targets)\n",
    "\n",
    ">$$\\mathcal{F}_{ts} = - \\sum^n_{i=1} \\sum_{\\omega} \n",
    "P(\\omega|\\mathbf{x}_i;\\mathcal{M}_T)\n",
    "\\log P(\\omega|\\mathbf{x}_i;\\mathcal{M}_S)$$\n",
    "\n",
    "* **Replace the Teacher by an Ensemble**\n",
    "\n",
    ">$$\\mathcal{F}_{ts} = - \\sum^n_{i=1} \\sum_{\\omega} \n",
    "\\left[\n",
    "\\frac{1}{N} \\sum^N_{j=1} P \\left( \\omega|\\mathbf{x}_i;\\mathcal{M}^{(j)} \\right)\n",
    "\\right]\n",
    "\\log P(\\omega|\\mathbf{x}_i;\\mathcal{M}_S)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Kernels for Structured Data\n",
    "\n",
    ">1. **Extract features from input objects**\n",
    "1. **Compute kernels (dot products) using those features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. String Kernels\n",
    "\n",
    "* **String Kernel**\n",
    "\n",
    ">1. Given a list of substrings $s_1,s_2,... \\in \\mathcal{A}^\\star$, encode $\\boldsymbol{\\phi}(x) = (\\phi_{s_1}(x), \\phi_{s_2}(x), ...)^T$\n",
    ">1. $\\phi_{s}(x)$: indicate the occurrence of $s$ in $x$\n",
    ">1. Kernel between two strings defined as dot product:\n",
    "\n",
    ">$$k(x,x') = \\boldsymbol{\\phi}(x)^T \\boldsymbol{\\phi}(x') \n",
    "= \\sum_{s\\in\\{s_1,s_2,...\\}} \\phi_s(x) \\phi_s(x')$$\n",
    "\n",
    "* **Example: Gap-weighted Kernel**\n",
    "\n",
    ">* $\\phi_{s}(x)$: # occurrences, $\\lambda^n$ for $n$ gaps\n",
    "\n",
    "* **Example: $k$-spectrum kernel**\n",
    "\n",
    ">* $\\phi_{s}(x)$: # **exact** occurrences ($s$: any possible length-$k$ sequence)\n",
    ">* Selection of $k$\n",
    ">  * Large $k$: co-occurrence is more informative\n",
    ">  * Small $k$: # co-occurrence increases\n",
    ">  * $k=1$: **Bag-of-words kernel**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Tree Kernels\n",
    "\n",
    "* **Definitions**\n",
    "\n",
    ">* **Subtree:** tree formed by selecting one node an all its descendants\n",
    ">* **Subset Tree:** subtree including all children of a node or none of them\n",
    "\n",
    "* **Tree Kernel**\n",
    "\n",
    ">1. Given a list of all possible subset trees $t_1,t_2,...$, each tree $\\mathcal{T}$ is encoded using the feature vector $\\boldsymbol{\\phi}(\\mathcal{T}) = (\\phi_{t_1}(\\mathcal{T}), \\phi_{t_2}(\\mathcal{T}), ...)^T$\n",
    ">2. Each $\\phi_{t}(\\mathcal{T})$ counts occurrences of $t$ in $\\mathcal{T}$ ($\\mathcal{V}(\\mathcal{T})$: set of nodes in $\\mathcal{T}$)\n",
    ">$$$$\n",
    ">$$\\phi_t (\\mathcal{T}) = \\sum_{n \\in \\mathcal{V}(\\mathcal{T})} I_t (n)\n",
    "\\;\\;\\;,\\;\\;\\;\n",
    "I_t(n) = \\bigg\\{ \\begin{matrix} 1 & t \\text{ found in } \\mathcal{T} \\text{ with root at node } n \\\\ 0 & \\text{otherwise} \\end{matrix} $$\n",
    ">$$$$\n",
    ">3. Kernel between two trees defined as dot product:\n",
    "\n",
    ">$$k(\\mathcal{T}_a,\\mathcal{T}_b) = \\boldsymbol{\\phi}(\\mathcal{T}_a)^T \\boldsymbol{\\phi}(\\mathcal{T}_b) = \\sum_{t \\in \\{t_1,t_2,...\\}} \\phi_t(\\mathcal{T}_a) \\phi_t(\\mathcal{T}_b)$$\n",
    "\n",
    "* **Efficient Computation of Tree Kernels and Example**\n",
    "\n",
    ">\\begin{align}\n",
    "k(\\mathcal{T}_a,\\mathcal{T}_b) &= \\sum_{n_a \\in \\mathcal{V}(\\mathcal{T}_a)}\n",
    "\\sum_{n_b \\in \\mathcal{V}(\\mathcal{T}_b)} f(n_a, n_b) \\\\\n",
    "\\\\\n",
    "f(n_a,n_b) &= \\sum_{t \\in \\{t_1,t_2,...\\}} I_t(n_a) I_t(n_b)\n",
    "\\end{align}\n",
    "\n",
    ">* $f(n_a,n_b)$: # common subset trees at $n_a$ and $n_b$\n",
    "\n",
    ">|$\\hspace{40mm}$Condition|                         Then|\n",
    "|-|-|\n",
    "|$n_a \\neq n_b$ **or** $\\text{ch}(n_a) \\neq \\text{ch}(n_b)$|$f(n_a,n_b)=0$|\n",
    "|else if $n_a$ and $n_b$ are leaf nodes|$f(n_a,n_b)=1$|\n",
    "|otherwise|$f(n_a,n_b)=\\prod^{|\\text{ch}(n_a)|}_{i=1} g(\\text{ch}(n_a)_i,\\text{ch}(n_b)_i)$<br/><br/>$g(n_1,n_2) = \\bigg\\{ \\begin{matrix} 1 & \\text{if } n_1 \\text{ or } n_2 \\text{ leaf} \\\\ 1+f(n_1,n_2) & \\text{otherwise} \\end{matrix}$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Graph Kernels\n",
    "\n",
    "* **Graphs and Graph Walks**\n",
    "\n",
    ">* **Graph:** $\\mathcal{G}=\\{ \\mathcal{V},\\mathcal{E} \\}$ where $\\mathcal{E} = \\{ (i,j);i,j\\in\\mathcal{V} \\}$\n",
    ">* **Graph Walks:** $k$-length walk defined as $w=\\{v_1,...,v_{k+1}\\}$ where $(v_i,v_{i+1}) \\in \\mathcal{E}$\n",
    ">* Number of $k$-length walks between nodes $i$ and $j$:\n",
    "\n",
    ">$$[\\mathbf{A}^k]_{i,j} = \\sum^{|\\mathcal{V}|}_{s_1 = 1} \\cdots \\sum^{|\\mathcal{V}|}_{s_{k-1} = 1} a_{i,s_1} a_{s_1,s_2} \\cdot a_{s_{k-1},j}$$\n",
    "\n",
    "* **Random-walk Graph Kernel**\n",
    "\n",
    ">* $k(\\mathcal{G},\\mathcal{G}')$: # common walks in the two graphs\n",
    ">* Computed using **direct product graph**\n",
    "\n",
    ">\\begin{align}\n",
    "\\mathcal{G}_\\times &= (\\mathcal{V}_\\times, \\mathcal{E}_\\times) \\\\\n",
    "\\mathcal{V}_\\times &= \\{ (a,a') ; a\\in\\mathcal{V} \\text{ and } a'\\in\\mathcal{V}' \\} \\\\\n",
    "\\mathcal{E}_\\times &= \\{ ((a,a'),(b,b')) ; (a,b)\\in\\mathcal{E} \\text{ and } (a',b')\\in\\mathcal{E}' \\}\n",
    "\\end{align}\n",
    "\n",
    ">* **Kernel Definition** ($\\mathbf{A}_\\times = \\mathbf{A} \\otimes \\mathbf{A}'$ where $\\otimes$ is the **Kronecker product**)\n",
    "\n",
    ">$$k(\\mathcal{G},\\mathcal{G}') = \\sum^{|\\mathcal{V}_\\times|}_{i,j = 1}\n",
    "\\left[ \\sum^\\infty_{n=0} \\lambda^n \\mathbf{A}^n_\\times \\right]_{i,j}\n",
    "= \\mathbf{1}^T [\\mathbf{I} - \\lambda \\mathbf{A} \\otimes \\mathbf{A}']^{-1} \\mathbf{1}\n",
    "\\Leftrightarrow \\mathbf{x} = \\mathbf{1} + \\lambda (\\mathbf{A} \\otimes \\mathbf{A}')\\mathbf{x}$$\n",
    "\n",
    ">* **Problems:** \n",
    ">  * A walk can visit the same cycle multiple times $\\rightarrow$ small structural similarities can produce huge kernel values\n",
    ">  * High cost, $\\mathcal{O}(n^3)$\n",
    "\n",
    "* **Weisfeiler-Lehman Graph Kernel**\n",
    "\n",
    ">1. Create a set with labels of adjacent vertices & sort\n",
    ">1. Add vertex label as a prefix\n",
    ">1. Compress resulting label sequence into a **unique value**\n",
    ">1. Assign the unique value as new **vertex label**\n",
    ">1. Apply the **bag-of-words** kernel to the vertex labels (include the initial labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Fisher Kernel\n",
    "\n",
    "* **Idea**\n",
    "\n",
    ">* Use a **probabilistic generative model** to obtain a **fixed-length vector representation** of complex structured data\n",
    "\n",
    "* **Steps**\n",
    "\n",
    ">1. Train $p(\\mathbf{x}|\\boldsymbol{\\theta})$ on $\\{\\mathbf{x}_n\\}^N_{n=1}$ (e.g. using MLE)\n",
    ">1. Define the **Fisher score vector** as $\\boldsymbol{\\phi}(\\mathbf{x}_n) = \\nabla_{\\boldsymbol{\\theta}} \\log p(\\mathbf{x}_n|\\boldsymbol{\\theta}) |_{\\boldsymbol{\\theta}_{\\text{MLE}}}$\n",
    ">1. Define **naive Fisher kernel** as $k(\\mathbf{x}_n,\\mathbf{x}_m) = \\boldsymbol{\\phi}(\\mathbf{x}_n)^T \\boldsymbol{\\phi}(\\mathbf{x}_m)$\n",
    "\n",
    "* **Example: Mixture of Gaussians**\n",
    "\n",
    ">\\begin{align}\n",
    "p(\\mathbf{x}|\\boldsymbol{\\theta}) &= \\sum^K_{k=1} p(\\mathbf{x}|\\theta_k)\\pi_k \\\\\n",
    "[\\boldsymbol{\\phi}(\\mathbf{x}_n)]_{pi_k} &= \\frac{\\partial \\log p(\\mathbf{x}|\\boldsymbol{\\theta})}{\\partial \\pi_k} \\bigg|_{\\boldsymbol{\\theta}=\\boldsymbol{\\theta}_{\\text{MLE}}}\n",
    "= \\frac{p(\\mathbf{x}|\\theta^{\\text{MLE}}_k)}{\\sum_k p(\\mathbf{x}|\\theta^{\\text{MLE}}_k) \\pi^{\\text{MLE}}_k}\n",
    "\\end{align}\n",
    "\n",
    ">* $[\\boldsymbol{\\phi}(\\mathbf{x}_n)]_{pi_k}$: amount by which the $k$-th component contributes to generate $\\mathbf{x}_n$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
