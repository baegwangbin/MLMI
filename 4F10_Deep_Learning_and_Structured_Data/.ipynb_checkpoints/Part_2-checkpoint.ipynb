{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Building Blocks\n",
    "\n",
    "* **Activation Function**\n",
    "\n",
    ">|Activation Function|$\\hspace{50mm}$Formula|$\\hspace{30mm}$Output|\n",
    "|-|-|-|\n",
    "|**Heaviside**|$\\phi(z_i)=\\bigg\\{ \\begin{matrix} 0&z_i<0 \\\\ 1&z_i\\geq0 \\end{matrix}$||\n",
    "|**Sigmoid**|$\\phi(z_i)=\\frac{1}{1+\\exp(-z_i)}$|$0\\leq y_i(\\mathbf{x}) \\leq 1$|\n",
    "|**Softmax**|$\\phi(z_i)=\\frac{\\exp(z_i)}{\\sum_j \\exp(z_j)}$|$0\\leq y_i(\\mathbf{x}) \\leq 1 \\; \\sum_i y_i(\\mathbf{x})=1$|\n",
    "|**tanh**|$\\phi(z_i)=\\frac{\\exp(z_i)-\\exp(-z_i)}{\\exp(z_i)+\\exp(-z_i)}$|$-1\\leq y_i(\\mathbf{x}) \\leq 1$|\n",
    "|**ReLU**|$\\phi(z_i)=\\max(0,z_i)$||\n",
    "|**Noisy ReLU**|$\\phi(z_i)= \\max(0,z_i+\\epsilon)$|$\\epsilon\\sim\\mathcal{N}(0,\\sigma^2)$|\n",
    "|**Leaky ReLU**|$\\phi(z_i)=\\bigg\\{ \\begin{matrix} z_i&z_i\\geq0 \\\\ \\alpha z_i&z_i<0 \\end{matrix}$|$\\;$|\n",
    "\n",
    "* **Pooling/Max-Out Functions** (reduces #parameters)\n",
    "\n",
    ">|Pooling Function|$\\hspace{70mm}$Formula|\n",
    "|-|-|\n",
    "|**maxout**|$\\phi(y_1,y_2,y_3) = \\max(y_1,y_2,y_3)$|\n",
    "|**soft-maxout**|$\\phi(y_1,y_2,y_3)=\\log \\left( \\sum^3_{i=1} \\exp(y_i) \\right)$|\n",
    "|**p-norm**|$\\phi(y_1,y_2,y_3) = (\\sum^3_{i=1} |y_i|^p)^{1/p}$|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Examples of NN \n",
    "\n",
    "* **CNN**\n",
    "\n",
    ">$$\\phi(z_{ij}) = \\phi \\left( \\sum_{kl} w_{kl} x_{(i-k)(j-l)} \\right)$$\n",
    "\n",
    ">* **Parameters:**\n",
    ">  * **Number(depth)** of filters\n",
    ">  * **Receptive Field** (height $\\times$ width $\\times$ depth)\n",
    ">  * **Stride** / **Dilation** / **Zero-padding** \n",
    "\n",
    "* **Autoencoders** (Non-linear Feature Extraction / can be used to **denoise** data)\n",
    "\n",
    ">* Training criterion: $E(\\boldsymbol{\\theta}) = \\sum^n_{p=1} f(\\mathbf{x}_p,\\hat{\\mathbf{x}}_p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Network Training and Error Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Training Criteria\n",
    "\n",
    "* **Classification**\n",
    "\n",
    ">$$\\mathcal{D} = \\{(\\mathbf{x}_1,\\mathbf{t}_1),...,(\\mathbf{x}_N,\\mathbf{t}_N)\\}$$\n",
    "\n",
    ">* **Least Squares Error:**\n",
    "\n",
    ">$$E(\\boldsymbol{\\theta}) = \\frac{1}{2} \\sum^N_{p=1} ||\\mathbf{y}(\\mathbf{x}_p) - \\mathbf{t}_p||^2 = \\frac{1}{2} \\sum^N_{p=1} \\sum^K_{i=1} (y_i(\\mathbf{x}_p) - t_{pi})^2$$\n",
    "\n",
    ">* **Cross Entropy:**\n",
    "\n",
    ">\\begin{align}\n",
    "E(\\boldsymbol{\\theta}) &= - \\sum^N_{p=1} \\sum^K_{i=1} t_{pi} \\log (y_i(\\mathbf{x}_p)) \\\\\n",
    "\\text{binary} \\rightarrow &= - \\sum^N_{p=1} (t_p \\log (y(\\mathbf{x}_p)) + (1-t_p) \\log (1-y(\\mathbf{x}_p)))\n",
    "\\end{align}\n",
    "\n",
    "* **Regression**\n",
    "\n",
    ">$$\\mathcal{D} = \\{(\\mathbf{x}_1,\\mathbf{y}_1),...,(\\mathbf{x}_N,\\mathbf{y}_N)\\}$$\n",
    "\n",
    ">* **Least Squares Error:**\n",
    "\n",
    ">$$E(\\boldsymbol{\\theta}) = \\frac{1}{2} \\sum^N_{p=1} (\\mathbf{y}(\\mathbf{x}_p) - \\mathbf{y}_p)^T (\\mathbf{y}(\\mathbf{x}_p) - \\mathbf{y}_p)$$\n",
    "\n",
    ">* LS is equivalent to MLE with a single Gaussian\n",
    "\n",
    ">$$E(\\boldsymbol{\\theta}) = \\sum^N_{p=1} \\log (p(\\mathbf{y}_p|\\mathbf{x}_p;\\boldsymbol{\\theta}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Mixture Density NN\n",
    "\n",
    "* **Predict a Mixture of $M$ Gaussians**\n",
    "\n",
    ">$$\\mathbf{y}_m (\\mathbf{x}_p) = \\begin{bmatrix} \\mathcal{F}^{(c)}_m (\\mathbf{x}_p) \\\\ \\mathcal{F}^{(\\mu)}_m (\\mathbf{x}_p) \\\\ \\mathcal{F}^{(\\sigma)}_m (\\mathbf{x}_p) \\end{bmatrix} = \\begin{bmatrix} \\text{prior} \\\\ \\text{mean} \\\\ \\text{variance} \\end{bmatrix}$$\n",
    "\n",
    ">$$p(\\mathbf{y}_p|\\mathbf{x}_p;\\boldsymbol{\\theta}) = \\sum^M_{m=1} \\mathcal{F}_m^{(c)} (\\mathbf{x}_p) \\mathcal{N} \\left( \\mathbf{y}_p;\\mathcal{F}_m^{(\\mu)} (\\mathbf{x}_p), \\mathcal{F}_m^{(\\sigma)} (\\mathbf{x}_p) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Back Propagation\n",
    "\n",
    "* **Single Layer Perceptron Training** (ignore bias)\n",
    "\n",
    ">\\begin{align}\n",
    "\\frac{\\partial E(\\boldsymbol{\\theta})}{\\partial w_i} &= \\left( \\frac{\\partial z}{\\partial w_i} \\right) \\left( \\frac{\\partial y(\\mathbf{x})}{\\partial z} \\right) \\left( \\frac{\\partial E(\\boldsymbol{\\theta})}{\\partial y(\\mathbf{x})} \\right) \\\\\n",
    "\\frac{\\partial E^{(p)}(\\boldsymbol{\\theta})}{\\partial w_i} &= x_{pi} \\times y(\\mathbf{x}_p) (1-y(\\mathbf{x}_p)) \\times (y(\\mathbf{x}_p) - t_p)\n",
    "\\end{align}\n",
    "\n",
    "* **Multiple Layer** $\\rightarrow$ use **Backward Recursion**\n",
    "\n",
    ">$$\\frac{\\partial E(\\boldsymbol{\\theta})}{\\partial \\mathbf{z}^{(k)}} = \\boldsymbol{\\delta}^{(k)} = \\boldsymbol{\\Lambda}^{(k)} \\mathbf{W}^{(k+1)T} \\boldsymbol{\\delta}^{(k+1)} \\;\\;\\;,\\;\\;\\; \\frac{\\partial E(\\boldsymbol{\\theta})}{\\partial \\mathbf{W}^{(k)}} = \\boldsymbol{\\delta}^{(k)} \\mathbf{x}^{(k)T}$$\n",
    "\n",
    ">$$\\boldsymbol{\\Lambda}^{(k)} = \\frac{\\partial \\mathbf{y}^{(k)} }{\\partial \\mathbf{z}^{(k)}}\n",
    "\\;\\;\\;,\\;\\;\\;\n",
    "\\mathbf{W}^{(k+1)} = \\frac{\\partial \\mathbf{z}^{(k+1)}}{\\partial \\mathbf{y}^{(k)}} = \\frac{\\partial \\mathbf{z}^{(k+1)}}{\\partial \\mathbf{x}^{(k+1)}}\n",
    "\\;\\;\\;,\\;\\;\\;\n",
    "\\boldsymbol{\\delta}^{(k+1)} = \\frac{\\partial E(\\boldsymbol{\\theta})}{\\partial \\mathbf{z}^{(k+1)}}\n",
    "$$\n",
    "\n",
    ">* $\\boldsymbol{\\Lambda}^{(k)}$: activation derivative matrix\n",
    ">* $\\mathbf{W}^{(k+1)}$: weight matrix\n",
    ">* $\\boldsymbol{\\delta}^{(k+1)}$: error vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Gradient Descent\n",
    "\n",
    "* **Stochastic Gradient Descent**\n",
    "\n",
    ">$$ \\boldsymbol{\\theta} [\\tau+1] = \\boldsymbol{\\theta} [\\tau] - \\Delta \\boldsymbol{\\theta} [\\tau] = \\boldsymbol{\\theta} [\\tau] - \\eta \\frac{\\partial E}{\\partial \\boldsymbol{\\theta}} \\bigg| _{\\boldsymbol{\\theta}[\\tau]}$$\n",
    "\n",
    "* **Batch/online Gradient Descent**\n",
    "\n",
    ">$$E(\\boldsymbol{\\theta}) = - \\sum_{p\\in \\tilde{\\mathcal{D}}} \\sum^K_{i=1} t_{pi} \\log (y_i(\\mathbf{x}_p))$$\n",
    "\n",
    ">* **Batch-size:** small (poorly estimated gradient) / large (each update expensive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Gradient Descent Refinements\n",
    "\n",
    "* **Momentum**\n",
    "\n",
    ">\\begin{align}\n",
    "\\Delta \\boldsymbol{\\theta} [\\tau] &= \\eta \\frac{\\partial E (\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} \\bigg|_{\\boldsymbol{\\theta}[\\tau]} + \\alpha \\Delta \\boldsymbol{\\theta} [\\tau-1]\\\\\n",
    "&= \\eta \\nabla (E(\\boldsymbol{\\theta}[\\tau])) + \\alpha \\Delta \\boldsymbol{\\theta} [\\tau-1]\n",
    "\\end{align}\n",
    "\n",
    "* **Adaptive Learning Rates**\n",
    "\n",
    ">$$\\eta[\\tau+1] = \\bigg\\{ \\begin{matrix} 1.1 \\eta[\\tau] & \\text{if } E(\\boldsymbol{\\theta}[\\tau]) < E(\\boldsymbol{\\theta}[\\tau-1]) \\\\ \n",
    "0.5 \\eta[\\tau] & \\text{if } E(\\boldsymbol{\\theta}[\\tau]) > E(\\boldsymbol{\\theta}[\\tau-1])\\end{matrix}$$\n",
    "\n",
    ">* Increase $\\eta$ when going in the **correct direction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Second-order Approximation\n",
    "\n",
    "* **Second-order Approximation**\n",
    "\n",
    ">$$E(\\boldsymbol{\\theta}) = E(\\boldsymbol{\\theta}[\\tau]) + (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}[\\tau])^T \\mathbf{g} + \\frac{1}{2} (\\boldsymbol{\\theta}-\\boldsymbol{\\theta}[\\tau])^T \\mathbf{H} (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}[\\tau]) + \\mathcal{O}(\\boldsymbol{\\theta}^3)$$\n",
    "\n",
    ">$$\\mathbf{g} = \\nabla E(\\boldsymbol{\\theta}[\\tau]) \\;\\;\\;,\\;\\;\\;\n",
    "(\\mathbf{H})_{ij} = h_{ij} = \\frac{\\partial^2 E(\\boldsymbol{\\theta})}{\\partial \\theta_i \\partial \\theta_j} \\bigg|_{\\boldsymbol{\\theta}[\\tau]}$$\n",
    "\n",
    "* **Ignore Higher-order Terms** & **Equate to Zero**\n",
    "\n",
    ">$$\\nabla E(\\boldsymbol{\\theta}) = \\mathbf{g} + \\mathbf{H}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}[\\tau])$$\n",
    "\n",
    ">$$\\boldsymbol{\\theta}[\\tau + 1] = \\boldsymbol{\\theta}[\\tau] - \\mathbf{H}^{-1} \\mathbf{g} \\;\\;\\;\\rightarrow\\;\\;\\; \\Delta \\boldsymbol{\\theta}[\\tau] = \\mathbf{H}^{-1} \\mathbf{g}$$\n",
    "\n",
    ">* $\\mathbf{H}^{-1} \\mathbf{g}$: **Newton direction**\n",
    "\n",
    "* **Issues**\n",
    "\n",
    ">* **Computational cost:** Hessian evaluation: $\\mathcal{O}(N^2)$ & Hessian inversion: $\\mathcal{O}(N^3)$\n",
    ">* **Highly non-quadratic surface** $\\rightarrow$ unstable optimization\n",
    ">* $\\mathbf{H}$ must be **positive-definite** (if not, $\\mathbf{H}^{-1} \\mathbf{g}$ might head towards a maximum or saddle point)\n",
    "\n",
    ">$$\\mathbf{v}^T \\mathbf{Hv} > 0 \\;\\;\\;\\forall\\;\\mathbf{v} \\;\\;\\; \\rightarrow \\;\\;\\; \\tilde{\\mathbf{H}} = \\mathbf{H} + \\lambda \\mathbf{I}$$\n",
    "\n",
    "* **QuickProp**\n",
    "\n",
    ">* **Assumptions:** quadratic error surface & independent weight gradients (i.e. diagonal Hessian)\n",
    "\n",
    ">\\begin{align}\n",
    "E(\\theta) &\\approx E(\\theta[\\tau]) + b(\\theta - \\theta[\\tau]) + a(\\theta - \\theta[\\tau])^2 \\\\\n",
    "\\\\\n",
    "\\frac{\\partial E(\\theta)}{\\partial \\theta} &\\approx b + 2a (\\theta - \\theta[\\tau])\n",
    "\\end{align}\n",
    "\n",
    ">* **Condition:** after update $\\Delta\\theta[\\tau]$, the gradient should be zero\n",
    "\n",
    ">$$g[\\tau-1] = b-2a\\Delta\\theta[\\tau-1],\\;\\;\\; 0 = b+2a\\Delta\\theta[\\tau],\\;\\;\\; g[\\tau] = b$$\n",
    "\n",
    ">$$\\rightarrow \\Delta\\theta[\\tau] = \\frac{g[\\tau]}{g[\\tau-1] - g[\\tau]} \\Delta\\theta[\\tau-1]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Optimization Refinement\n",
    "\n",
    "* **Data Pre-processing:** subtract by mean, divide by stdev\n",
    "* **Dropout:** randomly de-activate $x$% of the nodes\n",
    "* **Regularization**\n",
    "\n",
    ">$$\\tilde{E}(\\boldsymbol{\\theta}) = E(\\boldsymbol{\\theta}) + \\nu \\Omega (\\boldsymbol{\\theta}) \\;\\;\\;,\\;\\;\\; \\Omega(\\boldsymbol{\\theta}) = \\frac{1}{2} \\sum^{L+1}_{l=1} \\sum_{i,j} w_{ij}^{(l)2}$$\n",
    "\n",
    "* **Network Initialization:** e.g. Gaussian random initialization $\\rightarrow$ sigmoid\n",
    "\n",
    "* **Xavier Initialization**\n",
    "\n",
    ">$$\\text{Var}(y_i) = \\text{Var}(\\mathbf{w}^T_i \\mathbf{x}) = n\\times \\text{Var}(w_{ij}) \\text{Var}(x_i)$$\n",
    "\n",
    ">$$\\text{Var}(w_{ij}) = \\frac{1}{n}$$\n",
    "\n",
    ">* Assume $n$-dim input with zero mean and identity variance\n",
    ">* Avoid having too small or too large weights\n",
    ">* Prevents **exploding/vanishing gradients**\n",
    "\n",
    "* **Batch Normalization**\n",
    "\n",
    ">$$\\mathbf{y}_p^{(k-1)} \\;\\;\\; \\underset{\\text{normalize}}{\\rightarrow} \\;\\;\\; \\tilde{y}_{pj}^{(k-1)} = \\frac{y^{(k-1)}_{pj} - \\tilde{\\mu}_j^{(k)}}{\\tilde{\\sigma}^{(k)}_j} \\;\\;\\;\\underset{\\text{input to layer } k}{\\rightarrow}$$\n",
    "\n",
    ">* Normalization over a batch $\\mathcal{D}$\n",
    "\n",
    ">$$\\frac{\\partial E(\\boldsymbol{\\theta})}{\\partial \\mathbf{y}^{(k-1)}} = \\frac{\\partial \\tilde{\\mathbf{y}}^{(k-1)}}{\\partial \\mathbf{y}^{(k-1)}} \\frac{\\partial E(\\boldsymbol{\\theta})}{\\partial \\tilde{\\mathbf{y}}^{(k-1)}}$$\n",
    "\n",
    ">* Normalization at test time: **expected normalization**\n",
    "\n",
    ">$$\\bar{\\mu}_j^{(k)} = \\mathbb{E} \\left[ \\tilde{\\mu}_j^{(k)} \\right] \\;\\;\\;,\\;\\;\\;\n",
    "\\bar{\\sigma}_j^{(k)2} = \\frac{m}{m-1} \\mathbb{E} \\left[ \\tilde{\\sigma}_j^{(k)2} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deep Learning for Sequence Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Sequence I/O Pair Modelling\n",
    "\n",
    "><img src='images/image12.png' width=500>\n",
    "\n",
    "* **RNN - Recurrent Neural Networks** (i.e. Elman Networks)\n",
    "\n",
    ">\\begin{align}\n",
    "\\mathbf{h}_t &= \\mathbf{f}^h (\\mathbf{W}^f_h \\mathbf{x}_t + \\mathbf{W}^r_h \\mathbf{h}_{t-1} + \\mathbf{b}_h) \\\\\n",
    "\\mathbf{y}(\\mathbf{x}_{1:t}) &= \\mathbf{f}^f (\\mathbf{W}_y \\mathbf{h}_t + \\mathbf{b}_y)\n",
    "\\end{align}\n",
    "\n",
    ">* $\\mathbf{h}_t$ encodes $\\mathbf{x}_{1:t}$\n",
    "\n",
    ">$$\\mathcal{F}(\\mathbf{x}_{1:t}) = \\mathcal{F}(\\mathbf{x}_t,\\mathbf{x}_{1:t-1}) \\approx \\mathcal{F}(\\mathbf{x}_t,\\mathbf{h}_{t-1}) \\approx \\mathcal{F}(\\mathbf{h}_t) = \\mathbf{y}(\\mathbf{x}_{1:t}) = \\mathbf{y}_t$$\n",
    "\n",
    "* **Jordan Networks**\n",
    "\n",
    ">* $\\mathbf{h}_t$ encodes $\\mathbf{y}_{1:t}$\n",
    "\n",
    ">$$\\mathcal{F}(\\mathbf{x}_{1:t},\\mathbf{y}_{1:t-1}) \\approx \\mathcal{F}(\\mathbf{x}_t,\\mathbf{y}_{1:t-1}) \\approx \\mathcal{F}(\\mathbf{x}_t,\\mathbf{h}_{t-1}) \\approx \\mathcal{F}(\\mathbf{h}_t) = \\mathbf{y}_t$$\n",
    "\n",
    "* **Bi-directional RNN**\n",
    "\n",
    ">$$\\mathcal{F}_t (\\mathbf{x}_{1:T}) = \\mathcal{F}(\\mathbf{x}_{1:t},\\mathbf{x}_{t:T}) \\approx \\mathcal{F}(\\mathbf{h}_t, \\tilde{\\mathbf{h}}_t) = \\mathbf{y}_t(\\mathbf{x}_{1:T})$$\n",
    "\n",
    "* **GRU - Gated Recurrent Unit**\n",
    "\n",
    "><img src='images/image13.png' width=300>\n",
    "\n",
    ">\\begin{align}\n",
    "\\mathbf{i}_f &= \\boldsymbol{\\sigma} (\\mathbf{W}^f_f \\mathbf{x}_t + \\mathbf{W}^r_f \\mathbf{h}_{t-1} + \\mathbf{b}_f)\\\\\n",
    "\\mathbf{i}_o &= \\boldsymbol{\\sigma} (\\mathbf{W}^f_o \\mathbf{x}_t + \\mathbf{W}^r_o \\mathbf{h}_{t-1} + \\mathbf{b}_o)\\\\\n",
    "\\tilde{\\mathbf{h}}_t &= \\mathbf{f} (\\mathbf{W}^f_h \\mathbf{x}_t + \\mathbf{W}^r_h (\\mathbf{i}_f \\odot \\mathbf{h}_{t-1}) + \\mathbf{b}_h) \\\\\n",
    "\\mathbf{h}_t &= \\mathbf{i}_o \\odot \\mathbf{h}_{t-1} + (\\mathbf{1} - \\mathbf{i}_o) \\odot \\tilde{\\mathbf{h}}_t\n",
    "\\end{align}\n",
    "\n",
    ">* $\\mathbf{i}_f$: forget gate (gating over time)\n",
    ">* $\\mathbf{i}_o$: output gate (gating over features and time)\n",
    ">* $\\odot$: element-wise multiplication\n",
    "\n",
    "* **LSTM - Long-Short Term Memory Networks**\n",
    "\n",
    "><img src='images/image14.png' width=200>\n",
    "\n",
    ">* Three Gates - **Forget Gate** ($\\mathbf{i}_f$), **Input Gate** ($\\mathbf{i}_i$) and **Output Gate** ($\\mathbf{i}_o$)\n",
    "\n",
    ">\\begin{align}\n",
    "\\mathbf{i}_f &= \\boldsymbol{\\sigma} (\\mathbf{W}^f_f \\mathbf{x}_t + \\mathbf{W}^r_f \\mathbf{h}_{t-1} + \\mathbf{W}^m_f \\mathbf{c}_{t-1} + \\mathbf{b}_f)\\\\\n",
    "\\mathbf{i}_i &= \\boldsymbol{\\sigma} (\\mathbf{W}^f_i \\mathbf{x}_t + \\mathbf{W}^r_i \\mathbf{h}_{t-1} + \\mathbf{W}^m_i \\mathbf{c}_{t-1} +\\mathbf{b}_i)\\\\\n",
    "\\mathbf{i}_o &= \\boldsymbol{\\sigma} (\\mathbf{W}^f_o \\mathbf{x}_t + \\mathbf{W}^r_o \\mathbf{h}_{t-1} + \\mathbf{W}^m_o \\mathbf{c}_{t} +\\mathbf{b}_o)\\\\\n",
    "\\end{align}\n",
    "\n",
    ">* **Memory Cell** and **History Vector**\n",
    "\n",
    ">\\begin{align}\n",
    "\\mathbf{c}_t &= \\mathbf{i}_f \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_i \\odot \\mathbf{f}^m (\\mathbf{W}^f_c \\mathbf{x}_t + \\mathbf{W}^r_c \\mathbf{h}_{t-1} + \\mathbf{b}_c) \\\\\n",
    "\\mathbf{h}_t &= \\mathbf{i}_o \\odot \\mathbf{f}^h (\\mathbf{c}_t)\n",
    "\\end{align}\n",
    "\n",
    ">* Memory cell weight matrices $\\mathbf{W}^m$: diagonal\n",
    "\n",
    "* **Residual Networks**\n",
    "\n",
    "><img src='images/image08.png' width=200>\n",
    "\n",
    ">$$\\mathbf{y}(\\mathbf{x}) = \\mathcal{F}(\\mathbf{x}) + \\mathbf{x}$$\n",
    "\n",
    "* **Highway Connections**\n",
    "\n",
    "><img src='images/image15.png' width=200>\n",
    "\n",
    ">\\begin{align}\n",
    "\\mathbf{i}_h &= \\boldsymbol{\\sigma} (\\mathbf{W}^f_l \\mathbf{x}_t + \\mathbf{W}^r_l \\mathbf{h}_{t-1} + \\mathbf{b}_l) \\\\\n",
    "\\mathbf{h}_t &= \\mathbf{i}_h \\odot \\mathbf{f} (\\mathbf{W}^f_h \\mathbf{x}_t + \\mathbf{W}^r_h \\mathbf{h}_{t-1} + \\mathbf{b}_h) + (\\mathbf{1} - \\mathbf{i}_h) \\odot \\mathbf{x}_t\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Input Sequence to Target\n",
    "\n",
    "* **Averaging**\n",
    "\n",
    "><img src='images/image16.png' width=300>\n",
    "\n",
    ">$$\\mathbf{h}_t = \\mathcal{F}(\\mathbf{x}_t) \\;\\;\\;\\rightarrow\\;\\;\\;\n",
    "\\mathbf{c} = \\frac{1}{T} \\sum^T_{t=1} \\mathbf{h}_t \\;\\;\\;\\rightarrow\\;\\;\\;\n",
    "\\mathbf{h} = \\mathcal{F}(\\mathbf{c}) \\;\\;\\;\\rightarrow\\;\\;\\;\n",
    "\\mathbf{y} = \\mathcal{F}(\\mathbf{h})$$\n",
    "\n",
    "* **RNN Encoding** (Sequence Embedding)\n",
    "\n",
    "><img src='images/image17.png' width=300>\n",
    "\n",
    ">$$\\mathbf{h}_t = \\mathcal{F}(\\mathbf{x}_t,\\mathbf{h}_{t-1}) \\;\\;\\;\\rightarrow\\;\\;\\;\n",
    "\\mathbf{h} = \\mathcal{F}(\\mathbf{h}_T)$$\n",
    "\n",
    ">* Use bi-directional information to avoid focusing on later inputs ($\\mathbf{h} = \\mathcal{F}(\\mathbf{h}_T,\\tilde{\\mathbf{h}}_1)$)\n",
    "\n",
    "* **Attention Mechanism** (yields a **pmf** over the sequence)\n",
    "\n",
    "><img src='images/image18.png' width=400>\n",
    "\n",
    ">$$\\tilde{\\mathbf{h}} = \\mathcal{F}(\\mathbf{k}) \\;\\;\\;\\rightarrow\\;\\;\\;\n",
    "e_t = \\mathcal{F} (\\tilde{\\mathbf{h}},\\mathbf{h}_t) \\;\\;\\;\\rightarrow\\;\\;\\;\n",
    "\\alpha_t = \\frac{\\exp(e_t)}{\\sum^T_{i=1} \\exp(e_i)} \\;\\;\\;\\rightarrow\\;\\;\\;\n",
    "\\mathbf{c} = \\sum^T_{t=1} \\alpha_t \\mathbf{h}_t$$\n",
    "\n",
    ">* **Form of Attention Mechanism** (i.e. how relevant is that observation to the key)\n",
    ">  * **Dot-product:** $e_t = \\mathbf{h}^T_t \\mathbf{W}_{xk} \\tilde{\\mathbf{h}}$\n",
    ">  * **Additive:** $e_t = \\mathbf{w}^T \\tanh (\\mathbf{W}_x \\mathbf{h}_t + \\mathbf{W}_k \\tilde{\\mathbf{h}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Sequence to Sequence\n",
    "\n",
    "* **Encoder-Decoder Sequence Models**\n",
    "\n",
    ">\\begin{align}\n",
    "p(\\mathbf{y}_{1:K}|\\mathbf{x}_{1:L}) &= \\prod^K_{i=1} p(\\mathbf{y}_i |\\mathbf{y}_{1:i-1}, \\mathbf{x}_{1:L}) \\\\\n",
    "&\\approx \\prod^K_{i=1} p(\\mathbf{y}_i|\\mathbf{y}_{i-1},\\tilde{\\mathbf{h}}_{i-1},\\mathbf{c})\n",
    "\\end{align}\n",
    "\n",
    ">* Map $\\mathbf{x}_{1:L}$ to a fixed-length vector $\\mathbf{c} \\rightarrow \\mathbf{c} = \\phi(\\mathbf{x}_{1:L})$\n",
    ">* **RNN Encoder-Decoder Model:** $\\mathbf{c} = \\phi(\\mathbf{x}_{1:L}) = \\mathbf{h}_L$\n",
    ">  * **Limitation:** context-dependence is global\n",
    "\n",
    "><img src='images/image19.png' width=400>\n",
    "\n",
    "* **Attention-Based Models** (introduce **attention layer**)\n",
    "\n",
    "><img src='images/image20.png' width=400>\n",
    "\n",
    ">$$p(\\mathbf{y}_{1:K}|\\mathbf{x}_{1:L}) \\approx \\prod^K_{i=1} p(\\mathbf{y}_i | \\mathbf{y}_{i-1}, \\tilde{\\mathbf{h}}_{i-1}, \\mathbf{c}_i) \\approx \\prod^K_{i=1} p(\\mathbf{y}_i | \\tilde{\\mathbf{h}}_i)$$\n",
    "\n",
    ">$$e_{i\\tau} = \\mathcal{F}(\\tilde{\\mathbf{h}}_{i-1}, \\mathbf{h}_\\tau) \\;\\;\\;\\rightarrow\\;\\;\\;\n",
    "\\alpha_{i\\tau} = \\frac{\\exp(e_{i\\tau})}{\\sum^L_{j=1} \\exp(e_{ij})} \\;\\;\\;\\rightarrow\\;\\;\\;\n",
    "\\mathbf{c}_i = \\sum^L_{\\tau=1} \\alpha_{i\\tau} \\mathbf{h}_{\\tau}$$\n",
    "\n",
    ">* $e_{i\\tau}$: how well position $i-1$ in output matches position $\\tau$ in input\n",
    "\n",
    "* **Inference for NMT**\n",
    "\n",
    ">$$P(w_i|\\hat{w}_{0:i-1},w_{1:L}^{(s)}) \\approx P(w_i|\\hat{w}_{0:i-1},\\mathbf{x}_{1:L}) \\approx P(w_i|\\hat{\\mathbf{y}}_{0:i-1},\\mathbf{c}_{1:i}) \\approx P(w_i|\\tilde{\\mathbf{h}}_i)$$\n",
    "\n",
    ">1. Embed previous word $\\hat{w}_{i-1} \\rightarrow \\hat{\\mathbf{y}}_{i-1}$\n",
    ">1. Compute context information $\\mathbf{c}_i = \\mathcal{F} (\\mathbf{x}_{1:L}, \\tilde{\\mathbf{h}}_{i-1})$\n",
    ">1. Generate new history vector $\\tilde{\\mathbf{h}}_i = \\mathcal{F} (\\tilde{\\mathbf{h}}_{i-1}, \\hat{\\mathbf{y}}_{i-1},\\mathbf{c}_i)$\n",
    ">1. Given $\\tilde{\\mathbf{h}}_i$, compute pmf over embedding space $\\mathbf{y}$\n",
    ">1. Draw $\\hat{w}_i$ from pmf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
