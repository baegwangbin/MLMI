{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Probability of Error & Decision Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Bayes' Decision Rule\n",
    "\n",
    "* **Expected Loss**\n",
    "\n",
    ">$$\\mathcal{L}_{\\text{act}} = \\int \\Big[ \\sum^K_{i=1} \\mathcal{L} (f(\\mathbf{x},\\boldsymbol{\\theta}),\\omega_i) P(\\omega_i|\\mathbf{x}) \\Big] p(\\mathbf{x})d\\mathbf{x}$$\n",
    "\n",
    ">* $f(\\mathbf{x},\\boldsymbol{\\theta})$: prediction given model parameters\n",
    "\n",
    "* **Empirical Loss**\n",
    "\n",
    ">$$\\mathcal{L}_{\\text{eval}} = \\frac{1}{N} \\sum^N_{i=1} \\mathcal{L} \\big( f(\\mathbf{x}_i,\\boldsymbol{\\theta}),y_i \\big)$$\n",
    "\n",
    ">* Computed using **held-out** evaluation set\n",
    ">* $y_i \\in \\{\\omega_1,...,\\omega_K\\}$: labels\n",
    ">* As $N\\rightarrow \\infty$, $\\mathcal{L}_{\\text{eval}} \\rightarrow \\mathcal{L}_{\\text{act}}$\n",
    "\n",
    "* **Bayes' Decision Rule** \n",
    "\n",
    ">* BDR: make decision that minimizes loss (i.e. probability of error)\n",
    ">* Assume that $\\mathcal{L}(\\hat{\\omega},\\omega_i) = 0$ if $\\hat{\\omega}=\\omega_i$ and $1$ otherwise\n",
    "\n",
    ">\\begin{align}\n",
    "\\hat{\\omega} &= f(\\mathbf{x}^\\star, \\boldsymbol{\\theta}) \\\\\n",
    "&= \\text{argmin}_\\omega \\bigg\\{ \\sum^K_{i=1} \\mathcal{L}(\\omega,\\omega_i) P(\\omega_i|\\mathbf{x}^\\star) \\bigg\\} \\\\\n",
    "&= \\text{argmax}_\\omega \\bigg\\{ P(\\omega|\\mathbf{x}^\\star) \\bigg\\}\n",
    "\\end{align}\n",
    "\n",
    ">* $P(\\omega|\\mathbf{x}^\\star)$: **classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Classifier\n",
    "\n",
    "* **Types of Classifiers**\n",
    "\n",
    ">* **Generative Models:** model the joint distribution $p(\\mathbf{x},\\omega;\\boldsymbol{\\theta})$\n",
    ">  * Posterior: obtained from **Bayes' rule**\n",
    "\n",
    ">$$P(\\omega_i|\\mathbf{x}^\\star;\\boldsymbol{\\theta}) = \\frac{p(\\mathbf{x}^\\star,\\omega_i;\\boldsymbol{\\theta})}{\\sum^K_{j=1} p(\\mathbf{x}^\\star,\\omega_j;\\boldsymbol{\\theta})}$$\n",
    "\n",
    ">* **Discriminative Models:** model the posterior $P(\\omega|\\boldsymbol{x}^\\star;\\boldsymbol{\\theta})$\n",
    "\n",
    ">* **Discriminant Functions:** model the mapping directly (no posterior)\n",
    "\n",
    "* **Binary Classification** (assume $\\omega_1 = 1, \\omega_2 = -1$)\n",
    "\n",
    ">* **Empirical Loss**\n",
    "\n",
    ">$$\\mathcal{L}_{\\text{eval}} = P(\\text{error}) = \\frac{1}{2N} \\sum^N_{i=1} |f(\\mathbf{x}_i,\\boldsymbol{\\theta})-y_i|$$\n",
    "\n",
    ">* **True Probability of Error**\n",
    "\n",
    "><img src=\"images/image01.png\" width=250>\n",
    "\n",
    ">\\begin{align}\n",
    "P(\\text{error}) &= P(\\mathbf{x}\\in\\Omega_2,\\omega_1) + P(\\mathbf{x}\\in\\Omega_1,\\omega_2) \\\\\n",
    "&= P(\\mathbf{x}\\in\\Omega_2|\\omega_1)P(\\omega_1) + P(\\mathbf{x}\\in\\Omega_1|\\omega_2)P(\\omega_2) \\\\\n",
    "&= \\int_{\\Omega_2} p(\\mathbf{x}|\\omega_1)P(\\omega_1)d\\mathbf{x} + \\int_{\\Omega_1} p(\\mathbf{x}|\\omega_2)P(\\omega_2)d\\mathbf{x} \n",
    "\\end{align}\n",
    "\n",
    ">* **$P(\\text{error})$ marginalizes over joint distribution**\n",
    "\n",
    ">$$P(\\text{error}) = \\int_{\\Omega_2} p(\\mathbf{x},\\omega_1)d\\mathbf{x} + \\int_{\\Omega_1} p(\\mathbf{x},\\omega_2)d\\mathbf{x}$$\n",
    "\n",
    ">* **Generative Model**\n",
    "\n",
    ">$$P(\\text{error}) = \\int_{\\Omega_2} p(\\mathbf{x}|\\omega_1)P(\\omega_1)d\\mathbf{x} + \\int_{\\Omega_1} p(\\mathbf{x}|\\omega_2)P(\\omega_2)d\\mathbf{x} $$\n",
    "\n",
    ">* **Discriminative Model**\n",
    "\n",
    ">$$P(\\text{error}) = \\int_{\\Omega_2} P(\\omega_1|\\mathbf{x})p(\\mathbf{x})d\\mathbf{x} + \\int_{\\Omega_1} P(\\omega_2|\\mathbf{x})p(\\mathbf{x})d\\mathbf{x} $$\n",
    "\n",
    "* **Unequal Loss Function** \n",
    "\n",
    ">* **Loss**\n",
    "\n",
    ">\\begin{align}\n",
    "\\mathcal{L}(f(\\mathbf{x}^\\star,\\boldsymbol{\\theta})=\\omega_2,\\omega_1) &= \\mathcal{C}_{21} \\\\ \\mathcal{L}(f(\\mathbf{x}^\\star,\\boldsymbol{\\theta})=\\omega_1,\\omega_2) &= \\mathcal{C}_{12}\n",
    "\\end{align}\n",
    "\n",
    ">* **Bayes' Decision Rule**\n",
    "\n",
    ">$$\\hat{\\omega} = \\text{argmin}_\\omega \\bigg\\{ \\sum^2_{i=1} \\mathcal{L}(\\omega,\\omega_i) P(\\omega_i|\\mathbf{x}^\\star;\\boldsymbol{\\theta})\\bigg\\}$$\n",
    "\n",
    ">$$\\frac{P(\\omega_1|\\mathbf{x}^\\star;\\boldsymbol{\\theta})}{P(\\omega_2|\\mathbf{x}^\\star;\\boldsymbol{\\theta})} = \\frac{\\mathcal{C}_{12}}{\\mathcal{C}_{21}} \\rightarrow \\text{operating threshold}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Parameter Estimation\n",
    "\n",
    "* **MLE - Maximum Likelihood Estimation**\n",
    "\n",
    ">* **Supervised:**\n",
    "\n",
    ">\\begin{align}\n",
    "\\boldsymbol{\\theta} &= \\text{argmax}_\\boldsymbol{\\theta} \\bigg\\{ \\log P(y_1,...,y_N|\\mathbf{x}_1,...,\\mathbf{x}_N;\\boldsymbol{\\theta}) \\bigg\\} \\\\\n",
    "&= \\text{argmax}_\\boldsymbol{\\theta} \\bigg\\{ \\sum^N_{i=1} \\log P(y_i|\\mathbf{x}_i;\\boldsymbol{\\theta}) \\bigg\\}\n",
    "\\end{align}\n",
    "\n",
    ">* **Unsupervised:**\n",
    "\n",
    ">$$\\boldsymbol{\\theta} = \\text{argmax}_\\boldsymbol{\\theta} \\bigg\\{ \\sum^N_{i=1} \\log p(\\mathbf{x}_i;\\boldsymbol{\\theta}) \\bigg\\}$$\n",
    "\n",
    "* **Generative Models**\n",
    "\n",
    ">$$P(\\omega_i|\\mathbf{x}^\\star) \\approx \\frac{p(\\mathbf{x}^\\star,\\omega_i;\\boldsymbol{\\theta})}{\\sum^K_{j=1} p(\\mathbf{x}^\\star,\\omega_j;\\boldsymbol{\\theta})} = \\frac{p(\\mathbf{x}^\\star|\\omega_i;\\boldsymbol{\\theta})P(\\omega_i)}{\\sum^K_{j=1} p(\\mathbf{x}^\\star|\\omega_j;\\boldsymbol{\\theta})P(\\omega_j)}$$\n",
    "\n",
    ">$$\\hat{\\boldsymbol{\\theta}}_i = \\text{argmax}_{\\boldsymbol{\\theta}} \\bigg\\{ \\sum_{j:y_j=\\omega_i} \\log p(\\mathbf{x}_j|\\omega_i;\\boldsymbol{\\theta}) \\bigg\\}$$\n",
    "\n",
    "* **Multivariate Gaussian Class Conditional PDFs**\n",
    "\n",
    ">$$p(\\mathbf{x}|\\omega_i;\\boldsymbol{\\theta}_i) = \\mathcal{N}(\\mathbf{x};\\boldsymbol{\\mu}_i,\\boldsymbol{\\Sigma}_i) = \\frac{1}{(2\\pi)^{\\frac{d}{2}} |\\boldsymbol{\\Sigma}_i|^{\\frac{1}{2}}} \\exp \\left( -\\frac{1}{2} (\\mathbf{x}-\\boldsymbol{\\mu}_i)^T \\boldsymbol{\\Sigma}_i^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_i) \\right)$$\n",
    "\n",
    ">\\begin{align}\n",
    "\\hat{\\boldsymbol{\\mu}}_i &= \\frac{\\sum_{j:y_j=\\omega_i} \\mathbf{x}_j}{\\sum_{j:y_j=\\omega_i} 1} \\\\\n",
    "\\hat{\\boldsymbol{\\Sigma}}_i &= \\frac{\\sum_{j:y_j=\\omega_i} (\\mathbf{x}_j-\\hat{\\boldsymbol{\\mu}}_i)(\\mathbf{x}_j-\\hat{\\boldsymbol{\\mu}}_i)^T}{\\sum_{j:y_j=\\omega_i} 1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Decision Boundary\n",
    "\n",
    "* **Binary Classification**\n",
    "\n",
    ">* Boundary occurs when class posteriors are the same\n",
    "\n",
    ">$$\\log P(\\omega_1|\\mathbf{x};\\boldsymbol{\\theta}) = \\log P(\\omega_2|\\mathbf{x};\\boldsymbol{\\theta})$$\n",
    "\n",
    ">* For generative classifier:\n",
    "\n",
    ">$$\\log (P(\\omega_1)p(\\mathbf{x}|\\omega_1;\\boldsymbol{\\theta}_1)) = \\log (P(\\omega_2)p(\\mathbf{x}|\\omega_2;\\boldsymbol{\\theta}_2))$$\n",
    "\n",
    "* **Multivariate Gaussian**\n",
    "\n",
    ">* General: **Hyper-quadratic** decision boundary\n",
    ">* $\\boldsymbol{\\Sigma}_1=\\boldsymbol{\\Sigma}_2$ $\\rightarrow$ **Linear** decision boundary\n",
    "\n",
    ">$$\\mathbf{x}^T\\mathbf{Ax} + \\mathbf{b}^T\\mathbf{x} + c = 0$$\n",
    "\n",
    ">* $\\mathbf{A} = \\boldsymbol{\\Sigma}^{-1}_1 - \\boldsymbol{\\Sigma}^{-1}_2$\n",
    ">* $\\mathbf{b} = 2(\\boldsymbol{\\Sigma}^{-1}_2 \\boldsymbol{\\mu}_2 - \\boldsymbol{\\Sigma}^{-1}_1 \\boldsymbol{\\mu}_1)$\n",
    ">* $c=\\boldsymbol{\\mu}^T_1 \\boldsymbol{\\Sigma}^{-1}_1 \\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}^T_2 \\boldsymbol{\\Sigma}^{-1}_2 \\boldsymbol{\\mu}_2 - \\log \\left( \\frac{|\\boldsymbol{\\Sigma}_2|}{|\\boldsymbol{\\Sigma}_1|} \\right) - 2\\log \\left( \\frac{P(\\omega_1)}{P(\\omega_2)} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Graphical Models and Conditional Independence\n",
    "\n",
    "* **Structured distribution:** written as a product of simpler factors $\\rightarrow$ **CI**\n",
    "* **Graphical Models:** nodes are random variables / edges connect variables for which no CI exist\n",
    "* **Inference in Graphical Models:** use factorization to reduce computational cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Bayesian Networks \n",
    "\n",
    "* **Bayesian Networks:** directed acyclic graph\n",
    "\n",
    "* **Factorization**\n",
    "\n",
    ">$$p(X_1,...,X_d) = \\prod^d_{i=1} p(X_i|PA^\\mathcal{G}_{X_i})$$\n",
    ">* $PA^\\mathcal{G}_{X_i}$: parents of $X_i$ in $\\mathcal{G}$\n",
    "\n",
    "* **Conditional Independencies**\n",
    "\n",
    ">$$X_i \\perp ND^\\mathcal{G}_{X_i} | PA^\\mathcal{G}_{X_i}$$\n",
    ">* $ND^\\mathcal{G}_{X_i}$: non-descendants of $X_i$ in $\\mathcal{G}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Markov Networks\n",
    "\n",
    "* **Markov Networks:** undirected graphical models\n",
    "\n",
    "* **Gaussian with Sparse Precision Matrix** ($\\lambda_{i,j} \\neq 0$ for $(i,j)\\in\\mathcal{E}$)\n",
    "\n",
    ">\\begin{align}\n",
    "p(X_1,...,X_d) &= \\frac{1}{\\sqrt{2\\pi|\\boldsymbol{\\Sigma}|}} \\exp \\{ (X_1,...,X_d)^T \\boldsymbol{\\Sigma}^{-1} (X_1,...,X_d) \\} \\\\\n",
    "&\\propto \\exp \\bigg\\{ -\\frac{1}{2} \\sum_{(i,j)\\in \\mathcal{E}} \\lambda_{i,j} X_i X_j\\bigg\\} = \\prod_{(i,j)\\in \\mathcal{E}} \\exp \\bigg\\{ -\\frac{1}{2} \\lambda_{i,j} X_i X_j \\bigg\\}\n",
    "\\end{align}\n",
    "\n",
    "* **Positive Potential Functions** and **Cliques**\n",
    "\n",
    ">$$\\phi_1(\\mathbf{D}_1),...,\\phi_k(\\mathbf{D}_k)$$\n",
    "\n",
    ">* $\\mathbf{D}_i$: forms a clique of $\\mathcal{G}$\n",
    ">* **Clique:** fully connected subset of nodes\n",
    "\n",
    "* **Factorization**\n",
    "\n",
    ">$$p(X_1,...,X_d) = Z^{-1} \\prod^k_{i=1} \\phi_i (\\mathbf{D}_i)$$\n",
    "\n",
    ">* **Partition function** (or normalizing constant): $Z = \\sum_{X_{1:d}} \\prod^k_{i=1} \\phi_i (\\mathbf{D}_i)$\n",
    "\n",
    "* **Conditional Independencies**\n",
    "\n",
    ">$$A \\perp B | C$$\n",
    ">* Such that $C$ separates $A$ from $B$ in $\\mathcal{G}$ (i.e. $C$ blocks all paths in $\\mathcal{G}$ between $A$ and $B$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Latent Variable and Sequence Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Latent Variable Models\n",
    "\n",
    "* **Latent Variables**\n",
    "\n",
    ">* Do not have to have any meaning\n",
    ">* Never observed in test / possibly in training\n",
    ">* Marginalized over to get probabilities\n",
    "\n",
    "><img src=\"images/image02.png\" width=400>\n",
    "\n",
    ">* **Discrete (mixture models, HMMs):** $\\sum^M_{m=1} P(c_m)P(\\mathbf{x}|c_m)$\n",
    ">* **Continuous (factor-analysis):** $\\int p(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z})d\\mathbf{z}$\n",
    "\n",
    "* **Gaussian Mixture Models**\n",
    "\n",
    ">$$p(\\mathbf{x}) = \\sum^M_{m=1} P(c_m)p(\\mathbf{x}|c_m) = \\sum^M_{m=1} P(c_m) \\mathcal{N}(\\mathbf{x};\\boldsymbol{\\mu}_m,\\boldsymbol{\\Sigma}_m)$$\n",
    "\n",
    ">* component **prior** $\\times$ component **distribution**\n",
    "\n",
    "* **Factor Analysis**\n",
    "\n",
    ">* Can be viewed as: (1) Low-dim manifold representation or (2) compact covariance matrix for multivariate Gaussians\n",
    "\n",
    ">$$p(\\mathbf{x}) = \\int p(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z})d\\mathbf{z} = \\mathcal{N}(\\mathbf{x};\\mathbf{0},\\mathbf{CC}^T + \\boldsymbol{\\Sigma}_{\\text{diag}})$$\n",
    "\n",
    ">* $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z};\\mathbf{0},\\mathbf{I})$: low-dim subspace representation\n",
    ">* $p(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mathbf{x};\\mathbf{Cz},\\boldsymbol{\\Sigma}_{\\text{diag}})$ where $\\mathbf{C}$: loading matrix / $\\boldsymbol{\\Sigma}_{\\text{diag}}$: diagonal covariance matrix\n",
    ">* If $\\boldsymbol{\\Sigma}_{\\text{diag}} = \\sigma^2 \\mathbf{I}$ $\\rightarrow$ probabilistic PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Expectation Maximization\n",
    "\n",
    "* **Simple EM for GMM**\n",
    "\n",
    ">* Make initial guess of the parameters $\\boldsymbol{\\theta}^{(0)}$\n",
    ">* Repeat\n",
    ">  * Assign each observation to a component using **Bayes' decision rule**\n",
    ">  * Update the parameters $\\boldsymbol{\\theta}^{(k+1)}$\n",
    "\n",
    "* **Jensen's Inequality**\n",
    "\n",
    ">$$f \\left( \\sum^M_{m=1} \\lambda_m x_m \\right) \\geq \\sum^M_{m=1} \\lambda_m f(x_m)$$\n",
    "\n",
    ">* $f(\\cdot)$: any concave function & $\\sum_m \\lambda_m = 1$\n",
    "\n",
    "* **EM for Discrete Latent Variables**\n",
    "\n",
    ">\\begin{align}\n",
    "\\mathcal{L}(\\boldsymbol{\\theta}^{(k+1)}) - \\mathcal{L}(\\boldsymbol{\\theta}^{(k)}) &= \\sum^N_{i=1} \\log \\left( \\frac{p(\\mathbf{x}_i;\\boldsymbol{\\theta}^{(k+1)})}{p(\\mathbf{x}_i;\\boldsymbol{\\theta}^{(k)})} \\right) \\\\\n",
    "&= \\sum^N_{i=1} \\log \\left( \\frac{1}{p\\left(\\mathbf{x}_i|\\boldsymbol{\\theta}^{(k)}\\right)} \\sum^M_{m=1} \\left( p(\\mathbf{x}_i,c_m|\\boldsymbol{\\theta}^{(k+1)})\\right)\\right) \\\\\n",
    "&= \\sum^N_{i=1} \\log \\left( \\frac{1}{p\\left(\\mathbf{x}_i|\\boldsymbol{\\theta}^{(k)}\\right)} \\sum^M_{m=1} \\left( \\frac{P(c_m|\\mathbf{x}_i,\\boldsymbol{\\theta}^{(k)}) p(\\mathbf{x}_i,c_m|\\boldsymbol{\\theta}^{(k+1)})}{P(c_m|\\mathbf{x}_i,\\boldsymbol{\\theta}^{(k)})} \\right)\\right) \\\\\n",
    "&\\geq \\sum^N_{i=1} \\sum^M_{m=1} P(c_m|\\mathbf{x}_i,\\boldsymbol{\\theta}^{(k)}) \\log \\left( \\frac{p(\\mathbf{x}_i,c_m|\\boldsymbol{\\theta}^{(k+1)})}{p\\left(\\mathbf{x}_i|\\boldsymbol{\\theta}^{(k)}\\right)P(c_m|\\mathbf{x}_i,\\boldsymbol{\\theta}^{(k)})} \\right) \\\\\n",
    "&= \\mathcal{Q}(\\boldsymbol{\\theta}^{(k)},\\boldsymbol{\\theta}^{(k+1)}) - \\mathcal{Q}(\\boldsymbol{\\theta}^{(k)},\\boldsymbol{\\theta}^{(k)})\n",
    "\\end{align}\n",
    "\n",
    "* **Auxiliary Function**\n",
    "\n",
    ">$$\\mathcal{Q}(\\boldsymbol{\\theta}^{(k)},\\boldsymbol{\\theta}^{(k+1)}) = \\sum^N_{i=1} \\sum^M_{m=1} P(c_m|\\mathbf{x}_i,\\boldsymbol{\\theta}^{(k)}) \\log \\left(p(\\mathbf{x}_i,c_m|\\boldsymbol{\\theta}^{(k+1)}) \\right)$$\n",
    "\n",
    "* **Continuous Auxiliary Functions**\n",
    "\n",
    ">$$\\mathcal{Q}(\\boldsymbol{\\theta}^{(k)},\\boldsymbol{\\theta}^{(k+1)}) = \\int p\\left(\\mathbf{Z}|\\mathbf{X},\\boldsymbol{\\theta}^{(k)}\\right) \\log \\left( p\\left(\\mathbf{X},\\mathbf{Z}|\\boldsymbol{\\theta}^{(k+1)}\\right) \\right) d\\mathbf{Z}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Hidden Markov Models\n",
    "\n",
    "* **Discrete Kalman Filters**\n",
    "\n",
    "><img src=\"images/image03.png\" width=300>\n",
    "\n",
    ">$$\\mathbf{z}_t = \\mathbf{Az}_{t-1} + \\boldsymbol{\\nu}_t \\;\\;\\;,\\;\\;\\; \\mathbf{x}_t = \\mathbf{Cz}_t + \\boldsymbol{\\epsilon}_t$$\n",
    "\n",
    ">* $\\boldsymbol{\\nu}_t \\sim \\mathcal{N}(\\mathbf{0},\\boldsymbol{\\Sigma}_\\boldsymbol{\\nu})$ and $\\boldsymbol{\\epsilon}_t \\sim \\mathcal{N}(\\mathbf{0},\\boldsymbol{\\Sigma}_\\boldsymbol{\\epsilon})$ \n",
    "\n",
    ">\\begin{align}\n",
    "p(\\mathbf{x}_t|\\mathbf{x}_{1:t-1}) &= \\int p(\\mathbf{x}_t|\\mathbf{z}_t) p(\\mathbf{z}_t|\\mathbf{x}_{1:t-1}) d\\mathbf{z}_t \\\\\n",
    "&= \\int p(\\mathbf{x}_t|\\mathbf{z}_t) \\int p(\\mathbf{z}_t|\\mathbf{z}_{t-1}) p(\\mathbf{z}_{t-1}|\\mathbf{x}_{1:t-1}) d\\mathbf{z}_{t-1} d\\mathbf{z}_t \\\\\n",
    "&= \\int p(\\mathbf{x}_t|\\mathbf{z}_t) \\int p(\\mathbf{z}_t|\\mathbf{z}_{t-1}) \\frac{p(\\mathbf{x}_{t-1}|\\mathbf{z}_{t-1}) p(\\mathbf{z}_{t-1}|\\mathbf{x}_{1:t-2})}{p(\\mathbf{x}_{t-1}|\\mathbf{x}_{1:t-2})} d\\mathbf{z}_{t-1} d\\mathbf{z}_t\n",
    "\\end{align}\n",
    "\n",
    "* **Hidden Markov Models**\n",
    "\n",
    "><img src=\"images/image04.png\" width=250>\n",
    "\n",
    ">* $q_t$: latent variables / discrete state-space\n",
    ">* States - emitting or non-emitting\n",
    ">* **Conditional independence:** $P(q_t|q_{0:t-1}) = P(q_t|q_{t-1})$ and $p(\\mathbf{x}_t|\\mathbf{x}_{1:t-1}, q_{0:t}) = p(\\mathbf{x}_t|q_t)$\n",
    "\n",
    "* **Likelihood**\n",
    "\n",
    ">$$p(\\mathbf{x}_{1:T}) = \\sum_{\\mathbf{q}\\in \\mathbf{Q}_T} P(\\mathbf{q})p(\\mathbf{x}_{1:T}|\\mathbf{q}) = \\sum_{\\mathbf{q}\\in \\mathbf{Q}_T} P(q_0) \\prod^T_{t=1} P(q_t|q_{t-1})p(\\mathbf{x}_t|q_t)$$\n",
    "\n",
    "* **Parameters**\n",
    "\n",
    ">* **Transition matrix:** $\\mathbf{A}$ $\\rightarrow$ $a_{ij}=P(q_t=s_j|q_{t-1}=s_i)$\n",
    ">* **State output probability:** $b_j(\\mathbf{x}_t) = p(\\mathbf{x}_t|q_t=s_j)$\n",
    ">* Parameters usually estimated using **EM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Viterbi Algorithm\n",
    "\n",
    "* **Viterbi Algorithm**\n",
    "\n",
    ">$$p(\\mathbf{x}_{1:T}) = \\sum_{\\mathbf{q}\\in\\mathbf{Q}_T} p(\\mathbf{x}_{1:T},\\mathbf{q}) \\approx p(\\mathbf{x}_{1:T},\\hat{\\mathbf{q}})$$\n",
    "\n",
    ">* $\\hat{\\mathbf{q}} = \\text{argmax}_{\\mathbf{q}\\in\\mathbf{Q}_T} p(\\mathbf{x}_{1:T},\\mathbf{q})$\n",
    ">* Method: **extend partial paths in time** OR **best partial path to a state/time**\n",
    ">* **Total cost:** log sum the costs of all paths - $\\log(\\exp(a)+\\exp(b))$\n",
    "\n",
    "* **Formulation**\n",
    "\n",
    ">* **Initialization**\n",
    ">  * $\\phi_1(0) = 0.0$, $\\phi_j(0) = \\log (0)$, $\\phi_1(t) = \\log (0)$ for any $t$\n",
    "\n",
    ">* **Recursion**\n",
    ">  * for $t=1,...,T$ / for $j=2,...,N-1$\n",
    ">  * $\\phi_j(t) = \\max_{1\\leq k < N} \\{ \\phi_k (t-1) + \\log(a_{kj}) \\} + \\log (b_j(\\mathbf{x}_t))$\n",
    "\n",
    ">* **Termination**\n",
    ">  * $\\log (p(\\mathbf{x}_{1:T},\\hat{\\mathbf{q}})) = \\max_{1 < k < N} \\{ \\phi_k(T) + \\log(a_{kN}) \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Forward-Backward Algorithm\n",
    "\n",
    "* **Forward Probability**\n",
    "\n",
    ">\\begin{align}\n",
    "\\alpha_j(t) &= \\log(p(\\mathbf{x}_{1:t}, q_t=s_j)) \\\\\n",
    "&= \\log \\left( \\sum^N_{k=1} \\exp \\left( \\alpha_k(t-1) + \\log(a_{kj}) \\right) \\right) + \\log (b_j(\\mathbf{x}_t))\n",
    "\\end{align}\n",
    "\n",
    "* **Backward Probability**\n",
    "\n",
    ">\\begin{align}\n",
    "\\beta_j(t) &= \\log (p(\\mathbf{x}_{t+1:T}|q_t=s_j)) \\\\\n",
    "&= \\log \\left( \\sum^N_{k=1} \\exp \\left( \\beta_k(t+1) + \\log(a_{kj}) + \\log(b_k(\\mathbf{x}_{t+1})) \\right) \\right)\n",
    "\\end{align}\n",
    "\n",
    "* **Posterior**\n",
    "\n",
    ">\\begin{align}\n",
    "P(q_t=s_j|\\mathbf{x}_{1:T}) &= \\frac{\\exp (\\alpha_j(t)+\\beta_j(t))}{Z} \\\\\n",
    "Z &= \\sum^N_{i=1} \\exp(\\alpha_i(t) + \\beta_i(t))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Conditional Random Fields\n",
    "\n",
    "* **Maximum Entropy Markov Model**\n",
    "\n",
    ">\\begin{align}\n",
    "P(q_{0:T}|\\mathbf{x}_{1:T}) &= \\prod^T_{t=1} P(q_t|q_{t-1},\\mathbf{x}_t) \\\\\n",
    "P(q_t|q_{t-1},\\mathbf{x}_t) &= \\frac{1}{Z_t} \\exp \\left( \\sum^D_{i=1} \\lambda_i f_i (q_t,q_{t-1},\\mathbf{x}_t) \\right)\n",
    "\\end{align}\n",
    "\n",
    ">* **Extend to complete sequence**\n",
    "\n",
    ">$$P(q_{0:T}|\\mathbf{x}_{1:T}) = \\frac{1}{Z} \\exp \\left( \\sum^D_{i=1} \\lambda_i f_i (q_{0:T},\\mathbf{x}_{1:T}) \\right)$$\n",
    "\n",
    "* **Simple Linear Chain CRF**\n",
    "\n",
    "><img src=\"images/image05.png\" width=250>\n",
    "\n",
    ">$$P(q_{0:T}|\\mathbf{x}_{1:T}) = \\frac{1}{Z} \\exp \\left( \\sum^T_{t=1} \\left( \\sum^{D_t}_{i=1} \\lambda_i^t f_i (q_t,q_{t-1}) + \\sum^{D_a}_{i=1} \\lambda^a_i f_i (q_t,\\mathbf{x}_t) \\right) \\right)$$\n",
    "\n",
    ">* $D_t$: # transition style features with parameters $\\boldsymbol{\\lambda}^t$\n",
    ">* $D_a$: # acoustic style features with parameters $\\boldsymbol{\\lambda}^a$\n",
    ">* Directly related to unnormalized HMM parameters\n",
    "\n",
    "* **Linear Chain CRF**\n",
    "\n",
    "><img src=\"images/image06.png\" width=250>\n",
    "\n",
    ">$$P(q_{0:T}|\\mathbf{x}_{1:T}) = \\frac{1}{Z} \\exp \\left( \\sum^T_{t=1} \\left( \\sum^D_{i=1} \\lambda_i f_i (q_t,q_{t-1},\\mathbf{x}_t) \\right) \\right)$$\n",
    "\n",
    ">* Features similar to general MEMM / but normalized globally, not locally\n",
    "\n",
    "* **Normalization Term**\n",
    "\n",
    ">$$Z = \\sum_{\\mathbf{q} \\in \\mathbf{Q}_T} \\exp \\left( \\sum^T_{t=1} \\left( \\sum^{D_t}_{t=1} \\lambda_i^t f_i (q_t,q_{t-1}) + \\sum^{D_a}_{i=1} \\lambda^a_i f_i (q_t,\\mathbf{x}_t) \\right) \\right)$$\n",
    "\n",
    ">* Use equivalent of forward-backward algorithm\n",
    "\n",
    "* **General Sequence CRF**\n",
    "\n",
    ">* Undirected graph repeated each tim instance - set of cliques\n",
    "\n",
    ">$$P(q_{0:T}|\\mathbf{x}_{1:T}) = \\frac{1}{Z} \\exp \\left( \\sum^T_{t=1} \\sum_{\\mathcal{C} \\in \\mathbf{C}} \\boldsymbol{\\lambda}^T_{\\mathcal{C}} \\mathbf{f}(\\mathbf{q}_{\\mathcal{C}t},\\mathbf{x}_{1:T},t) \\right)$$\n",
    "\n",
    ">* $\\boldsymbol{\\lambda}^T_{\\mathcal{C}}$: time-independent parameters associated with clique $\\mathcal{C}$\n",
    ">* $\\mathbf{f}(\\mathbf{q}_{\\mathcal{C}t},\\mathbf{x}_{1:T},t)$: time-dependent features extracted from clique $\\mathcal{C}$ with time-dependent label sequence $\\mathbf{q}_{\\mathcal{C}t}$\n",
    "\n",
    "* **Simple Example**\n",
    "\n",
    "><img src=\"images/image07.png\" width=250>\n",
    "\n",
    ">\\begin{align}\n",
    "P(q_{0:T}|\\mathbf{x}_{1:T}) &= \\frac{1}{Z} \\exp \\left( \\sum^T_{t=1} \\sum_{\\mathcal{C} \\in \\mathbf{C}} \\boldsymbol{\\lambda}^T_{\\mathcal{C}} \\mathbf{f} (\\mathbf{q}_{\\mathcal{Ct}},\\mathbf{x}_{1:T},t) \\right) \\\\\n",
    "&= \\frac{1}{Z} \\exp \\left( \\sum^T_{t=1} \\left( \\boldsymbol{\\lambda}^{tT} \\mathbf{f} (q_t,q_{t-1}) + \\boldsymbol{\\lambda}^{aT} \\mathbf{f}(q_t,\\mathbf{x}_t) \\right) \\right)\n",
    "\\end{align}\n",
    "\n",
    ">* **Parameter Estimation** (fully observed, no need for EM)\n",
    ">* $\\hat{\\boldsymbol{\\lambda}} = \\text{argmax}_{\\boldsymbol{\\lambda}} \\{P(y_{1:T}|\\mathbf{x}_{1:T},\\boldsymbol{\\lambda})\\} $\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
