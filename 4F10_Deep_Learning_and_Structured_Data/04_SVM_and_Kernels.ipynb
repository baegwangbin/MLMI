{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Maximum Margin Classifier\n",
    "\n",
    "* **Decision Border** and **Linear Separability**\n",
    "\n",
    ">* Given a dataset $\\mathcal{D} = \\{(\\mathbf{x}_n,t_n)\\}^N_{n=1}$ where $t_n \\in \\{-1,1\\}$, \n",
    ">* The goal is to train a classifer such that\n",
    "\n",
    ">$$\\begin{matrix} y(\\mathbf{x}) \\geq 0 & \\text{if  } t_n = +1 \\\\ y(\\mathbf{x}) < 0 & \\text{if  } t_n = -1 \\end{matrix}$$\n",
    "\n",
    ">* **Decision Border:** $y(\\mathbf{x}) = 0$  \n",
    ">  * If the data is **linearly separable**, many possible borders have zero training error\n",
    ">  * Linear classifier: $y(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b$\n",
    "\n",
    "* **Maximum Margin Classifiers**\n",
    "\n",
    ">* **Idea:** choose a plane whose distance (i.e. **marging**) to the closest point (i.e. **support vectors**) in each class is maximal\n",
    "\n",
    ">* Choose the scale so that:\n",
    "\n",
    ">\\begin{align}\n",
    "y(\\mathbf{x}_+) &= \\mathbf{w}^T \\mathbf{x}_+ + b = +1 \\\\\n",
    "y(\\mathbf{x}_-) &= \\mathbf{w}^T \\mathbf{x}_- + b = -1 \n",
    "\\end{align}\n",
    "\n",
    ">* **Magnitude of the margin:**\n",
    "\n",
    ">$$\\frac{\\mathbf{w}^T (\\mathbf{x}_+ - \\mathbf{x}_-)}{2||\\mathbf{w}||} = \\frac{1}{||\\mathbf{w}||}$$\n",
    "\n",
    ">* Minimum $|y(\\mathbf{x})|$ achieved by support vectors $\\rightarrow$ $t_n y(\\mathbf{x}_n) \\geq 1 \\;\\forall\\;n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Optimization with Inequality Constraints\n",
    "\n",
    "* **Problem:** Maximize $f(x,y)$ subject to $g(x,y) \\geq 0$\n",
    "\n",
    "* **Equality Constraint $\\rightarrow$ Lagrange Multipliers**\n",
    "\n",
    ">$$\\text{Maximize}\\;\\;\\;\\mathcal{L}(x,y,\\lambda) = f(x,y) + \\lambda g(x,y)$$\n",
    "\n",
    ">$$\\rightarrow \\nabla_{x,y} f(x,y) = -\\lambda \\nabla_{x,y} g(x,y) \\;\\;\\;,\\;\\;\\; g(x,y)=0$$\n",
    "\n",
    ">* $\\lambda \\neq 0$: **Lagrange multiplier** (positive or negative)\n",
    "\n",
    "* **Karush-Kuhn-Tuker (KKT) Conditions**\n",
    "\n",
    ">* Constraint is **not active** at solution: $g(x_\\star, y_\\star) > 0$\n",
    "\n",
    ">>\\begin{align}\n",
    "\\nabla_{x,y} \\; f(x,y) &= 0 \\\\\n",
    "\\nabla_{x,y} \\; \\mathcal{L} (x,y,\\lambda) &= 0 \\;\\;\\; \\text{if} \\;\\;\\; \\lambda=0\n",
    "\\end{align}\n",
    "\n",
    ">* Constraint is **active** at solution: $g(x_\\star, y_\\star) = 0$\n",
    "\n",
    ">>$$\\nabla_{x,y} \\; f(x,y) = -\\lambda \\nabla_{x,y} \\; g(x,y)$$\n",
    "\n",
    ">>* $\\lambda$ must be positive / otherwise the constraint wouldn't be tight\n",
    ">>* **Solution:**\n",
    "\n",
    ">>$$\\max_{x,y} \\min_\\lambda \\mathcal{L}(x,y,\\lambda)$$\n",
    "\n",
    ">>* This solution satisfies the **KKT conditions**\n",
    "\n",
    ">>$$g(x,y) \\geq 0 \\;\\;\\;,\\;\\;\\; \\lambda \\geq 0 \\;\\;\\;,\\;\\;\\; \\lambda g(x,y) = 0$$\n",
    ">>$$\\nabla_{x,y} \\; f(x,y) = -\\lambda \\nabla_{x,y} \\; g(x,y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Optimization in SVM\n",
    "\n",
    "* **Optimization Problem**\n",
    "\n",
    ">$$\\text{Minimize} \\;\\;\\; \\frac{1}{2} ||\\mathbf{w}||^2 \\;\\;\\; \\text{s.t.} \\;\\;\\;\n",
    "t_n(\\mathbf{w}^T \\mathbf{x}_n + b) \\geq 1 \\;\\;\\;,\\;\\;\\; n=1,...,N$$\n",
    "\n",
    "* **Objective fn.** (introduce Lagrange multipliers)\n",
    "\n",
    ">$$\\mathcal{L} (\\mathbf{w},b,\\mathbf{a}) = \\frac{1}{2} ||\\mathbf{w}||^2 \n",
    "- \\sum^N_{n=1} a_n \\{ t_n (\\mathbf{w}^T \\mathbf{x}_n + b) - 1 \\}$$\n",
    "\n",
    ">* Negative sign in $a_n$: because we will maximize w.r.t. $a_n$\n",
    ">* By setting gradients w.r.t. $\\mathbf{w}$ and $b$ to zero:\n",
    "\n",
    ">$$\\mathbf{w}=\\sum^N_{n=1} a_n t_n \\mathbf{x}_n \\;\\;\\;,\\;\\;\\;\n",
    "0 = \\sum^N_{n=1} a_n t_n$$\n",
    "\n",
    ">* Substitute these results to yield a **dual problem**:\n",
    "\n",
    ">$$\\max_\\mathbf{a} \\left[ \\sum^N_{n=1} a_n - \\frac{1}{2} \\sum^N_{n=1} \\sum^N_{m=1} a_n a_m t_n t_m \\mathbf{x}_n^T \\mathbf{x}_m \\right]\n",
    "\\;\\;\\;\\text{s.t.}\\;\\;\\; \\sum^N_{n=1} a_n t_n = 0 \n",
    "\\;\\;\\;,\\;\\;\\; a_n \\geq 0 \\;\\forall\\; n$$\n",
    "\n",
    "* **Solution** (no analytic solution, too expensive for $N>50,000$)\n",
    "\n",
    ">* From KKT conditions, at convergence, it holds that:\n",
    "\n",
    ">$$a_n(t_n y(\\mathbf{x}_n) - 1)=0 \\;\\forall\\;n$$\n",
    "\n",
    ">$$a_n > 0 \\;\\;\\;\\rightarrow\\;\\;\\; t_n y(\\mathbf{x}_n) = 1 \\;\\;\\;\\text{(i.e. support vectors)}$$\n",
    "\n",
    ">* **Prediction**\n",
    "\n",
    ">$$y(\\mathbf{x}) = \\sum_{n \\in \\mathcal{S}} a_n t_n \\mathbf{x}_n^T \\mathbf{x} + b\n",
    "\\;\\;\\;,\\;\\;\\; \\mathcal{S} = \\{n: a_n > 0\\}$$\n",
    "\n",
    ">* **Bias** (average to improve numerical stability)\n",
    "\n",
    ">$$b = \\frac{1}{|\\mathcal{S}|} \\sum_{n\\in\\mathcal{S}} \\bigg\\{\n",
    "t_n - \\sum_{m \\in \\mathcal{S}} a_m t_m \\mathbf{x}_m^T \\mathbf{x}_n\n",
    "\\bigg\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Constraint Violation and Soft Margin\n",
    "\n",
    "* **Slack Variables**\n",
    "\n",
    ">$$t_n (\\mathbf{w}^T \\mathbf{x}_n + b) \\geq 1 - \\xi_n \n",
    "\\;\\;\\;,\\;\\;\\; \\xi_n \\geq 0 \\;\\;\\;,\\;\\;\\; n = 1,...,N$$\n",
    "\n",
    ">* $\\xi_n = 0$: correctly classified & outside the margin (or on the border)\n",
    ">* $0 \\leq \\xi_n \\leq 1$: correctly classified & inside the margin\n",
    ">* $\\xi_n > 1$: misclassified\n",
    "\n",
    "* **Optimization for Soft Margin**\n",
    "\n",
    ">$$\\text{Minimize} \\;\\;\\; \\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum^N_{n=1} \\xi_n\n",
    "\\;\\;\\; \\text{s.t.} \\;\\;\\;\n",
    "t_n(\\mathbf{w}^T \\mathbf{x}_n + b) \\geq 1 - \\xi_n \n",
    "\\;\\;\\;,\\;\\;\\; \\xi_n \\geq 0$$\n",
    "\n",
    ">* $C>0$: controls the **trade-off**\n",
    ">  * Small $C$: soft contraint / large margin\n",
    ">  * Large $C$: hard constraint / narrow margin \n",
    "\n",
    "* **Objective fn.** ($a_n \\geq 0$ and $\\mu_n \\geq 0$: Lagrange multipliers)\n",
    "\n",
    ">$$\\mathcal{L} (\\mathbf{w},b,\\mathbf{a}) = \\frac{1}{2} ||\\mathbf{w}||^2\n",
    "+ C \\sum^N_{n=1} \\xi_n\n",
    "- \\sum^N_{n=1} a_n \\{ t_n (\\mathbf{w}^T \\mathbf{x}_n + b) - 1 + \\xi_n\\}\n",
    "- \\sum^N_{n=1} \\mu_n \\xi_n$$\n",
    "\n",
    ">* By setting gradients w.r.t. $\\mathbf{w}$, $b$ and $\\xi_n$ to zero:\n",
    "\n",
    ">$$\\mathbf{w}=\\sum^N_{n=1} a_n t_n \\mathbf{x}_n \\;\\;\\;,\\;\\;\\;\n",
    "0 = \\sum^N_{n=1} a_n t_n \\;\\;\\;,\\;\\;\\;\n",
    "\\{a_n = C - \\mu_n \\}^N_{n=1}$$\n",
    "\n",
    ">* Substitute these results to yield a **dual problem**:\n",
    "\n",
    ">$$\\max_\\mathbf{a} \\left[ \\sum^N_{n=1} a_n - \\frac{1}{2} \\sum^N_{n=1} \\sum^N_{m=1} a_n a_m t_n t_m \\mathbf{x}_n^T \\mathbf{x}_m \\right]\n",
    "\\;\\;\\;\\text{s.t.}\\;\\;\\; \\sum^N_{n=1} a_n t_n = 0 \n",
    "\\;\\;\\;,\\;\\;\\; \\{ 0 \\leq a_n \\leq C\\}^N_{n=1}$$\n",
    "\n",
    "* **Solution**\n",
    "\n",
    ">* From KKT conditions, at convergence, it holds that:\n",
    "\n",
    ">$$a_n(t_n y(\\mathbf{x}_n) - 1 + \\xi_n)=0 \\;\\forall\\;n$$\n",
    "\n",
    ">$$a_n > 0 \\;\\;\\;\\rightarrow\\;\\;\\; t_n y(\\mathbf{x}_n) = 1 - \\xi_n \\;\\;\\;\\text{(i.e. support vectors)}$$\n",
    "\n",
    ">* $0 < a_n < C \\rightarrow \\mu_n > 0$ and $\\xi_n = 0$: lies on the border\n",
    ">* $a_n = C$: lie inside the margin\n",
    ">  * Correctly classified if $\\xi_n \\leq 1$ / Misclassified if $\\xi_n > 1$\n",
    "\n",
    ">* **Prediction**\n",
    "\n",
    ">$$y(\\mathbf{x}) = \\sum_{n \\in \\mathcal{S}} a_n t_n \\mathbf{x}_n^T \\mathbf{x} + b\n",
    "\\;\\;\\;,\\;\\;\\; \\mathcal{S} = \\{n: a_n > 0\\}$$\n",
    "\n",
    ">* **Bias**\n",
    "\n",
    ">$$b = \\frac{1}{|\\mathcal{M}|} \\sum_{n\\in\\mathcal{M}} \\bigg\\{\n",
    "t_n - \\sum_{m \\in \\mathcal{S}} a_m t_m \\mathbf{x}_m^T \\mathbf{x}_n\n",
    "\\bigg\\}\n",
    "\\;\\;\\;,\\;\\;\\;\n",
    "\\mathcal{M} = \\{ n: 0 < a_n < C \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Advanced Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Non-linear Max-margin Classifiers\n",
    "\n",
    "* **Non-linear Basis Function**\n",
    "\n",
    ">$$y(\\mathbf{x}) = \\mathbf{w}^T \\boldsymbol{\\phi}(\\mathbf{x}) + b$$\n",
    "\n",
    ">$$\\boldsymbol{\\phi}(\\mathbf{x}_n) = (\\phi_1 (\\mathbf{x}_n) ,..., \\phi_M (\\mathbf{x}_n))^T$$\n",
    "\n",
    ">* Example: **Gaussian Basis**\n",
    "\n",
    ">$$\\phi_m (\\mathbf{x}) = \\exp \\left\\{ \n",
    "-\\frac{1}{2s} (\\mathbf{x} - \\mathbf{c}_m)^T(\\mathbf{x} - \\mathbf{c}_m)\n",
    "\\right\\}$$\n",
    "\n",
    "* **Optimization and Predictions**\n",
    "\n",
    ">$$\\max_\\mathbf{a} \\left[ \\sum^N_{n=1} a_n - \\frac{1}{2} \\sum^N_{n=1} \\sum^N_{m=1} a_n a_m t_n t_m \\boldsymbol{\\phi}(\\mathbf{x}_n)^T \\boldsymbol{\\phi}(\\mathbf{x}_m) \\right]$$\n",
    "\n",
    ">$$y(\\mathbf{x}) = \\sum_{n \\in \\mathcal{S}}\n",
    "a_n t_n \\boldsymbol{\\phi}(\\mathbf{x}_n)^T \\boldsymbol{\\phi}(\\mathbf{x}) + b$$\n",
    "\n",
    "* **Gram Matrix,** $\\mathbf{K}$\n",
    "\n",
    ">$$k_{n,m} = \\boldsymbol{\\phi}(\\mathbf{x}_n)^T \\boldsymbol{\\phi}(\\mathbf{x}_m)$$\n",
    "\n",
    ">* Mapping & dot product $\\rightarrow$ expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Kernel Functions\n",
    "\n",
    "* **Kernel Functions**\n",
    "\n",
    ">$$k_{n,m} = k(\\mathbf{x}_n,\\mathbf{x}_m) = \\boldsymbol{\\phi}(\\mathbf{x}_n)^T \\boldsymbol{\\phi}(\\mathbf{x}_m)$$\n",
    "\n",
    ">* Example: \n",
    "\n",
    ">\\begin{align}\n",
    "\\boldsymbol{\\phi}(\\mathbf{x}) &= \\left[ 1, \\sqrt{2}\\;x_1, \\sqrt{2}\\;x_2, \\sqrt{2}\\;x_1x_2, x^2_1, x^2_2 \\right]^T \\\\\n",
    "\\\\\n",
    "k(\\mathbf{x}_n,\\mathbf{x}_m) &= \\boldsymbol{\\phi}(\\mathbf{x}_n)^T \\boldsymbol{\\phi}(\\mathbf{x}_m) \\\\\n",
    "&= 1 + 2 x_{n,1} x_{m,1} + 2 x_{n,2} x_{m,2} + 2 x_{n,1} x_{n,2} x_{m,1} x_{m,2}\n",
    "+ x^2_{n,1} x^2_{m,1} + x^2_{n,2} x^2_{m,2} \\\\\n",
    "&= (1 + x_{n,1}x_{m,1} + x_{n,2}x_{m,2})^2 \\\\\n",
    "&= (1 + \\mathbf{x}^T_n \\mathbf{x}_m)\n",
    "\\end{align}\n",
    "\n",
    ">* $k(\\cdot,\\cdot)$ is valid iff there is $\\phi(\\cdot)$ s.t. $k(\\mathbf{x}_n,\\mathbf{x}_m) = \\boldsymbol{\\phi}(\\mathbf{x}_n)^T \\boldsymbol{\\phi}(\\mathbf{x}_m)$\n",
    "\n",
    "* **Other Examples**\n",
    "\n",
    ">\\begin{align}\n",
    "\\text{Linear: } \\;\\;\\; k(\\mathbf{x}_n,\\mathbf{x}_m) &= \\mathbf{x}_n^T \\mathbf{x}_m \\\\\n",
    "\\text{Polynomial: } \\;\\;\\; k(\\mathbf{x}_n,\\mathbf{x}_m) &= (1 + \\mathbf{x}_n^T \\mathbf{x}_m)^d \\\\\n",
    "\\text{Gaussian: } \\;\\;\\; k(\\mathbf{x}_n,\\mathbf{x}_m) &= \\exp \\left( - \\frac{1}{2s} ||\\mathbf{x}_n - \\mathbf{x}_m||^2 \\right)\n",
    "\\end{align}\n",
    "\n",
    "* **Mercer's Condition:** $k(\\cdot,\\cdot)$ is a valid kernel iff\n",
    "\n",
    ">1. $k(\\cdot,\\cdot)$ is symmetric - i.e. $k(\\mathbf{x}_n, \\mathbf{x}_m) = k(\\mathbf{x}_m, \\mathbf{x}_n)$\n",
    ">2. Any gram matrix $K$ is positive semi-definite\n",
    "\n",
    ">$$\\mathbf{g}^T \\mathbf{Kg} = \\sum_{n,m} g_n k_{n,m} g_m \\geq 0\n",
    "\\;\\;\\;\\forall\\;\\;\\; \\mathbf{g} \\text{ and } \\{\\mathbf{x_n}\\}^N_{n=1}$$\n",
    "\n",
    "* **Example with Gaussian Kernel**\n",
    "\n",
    ">$$k(\\mathbf{x}_n,\\mathbf{x}_m) = \\exp \\left( - \\frac{1}{2s} ||\\mathbf{x}_n - \\mathbf{x}_m||^2 \\right) \\;\\;\\;\\text{and}\\;\\;\\; C = \\infty$$\n",
    "\n",
    ">* The rank of $\\mathbf{K} = \\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^T$ determines the **effective dimension** of the feature space\n",
    ">  * $s \\rightarrow 0$: becomes diagonal (i.e. rank=N) $\\rightarrow$ wiggly\n",
    ">  * $s \\rightarrow \\infty$: all entries in $\\mathbf{K}$ are the same (i.e. rank=1) $\\rightarrow$ smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Kernel Trick\n",
    "\n",
    "* **Kernel Trick**\n",
    "\n",
    ">* Any algorithm that operates on the inputs $\\mathbf{x}_1,...,\\mathbf{x}_N$ by using only their **dot products** can be implemented using kernels\n",
    "\n",
    "* **Kernel Least Squares**\n",
    "\n",
    ">\\begin{align}\n",
    "C &= \\frac{1}{2} (\\boldsymbol{\\Phi} \\mathbf{w} - \\mathbf{t})^T (\\boldsymbol{\\Phi} \\mathbf{w} - \\mathbf{t}) + \\frac{\\lambda}{2} \\mathbf{w}^T \\mathbf{w} \\\\\n",
    "&= \\frac{1}{2} \\mathbf{w}^T \\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi} \\mathbf{w}\n",
    "+ \\frac{1}{2} \\mathbf{t}^T\\mathbf{t}\n",
    "- \\mathbf{t}^T \\boldsymbol{\\Phi} \\mathbf{w}\n",
    "+ \\frac{\\lambda}{2} \\mathbf{w}^T \\mathbf{w}\n",
    "\\end{align}\n",
    "\n",
    ">* Gradient is 0 if:\n",
    "\n",
    ">$$\\mathbf{w} = \\boldsymbol{\\Phi}^T \\frac{(\\mathbf{t}-\\boldsymbol{\\Phi} \\mathbf{w})}{\\lambda} \\equiv \\boldsymbol{\\Phi}^T \\mathbf{a}$$\n",
    "\n",
    ">* Use this to replace $\\mathbf{w}$:\n",
    "\n",
    ">$$C = \\frac{1}{2} \\mathbf{a}^T \\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^T \\mathbf{a} + \\frac{1}{2} \\mathbf{t}^T \\mathbf{t} - \\mathbf{t}^T \\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^T \\mathbf{a} + \\frac{\\lambda}{2} \\mathbf{a}^T \\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^TA \\mathbf{a}$$\n",
    "\n",
    ">* Gradient is 0 if:\n",
    "\n",
    ">$$\\mathbf{a} = (\\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^T + \\lambda \\mathbf{I})^{-1} \\mathbf{t} = (\\mathbf{K} + \\lambda \\mathbf{I})^{-1} \\mathbf{t}$$\n",
    "\n",
    ">* **Prediction**\n",
    "\n",
    ">$$y(\\mathbf{x}) = \\mathbf{w}^T \\boldsymbol{\\phi}(\\mathbf{x})\n",
    "= \\mathbf{a}^T \\boldsymbol{\\Phi} \\;\\boldsymbol{\\phi}(\\mathbf{x})\n",
    "= \\sum^N_{n=1} a_n k(\\mathbf{x}_n,\\mathbf{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Multi-class Max-margin Classifiers\n",
    "\n",
    "* **One-vs-all Classification** (one classifier per class)\n",
    "\n",
    ">$$\\hat{t}_\\star = \\underset{k \\in \\{k_1,...,k_N\\}}{\\text{argmax}} \\mathbf{w}_k^T \\mathbf{x}_\\star + b_k$$\n",
    "\n",
    ">* **Problems:** Different output scales / Different # training examples\n",
    "\n",
    "* **Simultaneous Learning of Classifiers** (very expensive)\n",
    "\n",
    ">* Force the following condition\n",
    "\n",
    ">$$\\mathbf{w}^T_{t_n} \\mathbf{x}_n + b_{t_n} > \\mathbf{w}^T_{j} \\mathbf{x}_n + b_{j} \\;\\;\\;\\forall\\;\\;\\; j \\neq t_n$$\n",
    "\n",
    ">* New objective fn.:\n",
    "\n",
    ">$$\\frac{1}{2} \\sum^K_{k=1} ||\\mathbf{w}_k||^2 + C \\sum^N_{n=1,j\\neq t_n} \\xi_{n,j}$$\n",
    "\n",
    ">$$\\text{s.t.} \\;\\;\\;\n",
    "\\mathbf{w}^T_{t_n} \\mathbf{x}_n + b_{t_n} \\geq \\mathbf{w}^T_{j} \\mathbf{x}_n + b_{j} + 1 - \\xi_{n.j} \\;\\;\\;,\\;\\;\\;\n",
    "\\xi_{n,j} \\geq 0 \\;\\;\\;\n",
    "\\forall \\;\\;\\;\n",
    "n, \\; j\\neq t_n\n",
    "$$\n",
    "\n",
    ">* Prediction implemented in the same way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Kernels for Structured Data\n",
    "\n",
    ">1. **Extract features from input objects**\n",
    "1. **Compute kernels (dot products) using those features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. String Kernels\n",
    "\n",
    "* **String Kernel**\n",
    "\n",
    ">1. Given a list of substrings $s_1,s_2,... \\in \\mathcal{A}^\\star$, encode $\\boldsymbol{\\phi}(x) = (\\phi_{s_1}(x), \\phi_{s_2}(x), ...)^T$\n",
    ">1. $\\phi_{s}(x)$: indicate the occurrence of $s$ in $x$\n",
    ">1. Kernel between two strings defined as dot product:\n",
    "\n",
    ">$$k(x,x') = \\boldsymbol{\\phi}(x)^T \\boldsymbol{\\phi}(x') \n",
    "= \\sum_{s\\in\\{s_1,s_2,...\\}} \\phi_s(x) \\phi_s(x')$$\n",
    "\n",
    "* **Example: Gap-weighted Kernel**\n",
    "\n",
    ">* $\\phi_{s}(x)$: # occurrences, $\\lambda^n$ for $n$ gaps\n",
    "\n",
    "* **Example: $k$-spectrum kernel**\n",
    "\n",
    ">* $\\phi_{s}(x)$: # **exact** occurrences ($s$: any possible length-$k$ sequence)\n",
    ">* Selection of $k$\n",
    ">  * Large $k$: co-occurrence is more informative\n",
    ">  * Small $k$: # co-occurrence increases\n",
    ">  * $k=1$: **Bag-of-words kernel**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Tree Kernels\n",
    "\n",
    "* **Definitions**\n",
    "\n",
    ">* **Subtree:** tree formed by selecting one node an all its descendants\n",
    ">* **Subset Tree:** subtree including all children of a node or none of them\n",
    "\n",
    "* **Tree Kernel**\n",
    "\n",
    ">1. Given a list of all possible subset trees $t_1,t_2,...$, each tree $\\mathcal{T}$ is encoded using the feature vector $\\boldsymbol{\\phi}(\\mathcal{T}) = (\\phi_{t_1}(\\mathcal{T}), \\phi_{t_2}(\\mathcal{T}), ...)^T$\n",
    ">2. Each $\\phi_{t}(\\mathcal{T})$ counts occurrences of $t$ in $\\mathcal{T}$ ($\\mathcal{V}(\\mathcal{T})$: set of nodes in $\\mathcal{T}$)\n",
    ">$$$$\n",
    ">$$\\phi_t (\\mathcal{T}) = \\sum_{n \\in \\mathcal{V}(\\mathcal{T})} I_t (n)\n",
    "\\;\\;\\;,\\;\\;\\;\n",
    "I_t(n) = \\bigg\\{ \\begin{matrix} 1 & t \\text{ found in } \\mathcal{T} \\text{ with root at node } n \\\\ 0 & \\text{otherwise} \\end{matrix} $$\n",
    ">$$$$\n",
    ">3. Kernel between two trees defined as dot product:\n",
    "\n",
    ">$$k(\\mathcal{T}_a,\\mathcal{T}_b) = \\boldsymbol{\\phi}(\\mathcal{T}_a)^T \\boldsymbol{\\phi}(\\mathcal{T}_b) = \\sum_{t \\in \\{t_1,t_2,...\\}} \\phi_t(\\mathcal{T}_a) \\phi_t(\\mathcal{T}_b)$$\n",
    "\n",
    "* **Efficient Computation of Tree Kernels and Example**\n",
    "\n",
    ">\\begin{align}\n",
    "k(\\mathcal{T}_a,\\mathcal{T}_b) &= \\sum_{n_a \\in \\mathcal{V}(\\mathcal{T}_a)}\n",
    "\\sum_{n_b \\in \\mathcal{V}(\\mathcal{T}_b)} f(n_a, n_b) \\\\\n",
    "\\\\\n",
    "f(n_a,n_b) &= \\sum_{t \\in \\{t_1,t_2,...\\}} I_t(n_a) I_t(n_b)\n",
    "\\end{align}\n",
    "\n",
    ">* $f(n_a,n_b)$: # common subset trees at $n_a$ and $n_b$\n",
    "\n",
    ">|$\\hspace{40mm}$Condition|                         Then|\n",
    "|-|-|\n",
    "|$n_a \\neq n_b$ **or** $\\text{ch}(n_a) \\neq \\text{ch}(n_b)$|$f(n_a,n_b)=0$|\n",
    "|else if $n_a$ and $n_b$ are leaf nodes|$f(n_a,n_b)=1$|\n",
    "|otherwise|$f(n_a,n_b)=\\prod^{|\\text{ch}(n_a)|}_{i=1} g(\\text{ch}(n_a)_i,\\text{ch}(n_b)_i)$<br/><br/>$g(n_1,n_2) = \\bigg\\{ \\begin{matrix} 1 & \\text{if } n_1 \\text{ or } n_2 \\text{ leaf} \\\\ 1+f(n_1,n_2) & \\text{otherwise} \\end{matrix}$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Graph Kernels\n",
    "\n",
    "* **Graphs and Graph Walks**\n",
    "\n",
    ">* **Graph:** $\\mathcal{G}=\\{ \\mathcal{V},\\mathcal{E} \\}$ where $\\mathcal{E} = \\{ (i,j);i,j\\in\\mathcal{V} \\}$\n",
    ">* **Graph Walks:** $k$-length walk defined as $w=\\{v_1,...,v_{k+1}\\}$ where $(v_i,v_{i+1}) \\in \\mathcal{E}$\n",
    ">* Number of $k$-length walks between nodes $i$ and $j$:\n",
    "\n",
    ">$$[\\mathbf{A}^k]_{i,j} = \\sum^{|\\mathcal{V}|}_{s_1 = 1} \\cdots \\sum^{|\\mathcal{V}|}_{s_{k-1} = 1} a_{i,s_1} a_{s_1,s_2} \\cdot a_{s_{k-1},j}$$\n",
    "\n",
    "* **Random-walk Graph Kernel**\n",
    "\n",
    ">* $k(\\mathcal{G},\\mathcal{G}')$: # common walks in the two graphs\n",
    ">* Computed using **direct product graph**\n",
    "\n",
    ">\\begin{align}\n",
    "\\mathcal{G}_\\times &= (\\mathcal{V}_\\times, \\mathcal{E}_\\times) \\\\\n",
    "\\mathcal{V}_\\times &= \\{ (a,a') ; a\\in\\mathcal{V} \\text{ and } a'\\in\\mathcal{V}' \\} \\\\\n",
    "\\mathcal{E}_\\times &= \\{ ((a,a'),(b,b')) ; (a,b)\\in\\mathcal{E} \\text{ and } (a',b')\\in\\mathcal{E}' \\}\n",
    "\\end{align}\n",
    "\n",
    ">* **Kernel Definition** ($\\mathbf{A}_\\times = \\mathbf{A} \\otimes \\mathbf{A}'$ where $\\otimes$ is the **Kronecker product**)\n",
    "\n",
    ">$$k(\\mathcal{G},\\mathcal{G}') = \\sum^{|\\mathcal{V}_\\times|}_{i,j = 1}\n",
    "\\left[ \\sum^\\infty_{n=0} \\lambda^n \\mathbf{A}^n_\\times \\right]_{i,j}\n",
    "= \\mathbf{1}^T [\\mathbf{I} - \\lambda \\mathbf{A} \\otimes \\mathbf{A}']^{-1} \\mathbf{1}\n",
    "\\Leftrightarrow \\mathbf{x} = \\mathbf{1} + \\lambda (\\mathbf{A} \\otimes \\mathbf{A}')\\mathbf{x}$$\n",
    "\n",
    ">* **Problems:** \n",
    ">  * A walk can visit the same cycle multiple times $\\rightarrow$ small structural similarities can produce huge kernel values\n",
    ">  * High cost, $\\mathcal{O}(n^3)$\n",
    "\n",
    "* **Weisfeiler-Lehman Graph Kernel**\n",
    "\n",
    ">1. Create a set with labels of adjacent vertices & sort\n",
    ">1. Add vertex label as a prefix\n",
    ">1. Compress resulting label sequence into a **unique value**\n",
    ">1. Assign the unique value as new **vertex label**\n",
    ">1. Apply the **bag-of-words** kernel to the vertex labels (include the initial labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Fisher Kernel\n",
    "\n",
    "* **Idea**\n",
    "\n",
    ">* Use a **probabilistic generative model** to obtain a **fixed-length vector representation** of complex structured data\n",
    "\n",
    "* **Steps**\n",
    "\n",
    ">1. Train $p(\\mathbf{x}|\\boldsymbol{\\theta})$ on $\\{\\mathbf{x}_n\\}^N_{n=1}$ (e.g. using MLE)\n",
    ">1. Define the **Fisher score vector** as $\\boldsymbol{\\phi}(\\mathbf{x}_n) = \\nabla_{\\boldsymbol{\\theta}} \\log p(\\mathbf{x}_n|\\boldsymbol{\\theta}) |_{\\boldsymbol{\\theta}_{\\text{MLE}}}$\n",
    ">1. Define **naive Fisher kernel** as $k(\\mathbf{x}_n,\\mathbf{x}_m) = \\boldsymbol{\\phi}(\\mathbf{x}_n)^T \\boldsymbol{\\phi}(\\mathbf{x}_m)$\n",
    "\n",
    "* **Example: Mixture of Gaussians**\n",
    "\n",
    ">\\begin{align}\n",
    "p(\\mathbf{x}|\\boldsymbol{\\theta}) &= \\sum^K_{k=1} p(\\mathbf{x}|\\theta_k)\\pi_k \\\\\n",
    "[\\boldsymbol{\\phi}(\\mathbf{x}_n)]_{pi_k} &= \\frac{\\partial \\log p(\\mathbf{x}|\\boldsymbol{\\theta})}{\\partial \\pi_k} \\bigg|_{\\boldsymbol{\\theta}=\\boldsymbol{\\theta}_{\\text{MLE}}}\n",
    "= \\frac{p(\\mathbf{x}|\\theta^{\\text{MLE}}_k)}{\\sum_k p(\\mathbf{x}|\\theta^{\\text{MLE}}_k) \\pi^{\\text{MLE}}_k}\n",
    "\\end{align}\n",
    "\n",
    ">* $[\\boldsymbol{\\phi}(\\mathbf{x}_n)]_{pi_k}$: amount by which the $k$-th component contributes to generate $\\mathbf{x}_n$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
