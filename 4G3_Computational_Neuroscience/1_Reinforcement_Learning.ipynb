{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Markov Decision Process\n",
    "\n",
    "><img src = 'images/image1_01.png' width=200>\n",
    "\n",
    ">* **$R_t$: long-term total reward**\n",
    "\n",
    ">\\begin{align}\n",
    "\\text{Finite Horizon: } R_t &= \\sum^\\infty_{\\tau=0} r_{t+\\tau} \\\\\n",
    "\\text{Infinite Horizon: } R_t &= \\sum^\\infty_{\\tau=0} \\gamma^\\tau r_{t+\\tau} \\;\\;\\; \\gamma \\text{: discounting factor}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Open-Loop vs. Closed-Loop Control\n",
    "\n",
    ">\\begin{align}\n",
    "\\text{Open-Loop: } a^*_{t}(s_{t}) &= \\underset{a_{t}}{\\text{argmax}} \\underset{a_{t+1}}{\\max} \\left[ \\bar{r}(s_t,a_t) + \\sum_{s_{t+1}} P(s_{t+1}|s_t,a_t) \\bar{r}(s_{t+1},a_{t+1}) \\right] \\\\\n",
    "\\text{Closed-Loop: } a^*_{t}(s_{t}) &= \\underset{a_{t}}{\\text{argmax}} \\left[ \\bar{r}(s_t,a_t) + \\sum_{s_{t+1}} P(s_{t+1}|s_t,a_t) \\underset{a_{t+1}}{\\max} \\bar{r}(s_{t+1},a_{t+1}) \\right]\n",
    "\\end{align}\n",
    "\n",
    ">* Open-Loop: choose actions in advance / naive optimisation\n",
    ">* Closed-Loop: correct recursion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Bellman Equations\n",
    "\n",
    "* **State Value** (infinite sum $\\rightarrow$ finite no. of linear equations using recursion)\n",
    "\n",
    ">\\begin{align}\n",
    "V^\\pi (s) &= \\mathbb{E}_\\pi [R_t|s_t=s] \\\\\n",
    "&= \\mathbb{E}_\\pi \\left[ r_t + \\gamma \\sum^\\infty_{\\tau=0} \\gamma^\\tau r_{t+\\tau+1} \\Bigg| s_t = s \\right] \\\\\n",
    "&= \\sum_a \\pi(s,a) \\left[ \\bar{r}(s,a) + \\sum_{s'} P(s'|s,a) \\gamma \\mathbb{E}_\\pi \\left[ \\sum^\\infty_{\\tau=0} \\gamma^\\tau r_{t+\\tau+1} \\Bigg| s_{t+1}=s' \\right] \\right] \\\\\n",
    "&= \\sum_a \\pi(s,a) \\left[ \\bar{r}(s,a) + \\sum_{s'} P(s'|s,a) \\gamma V^\\pi (s') \\right]\n",
    "\\end{align}\n",
    "\n",
    "* **Action value**\n",
    "\n",
    ">\\begin{align}\n",
    "Q^\\pi (s,a) &= \\mathbb{E}_\\pi [R_t|s_t=s,a_t=a] \\\\\n",
    "&= \\bar{r}(s,a) + \\sum_{s'} P(s'|s,a) \\gamma \\sum_{a'} \\pi(s',a') Q^\\pi (s',a') \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\n",
    "\\end{align}\n",
    "\n",
    "* **Policy Improvement Theorem**\n",
    "\n",
    ">$$\\alpha'(s) = \\alpha(s) \\;\\;\\; \\forall s \\;\\;\\; \\text{except} \\; s^*$$\n",
    "\n",
    ">$$Q^\\pi (s^*, \\alpha'(s^*)) \\geq V^\\pi (s^*) \\;\\;\\;\\Rightarrow\\;\\;\\; V^{\\pi'}(s) \\geq V^{\\pi}(s) \\;\\;\\; \\forall s$$\n",
    "\n",
    ">* Or go all the way greedy: $\\alpha'(s) = \\underset{a}{\\text{argmax}}\\;Q^\\pi(s,a) \\;\\;\\; \\forall s$\n",
    "\n",
    "* **Generalised Policy Iteration**\n",
    "\n",
    ">* **Policy evaluation** ($V \\rightarrow V^\\pi$) & **Policy improvment** ($\\pi \\rightarrow \\text{greedy}(V)$)\n",
    ">* Guaranteed to converge to global optimum\n",
    "\n",
    "* **Bellman Optimality Equation** (simpler way to find optimal policy)\n",
    "\n",
    ">\\begin{align}\n",
    "V^*(s) &= \\max_a \\left[ \\bar{r}(s,a) + \\sum_{s'} P(s'|s,a) \\gamma V^*(s') \\right] \\\\\n",
    "Q^*(s,a) &= \\bar{r}(s,a) + \\sum_{s'} P(s'|s,a) \\gamma \\max_{a'} Q^* (s',a')\n",
    "\\end{align}\n",
    "\n",
    ">* **Problem 1:** difficult to solve non-linear function ($\\max$)\n",
    ">* **Problem 2:** useful only when there is no information on transition & reward prob."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Different Types of Learning\n",
    "\n",
    "><img src = 'images/image1_02.png' width=400>\n",
    "\n",
    "* **0. Monte Carlo Learning**\n",
    "\n",
    ">* Step 1. **Collect experience** following some policy\n",
    ">* Step 2. **Estimate value fn.** by averaging returns over episodes\n",
    "\n",
    "* **1. Caching-based Learning** (online estimation & bootstrapping)\n",
    "\n",
    ">\\begin{align}\n",
    "\\text{TD: } \\hat{V}^\\pi (s_t) &\\leftarrow \\hat{V}^\\pi (s_t) + \\epsilon \\left[ r_t + \\gamma \\hat{V}^\\pi(s_{t+1}) - \\hat{V}^\\pi (s_t) \\right] \\\\\n",
    "\\text{Q-learning: } \\hat{Q}^* (s_t,a_t) &\\leftarrow \\hat{Q}^* (s_t,a_t) + \\epsilon \\left[ r_t + \\gamma \\max_{a'} \\hat{Q}^*(s_{t+1},a') - \\hat{Q}^* (s_t,a_t) \\right]\n",
    "\\end{align}\n",
    "\n",
    ">* **TD: on-policy,** policy decides the next state\n",
    ">* **Q-Learning: off-policy,** optimal state is visited\n",
    "\n",
    "* **2. Model-based Learning** \n",
    "\n",
    ">* Estimate $P(s'|s,a)$ and $\\bar{r}(s,a)$ from experience $\\rightarrow$ run DP to obtain values and policy\n",
    ">* Strictly speaking, it is the optimal method, but it is difficult to estimate the model parameters\n",
    "\n",
    "* **3. Episodic Memory**\n",
    "\n",
    ">* Store episodes sequences $rightarrow$ try to follow actions that ended well\n",
    ">* No computation of average \n",
    "  \n",
    "* **Learning Curve Comparison**  \n",
    "  \n",
    "><img src = 'images/image1_03.png' width=400>\n",
    "\n",
    ">* Model-based: **clever learning / hard control**\n",
    ">* Model-free: **dull learning / easy control**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Evidences in Brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Memory Systems**\n",
    "\n",
    ">* **Caching-based:** Implicit Memory - Procedural (skills and habits)\n",
    ">* **Model-based:** Explicit Memory - Facts\n",
    ">* **Episodic Memory:** Explicit Memory - Events\n",
    "\n",
    "* **Dopamine = TD Prediction Error**\n",
    "\n",
    "><img src = 'images/image1_04.png' width=400>\n",
    "\n",
    ">$$\\hat{V}^\\pi(s_t) \\leftarrow \\hat{V}^\\pi(s_t) + \\epsilon \\left[ r_t + \\hat{V}^\\pi (s_{t+1}) - \\hat{V}^\\pi(s_t) \\right]$$\n",
    "\n",
    ">* Striatum: responsible for procedural memory / produces **dopamine**\n",
    ">* Activity in **ventral tegmental area:** in accordance with **TD prediction error** ($\\delta = r_t + \\Delta V$)\n",
    ">* After learning, TD prediction error moves **from $t_{\\text{reward}}$ to $t_{\\text{stimulus}}$**\n",
    "\n",
    "* **Devaluation**\n",
    "\n",
    ">* **Model-based:** sensitive to devaluation\n",
    ">* **Caching-based:** insensitive to devaluation\n",
    ">  * *Effect of devaluation $\\rightarrow$ indicate whether the behaviour is under the control of model-based or caching-based learning*\n",
    "\n",
    "><img src = 'images/image1_05.png' width=400>\n",
    "\n",
    ">* Brain listens to the **system with less uncertainty**\n",
    ">* Complexity of the scenario $\\uparrow$ & No. of trials $\\uparrow$ $\\Rightarrow$ **caching-based learning** becomes better (habitization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
