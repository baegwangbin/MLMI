{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Markov Decision Process\n",
    "\n",
    "><img src = 'images/image1_01.png' width=200>\n",
    "\n",
    ">* **$R_t$: long-term total reward**\n",
    "\n",
    ">\\begin{align}\n",
    "\\text{Finite Horizon: } R_t &= \\sum^\\infty_{\\tau=0} r_{t+\\tau} \\\\\n",
    "\\text{Infinite Horizon: } R_t &= \\sum^\\infty_{\\tau=0} \\gamma^\\tau r_{t+\\tau} \\;\\;\\; \\gamma \\text{: discounting factor}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Open-Loop vs. Closed-Loop Control\n",
    "\n",
    ">\\begin{align}\n",
    "\\text{Open-Loop: } a^*_{t}(s_{t}) &= \\underset{a_{t}}{\\text{argmax}} \\underset{a_{t+1}}{\\max} \\left[ \\bar{r}(s_t,a_t) + \\sum_{s_{t+1}} P(s_{t+1}|s_t,a_t) \\bar{r}(s_{t+1},a_{t+1}) \\right] \\\\\n",
    "\\text{Closed-Loop: } a^*_{t}(s_{t}) &= \\underset{a_{t}}{\\text{argmax}} \\left[ \\bar{r}(s_t,a_t) + \\sum_{s_{t+1}} P(s_{t+1}|s_t,a_t) \\underset{a_{t+1}}{\\max} \\bar{r}(s_{t+1},a_{t+1}) \\right]\n",
    "\\end{align}\n",
    "\n",
    ">* Open-Loop: choose actions in advance / naive optimisation\n",
    ">* Closed-Loop: correct recursion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Bellman Equations\n",
    "\n",
    ">* **Policy** \n",
    "\n",
    ">$$\\pi(s',a') = P(a'|s')$$\n",
    "\n",
    ">* **State Value**\n",
    ">\\begin{align}\n",
    "V^\\pi (s) &= \\mathbb{E}_\\pi [R_t|s_t=s] \\\\\n",
    "&= \\mathbb{E}_\\pi \\left[ r_t + \\gamma \\sum^\\infty_{\\tau=0} \\gamma^\\tau r_{t+\\tau+1} \\Bigg| s_t = s \\right] \\\\\n",
    "&= \\sum_a \\pi(s,a) \\left[ \\bar{r}(s,a) + \\sum_{s'} P(s'|s,a) \\gamma \\mathbb{E}_\\pi \\left[ \\sum^\\infty_{\\tau=0} \\gamma^\\tau r_{t+\\tau+1} \\Bigg| s_{t+1}=s' \\right] \\right] \\\\\n",
    "&= \\sum_a \\pi(s,a) \\left[ \\bar{r}(s,a) + \\sum_{s'} P(s'|s,a) \\gamma V^\\pi (s') \\right]\n",
    "\\end{align}\n",
    "\n",
    ">* **Action value**\n",
    "\n",
    ">\\begin{align}\n",
    "Q^\\pi (s,a) &= \\mathbb{E}_\\pi [R_t|s_t=s,a_t=a] \\\\\n",
    "&= \\bar{r}(s,a) + \\sum_{s'} P(s'|s,a) \\gamma \\sum_{a'} \\pi(s',a') Q^\\pi (s'a') \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\n",
    "\\end{align}\n",
    "\n",
    ">* **Bellman Optimality Equation** defines optimal policy\n",
    "\n",
    ">\\begin{align}\n",
    "V^*(s) &= \\max_a \\left[ \\bar{r}(s,a) + \\sum_{s'} P(s'|s,a) \\gamma V^*(s') \\right] \\\\\n",
    "Q^*(s,a) &= \\bar{r}(s,a) + \\sum_{s'} P(s'|s,a) \\gamma \\max_{a'} Q^* (s',a')\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Policy Improvement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
