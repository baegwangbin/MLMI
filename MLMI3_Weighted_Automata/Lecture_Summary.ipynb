{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLMI3: Probabilistic Automata\n",
    "Lecturer: Prof. Bill Byrne\n",
    "\n",
    "----\n",
    "\n",
    "# Table of Contents\n",
    "## 1. Formal Languages and Language Hierarchies\n",
    "* 1.1. Finite Languages\n",
    "* 1.2. Finite State Acceptors\n",
    "* 1.3. Regular Languages\n",
    "* 1.4. Context Free Grammars\n",
    "* 1.5. Right Linear Languages\n",
    "\n",
    "## 2. Probabilistic Automata\n",
    "* 2.1. PCFGs\n",
    "* 2.2. Weighted Finite State Acceptors\n",
    "* 2.3. WFSA Operations\n",
    "* 2.4. Weighted Finite State Transducers\n",
    "* 2.5. WFST Operations\n",
    "\n",
    "## 3. Distance, Kernels, Semirings\n",
    "* 3.1. String Distances\n",
    "* 3.2. Kernels and Counting Transducers\n",
    "* 3.3. Semirings\n",
    "\n",
    "## 4. Applications of Weighted Automata\n",
    "* 4.1. Acoustic Likelihoods\n",
    "* 4.2. Language Models\n",
    "* 4.3. Lexicons\n",
    "* 4.4. CI-to-CD Transducers\n",
    "* 4.5. WFSA ASR \n",
    "* 4.6. Tagging\n",
    "* 4.7. Keyboards\n",
    "\n",
    "## 5. Inference and Computation\n",
    "* 5.1. MLE\n",
    "* 5.2. MBR Training\n",
    "* 5.3. Distance Computation\n",
    "* 5.4. Inference Functions\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Formal Languages and Language Hierarchies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **The Chomsky Hierarchy**\n",
    "\n",
    ">|Type|Name|Rule|Instance|\n",
    "|-|-----------------|-----------------------------------------------|-----------|\n",
    "|0|Turing Equivalent|||\n",
    "|1|Context Sensitive|$\\alpha A \\beta \\rightarrow \\alpha \\beta \\gamma, \\gamma \\neq \\epsilon$||\n",
    "|2|Context Free     |$A \\rightarrow \\gamma$                         |           |\n",
    "|3|Regular          |$A \\rightarrow xB$ or $A \\rightarrow x$        |FSA        |\n",
    "|-|Finite           |$x \\in \\{x_1, \\dots, x_N ; x_n \\in \\Sigma^* \\}$|N-Best list|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Finite Languages\n",
    "\n",
    "* **Formal Languages**\n",
    "\n",
    ">* **Formal Language** $=$ set of **strings** \n",
    ">* **String** $=$ composed of **symbols**\n",
    ">* **Symbols** $=$ drawn from a set $\\Sigma$ (**alphabet** or **vacabulary**)\n",
    ">* **Automata** $=$ **acceptor** or **generator**\n",
    "\n",
    "* **Finite Languages**\n",
    "\n",
    ">* **Finite vocabulary** & Strings of a **fixed maximum length**\n",
    ">* $\\Rightarrow$ possible to enumerate every sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Finite State Acceptors\n",
    "\n",
    "* **Lattice**: Weighted, directed acyclic graph\n",
    "\n",
    ">* $\\Rightarrow$ used to represent the *output* of language processing systems\n",
    "\n",
    "* **FSA(Finite State Acceptor)**: directed graph specified as a 5-tuple $M=(Q, \\Sigma, E, q_0, F)$\n",
    "\n",
    ">* $Q$: finite set of states\n",
    ">* $\\Sigma$: alphabet (or vocabulary)\n",
    ">* $E$: set of edges - from $s(e)$ to $f(e)$, each labelled with a symbol $i(e) \\in \\Sigma$\n",
    ">* $q_0$: initial state\n",
    ">* $F \\subset Q$: set of final states \n",
    "\n",
    "* **Complete Path**: $p=e_1 \\dots e_{n_p}$\n",
    "\n",
    ">* Initial state, $i_p = s(e_1)$\n",
    ">* Final state $f_p = f(e_{n_p})$\n",
    ">* Produces the string $x=i(e_1) \\dots i(e_{n_p})$\n",
    ">* FSA *accepts* or *generates* the string $x$\n",
    ">* The generated strings form the language $L_M$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Regular Languages\n",
    "\n",
    "* **Operations on Strings**\n",
    "\n",
    ">* Length: $|s|$\n",
    ">* Empty string: $\\epsilon$, $|\\epsilon|=0$, $xy = \\epsilon xy = x \\epsilon y = xy \\epsilon$\n",
    ">* Concatenation:\n",
    ">  * $abc$ with $cde$ $\\rightarrow$ $abccde$\n",
    ">  * $abc$ with $\\epsilon$ $\\rightarrow$ $abc$\n",
    ">  * string $x$ with itself $n$ times $\\rightarrow$ $x^n$\n",
    "\n",
    "* **Operationson Sets of Strings**\n",
    "\n",
    ">* **Union**: $L_1 \\cup L_2$\n",
    ">* **Intersection**: $L_1 \\cap L_2$\n",
    ">* **Concatenation**: $L_1 L_2$ $\\rightarrow$ $L^0=\\{\\epsilon\\}, L^1=L, L^2=LL, \\dots$\n",
    ">* **Kleene Closure**: $L^* = \\cup_{n \\geq 0} L^n$\n",
    ">* **Positive Closure**: $L^+ = \\cup_{n \\geq 1} L^n = LL^*$ (Kleene closure but excluding $\\epsilon$)\n",
    "\n",
    "* **Regular Languages**: class of languages that are definable by regular expressions\n",
    "\n",
    ">* If $L_1$ and $L_2$ are regular languages over $\\Sigma$, so are (closure under)\n",
    "\n",
    ">  * $L_1 \\cup L_2$ , $L_1 \\cap L_2$ , $L_1 L_2$ , $L_1 - L_2$\n",
    ">  * Complementation: $\\Sigma^* - L_1$ (all possible strings not in $L_1$)\n",
    ">  * Reversal: $L_1^R$ (the reversal of all strings in $L_1$)\n",
    ">  * Kleene Closure: $L_1^*$ and $L_2^*$\n",
    "\n",
    "* **Pumping Lemma for Regular Languages**\n",
    "\n",
    ">$$\\text{Let } L \\text{ be an infinite regular language,}$$\n",
    ">$$\\text{Any sufficiently long } w \\in L \\text{ can be split as } w=xyz \\text{ such that } xy^nz \\in L, n \\geq 0$$\n",
    "\n",
    ">* String longer than the no. of states in FSA $\\rightarrow$ some states are visited multiple times\n",
    ">* $y$: string that is **pumped**\n",
    ">* If there is no string that can be pumped $\\rightarrow$ the language is **not regular**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Context Free Grammars\n",
    "\n",
    "* **CFG**: defined as a 4-tuple $G=(N,\\Sigma,R,S)$ \n",
    "\n",
    ">* $N$: set of nonterminal symbols \n",
    ">  * Can be renamed arbitrarily\n",
    ">  * For any two grammars $G_1$ and $G_2$, we can assume that $N_1 \\cap N_2 = \\phi$\n",
    ">* $\\Sigma$: set of terminal symbols\n",
    ">* $R$: set of productions $A \\rightarrow \\beta$, where $A \\in N$ and $\\beta \\in (\\Sigma \\cup N)^*$\n",
    ">* $S$: start symbol\n",
    "\n",
    "* **Derivations**\n",
    "\n",
    ">* **Direct Derivation:** $\\alpha A \\gamma \\Rightarrow \\alpha \\beta \\gamma$ (rule: $A \\rightarrow \\beta$ and $\\alpha, \\gamma \\in (\\Sigma \\cup N)^*$)\n",
    ">* **Derivation:** $\\alpha_1 \\underset{G}{\\overset{*}{\\Rightarrow}} \\alpha_m$ (arbitrary no. of rules derive $\\alpha_m$)\n",
    ">* **Language:** set of strings derived from the start symbol\n",
    "\n",
    ">$$\\mathcal{L}_G = \\{ w|w \\in \\Sigma^* \\text{ and } S \\underset{G}{\\overset{*}{\\Rightarrow}} w \\}$$\n",
    "\n",
    ">* **Ambiguity:** same string of words can have different tree\n",
    ">* $\\mathcal{T}_G(S)$: all the trees with yield $S$ generated by the grammar $G$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Right Linear Languages\n",
    "\n",
    "* **Definitions**\n",
    "\n",
    ">* **Linear Grammar:** CFG with at most **one** non-terminal on the RHS of its rules\n",
    ">* **Right-Linear:** All non-terminals in RHS are at the right ends\n",
    ">* **Left-Linear:** All non-terminals in RHS are at the left ends\n",
    "\n",
    "* **Construct FSA from a Right-Linear Grammar**\n",
    "\n",
    ">$$G = (N, \\Sigma, P, S) \\;\\; \\rightarrow \\;\\; M = (Q, \\Sigma, E, q_0, F)$$\n",
    "\n",
    ">* $Q = N \\cup \\{f\\}$ and $F=\\{f\\}$\n",
    ">* $A \\rightarrow xB \\;\\;\\; \\Rightarrow \\;\\;\\; x: (A) \\overset{x}{\\rightarrow} (B)$\n",
    ">* $A \\rightarrow x \\;\\;\\; \\Rightarrow \\;\\;\\; x: (A) \\overset{x}{\\rightarrow} (f)$\n",
    "\n",
    "* **Linear Languages: closure under**\n",
    "\n",
    ">* **Union:**  $L_A \\cup L_B = L_C$\n",
    ">* **Concatenation:** $L_A L_B = L_C$\n",
    ">* **Kleene Closure:** $L_A^* = L_C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Probabilistic Automata\n",
    "\n",
    ">$$0 \\leq P(w) \\leq 1 \\;\\;\\;,\\;\\;\\; \\sum_{w \\in L} P(w)=1$$\n",
    "\n",
    ">$$P(w): \\text{ how sensible } w \\text{ is}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. PCFGs\n",
    "\n",
    "* **PCFG = CFG + Probabilities**, $G = (N, \\Sigma, R, S)$\n",
    "* $R$: set of productions of the form $A \\rightarrow \\beta/p$, where $A \\in N$ and $\\beta \\in (\\Sigma \\cup N)^*$\n",
    "\n",
    ">$$p=P(A \\rightarrow \\beta | A) \\;\\;\\;,\\;\\;\\; \\sum_\\beta P(A \\rightarrow \\beta | A) = 1$$\n",
    "\n",
    "* **Probabilities over Derivations**\n",
    "\n",
    ">$$d = r_1, \\dots, r_n \\text{ derives } s \\in \\Sigma^* \\text{ from } S$$\n",
    "\n",
    ">$$p(d|S) = \\prod^n_{i=1} p(r_i) = \\prod_{r \\in R} p(r)^{\\#_d (r)}$$\n",
    "\n",
    ">* Probability of a tree = Probability of its derivation\n",
    "\n",
    ">* $\\#_d (r)$: no. of times $r$ occurs in the derivation $d$\n",
    "\n",
    "* **Ambiguity in PCFGs**\n",
    "\n",
    ">$$P(S) = \\sum_{T \\in \\mathcal{T}_G (S)} P(T)$$\n",
    "\n",
    ">* $\\mathcal{T}_G (S)$: trees with yield $S$ generated by the grammar $G$\n",
    ">* $P(S)$: prob. over sentences / $P(T)$: prob. over trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Weighted Finite State Acceptors\n",
    "\n",
    "* **WFSA = FSA + Weights**\n",
    "\n",
    ">$$ M = (Q, \\Sigma, E, q_0, F, \\rho) $$\n",
    "\n",
    ">$$s(e) \\overset{i(e)/w(e)}{\\longrightarrow}f(e) \\;\\;\\;,\\;\\;\\; \\rho(f): \\text{weight for final states } f \\in F$$\n",
    "\n",
    "* **Weights Assigned to Strings by Acceptors**\n",
    "\n",
    ">$$w(p) = w(e_1) \\otimes \\dots \\otimes w(e_{n_p}) \\otimes \\rho(f_p) = (\\otimes^{n_p}_{j=1} w(e_j)) \\otimes \\rho(f_p)$$\n",
    "\n",
    ">$$[\\![ A ]\\!] (x) = \\underset{p \\in P(x)}{\\bigoplus} w(p)$$\n",
    "\n",
    ">* $P(x)$: set of complete paths which generate $x$\n",
    ">* $[\\![ A ]\\!] (x)$: cost assigned to the string $x$ by the acceptor\n",
    "\n",
    "* **Operations on Weights**\n",
    "\n",
    ">|**Semiring**|$\\mathbb{K}$|$\\oplus$|$\\otimes$|$\\bar{0}$|$\\bar{1}$|\n",
    "|-----------|---------------------------------------|------------|-|-|-|\n",
    "|**Probability**|$\\mathbb{R}_+$                         |$+$|$\\times$|$0$|$1$|\n",
    "|**Log**        |$\\mathbb{R} \\cup \\{ -\\infty, \\infty \\}$|$\\oplus_{\\log}$|$+$|$\\infty$|$0$|\n",
    "|**Tropical**   |$\\mathbb{R} \\cup \\{ -\\infty, \\infty \\}$|$\\min$|$+$|$\\infty$|$0$|\n",
    "\n",
    ">* $k_1 \\oplus_{\\log} k_2 = - \\log (e^{-k_1} + e^{-k_2})$\n",
    "\n",
    "* **Weights under Semirings** (paths $p_1$ and $p_2$ generate $\\text{\"a b\"}$)\n",
    "\n",
    ">\\begin{align}\n",
    "\\textbf{Probability: } [\\![ A ]\\!] (\\text{\"a b\"}) &= p_1 (\\text{\"a b\"}) p_1 + p_2 (\\text{\"a b\"}) p_2 \\\\\n",
    "&= \\text{marginal probability} \\\\\n",
    "\\\\\n",
    "\\textbf{Log: } [\\![ A ]\\!] (\\text{\"a b\"}) &= - \\log \\big[ p_1 (\\text{\"a b\"}) p_1 + p_2 (\\text{\"a b\"}) p_2 \\big] \\\\\n",
    "&= \\text{negative log marginal probability} \\\\\n",
    "\\\\\n",
    "\\textbf{Tropical: } [\\![ A ]\\!] (\\text{\"a b\"}) &= -\\max \\big[ \\log p_1 (\\text{\"a b\"}) p_1, \\log p_2 (\\text{\"a b\"}) p_2 \\big] \\\\\n",
    "&= \\text{negative log Viterbi likelihood}\n",
    "\\end{align}\n",
    "\n",
    "* **Tropicalization**\n",
    "\n",
    "><img src = 'images/image01.png' width=500>\n",
    "\n",
    ">* Replace $(+, \\times)$ by $(\\min, +)$\n",
    ">* Replace joint probabilities by their negative logarithms\n",
    ">* This process is consistent (i.e. invertible) for arc weights and path weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. WFSA Operations\n",
    "\n",
    "* **Intersection:** $[\\![ C ]\\!] (x) = [\\![ A ]\\!] (x) \\otimes [\\![ B ]\\!] (x)$\n",
    "\n",
    "* **Union:** $[\\![ C ]\\!] (x) = [\\![ A ]\\!] (x) \\oplus [\\![ B ]\\!] (x)$\n",
    "\n",
    "\n",
    "\n",
    "><img src = 'images/image02.png' width = 600>\n",
    "\n",
    "><img src = 'images/image03.png' width = 400>\n",
    "\n",
    "><img src = 'images/image04.png' width = 500>\n",
    "\n",
    "* **Concatenation:** $[\\![ C ]\\!] (x) = \\underset{x_1,x_2:x=x_1x_2}{\\bigoplus} [\\![ A ]\\!] (x_1) \\otimes [\\![ B ]\\!] (x_2)$\n",
    "* **Closure:** $[\\![ A^* ]\\!] (x) = \\bigoplus^\\infty_{n=0} [\\![ A^n ]\\!] (x)$\n",
    "\n",
    "\n",
    "\n",
    "><img src = 'images/image05.png' width = 550>\n",
    "\n",
    "* **Determinization**\n",
    "\n",
    ">* After **determinization**,\n",
    ">  * unique starting state\n",
    ">  * no two transitions leaving a state share the same input label \n",
    ">  * arc weights may change / but string weights are unchanged\n",
    ">  * there may be new epsilon arcs\n",
    ">* **Minimization:** finds an equivalent machine with a minimal no. of states and arcs\n",
    "\n",
    "* **Pruning**\n",
    "\n",
    ">$$\\text{edge: } e = p \\overset{i/w}{\\longrightarrow} n$$\n",
    "\n",
    ">$$\\text{delete } e \\text{ if } d^* \\otimes c < d^r [p] \\otimes w \\otimes d[n]$$\n",
    "\n",
    ">* $d^*$: the weight of the best path through the FST\n",
    ">* $d^r[p]$: distance from the start state to $p$\n",
    ">* $d[n]$: shortest distance from $n$ to a final state\n",
    "\n",
    "* **Pushing**\n",
    "\n",
    ">* **Pushing** moves weights and/or labels towards the start or the end state\n",
    ">  * Towards the **start** state: **improve pruning**\n",
    ">  * Towards the **end** state: **help accumulating costs over paths**\n",
    ">* **Algorithm:**\n",
    ">  * Pushing makes the WFSA stochastic (in real semiring, for each state, weights add to 1)\n",
    ">  * Tropical & log semiring $\\rightarrow$ multiplicative inverse is simply arithmetic subtraction\n",
    "\n",
    ">$$w \\leftarrow (d[p])^{-1} w \\otimes d[n] \\;\\;\\;,\\;\\;\\; d[q]=\\underset{\\pi \\in P(q,F)}{\\bigoplus} w[\\pi]$$\n",
    "\n",
    "* **Failure Transitions**\n",
    "\n",
    ">||Consumes no symbol|Consumes symbol|\n",
    "|-|-|-|\n",
    "|Matches all|$\\epsilon$|$\\sigma$|\n",
    "|Mathches rest|$\\phi$|$\\rho$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Weighted Finite State Transducers\n",
    "\n",
    "* **WFST = WFSA + symbol-to-symbol mappings**\n",
    "\n",
    ">$$ M = (Q, \\Sigma, \\Delta, E, q_0, F) $$\n",
    "\n",
    ">$$s(e) \\overset{i(e):o(e)/w(e)}{\\longrightarrow}f(e) \\;\\;\\;,\\;\\;\\; \\Sigma \\text{ and } \\Delta: \\text{input and output alphabet}$$\n",
    "\n",
    "* **Weighted Mapping**\n",
    "\n",
    ">$$p \\in P(x,y) \\;\\;\\;,\\;\\;\\; x = i(e_1) \\dots i(e_{n_p}) \\;\\;\\;,\\;\\;\\; y=o(e_1) \\dots o(e_{n_p})$$\n",
    "\n",
    ">$$w(p) = (\\otimes^{n_p}_{j=1} w(e_j)) \\otimes \\rho(f_p)$$\n",
    "\n",
    ">$$[\\![ T ]\\!] (x,y) = \\underset{p \\in P(x,y)}{\\bigoplus} w(p)$$\n",
    "\n",
    ">* $[\\![ T ]\\!] (x,y)$: sum of all path weights along which $x$ is mapped to $y$\n",
    "\n",
    "* **WFST: Mapping between Regular Languages**\n",
    "\n",
    ">* $T_1$ and $T_2$: WFSAs $\\rightarrow$ $L_{T_1}$ and $L_{T_2}$: Regular languages\n",
    ">* $T$ maps strings $x \\in L_{T_1}$ to $y \\in L_{T_2}$ with weight $[\\![ T ]\\!] (x,y)$\n",
    ">* Regular languages and context-free languages are **closed under (finite) transduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. WFST Operations\n",
    "\n",
    "* **Projection**\n",
    "\n",
    ">* $T_1$: Projection on input $\\rightarrow$ $L_{T_1}$: input language\n",
    ">* $T_2$: Projection on output $\\rightarrow$ $L_{T_2}$: output language\n",
    "\n",
    "* **Composition**\n",
    "\n",
    ">$$[\\![ A \\circ B ]\\!] (x,z) = \\underset{y}{\\bigoplus} [\\![ A ]\\!] (x,y) \\otimes [\\![ B ]\\!] (y,z)$$\n",
    "\n",
    "* **Union**\n",
    "\n",
    ">$$[\\![ A \\oplus B ]\\!] (x,y) = [\\![ A ]\\!] (x,y) \\oplus [\\![ B ]\\!] (x,y)$$\n",
    "\n",
    "* **Concatenation**\n",
    "\n",
    ">$$[\\![ A \\otimes B ]\\!] (x,y) = \\underset{x=x_1 x_2, y=y_1 y_2}{\\bigoplus} [\\![ A ]\\!] (x_1,y_1) \\otimes [\\![ B ]\\!] (x_2,y_2)$$\n",
    "\n",
    "* **Closure**\n",
    "\n",
    ">$$[\\![ T^* ]\\!] (x,y) = \\overset{\\infty}{\\underset{n=0}{\\bigoplus}} [\\![ T^n ]\\!] (x,y)$$\n",
    "\n",
    "* **Disambiguation**\n",
    "\n",
    ">* **Ambiguity:** multiple paths accept same input string\n",
    ">* **Non-Functional:** multiple output paths for a single input string\n",
    ">* **Disambiguation:** creating a new WFST that encodes only the best-scoring path of each input string, while maintaining the arc-level mapping between input and output symbols\n",
    "\n",
    "* **Other Operations**\n",
    "\n",
    ">* **Connect:** remove useless states and arcs\n",
    ">* **Invert:** swaps input and output labels\n",
    ">* **Reverse:** reverse input and output languages\n",
    "\n",
    "* **Operational Complexity**\n",
    "\n",
    ">* Operations on 1 automata\n",
    ">  * Reversal, Inversion, Projection, Connection $\\rightarrow$ $O(|Q|+|E|)$\n",
    ">  * Epsilon removal $\\rightarrow$ **cubic**\n",
    ">  * Determinization $\\rightarrow$ **exponential**\n",
    "\n",
    ">* Operations on 2 automata\n",
    ">  * Composition, Intersection, Difference $\\rightarrow$ $O((|Q_1|+|E_1|)(|Q_2|+|E_2|))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Distances, Kernels, Semirings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. String Distances\n",
    "\n",
    "* **Symbol-to-Symbol Distance**\n",
    "\n",
    ">$$d(x,y) = \\bigg\\{ \\begin{matrix} 0 & y=x \\\\ d_r & y \\neq x \\end{matrix} \\;\\;\\; \\text{or} \\;\\;\\; d(x,y) = \\Bigg\\{ \\begin{matrix} 0 & y=x \\\\ d_r & y \\neq x  \\\\ d_d & x=\\epsilon \\text{ or } y=\\epsilon \\end{matrix}$$\n",
    "\n",
    ">* **Ambiguity** $\\rightarrow$ choose minimum distance under all allowable alignments\n",
    "\n",
    "* **Edit Distance Transducers** $T$\n",
    "\n",
    ">* Assigns cost for edits(replacement, deletion, insertion)\n",
    "\n",
    ">$$A \\circ T \\circ B \\rightarrow \\text{all alignments with all costs}$$\n",
    "\n",
    ">* **Method 1:** shortest path on $A \\circ T \\circ B$\n",
    ">* **Method 2:** input projection $\\rightarrow$ epsilon removal $\\rightarrow$ determinization (wrt tropical semiring) $\\rightarrow$ single path\n",
    "\n",
    ">  * But the actual symbol-to-symbol alignment can be lost\n",
    "\n",
    "* **Lattice & String**\n",
    "\n",
    ">* **Method 1:** find every alignment of every string in $C$ to $B$ ($C \\circ T \\circ B$)\n",
    ">* **Method 2:** find the single string in $C$ that aligns best to $B$ (ShortestPath $C \\circ T \\circ B$)\n",
    ">* **Method 3:** find the cost of the best alignment of every string in $C$ to $B$\n",
    ">* **Method 4:** use disambiguation algorithm (useful for Lattice-to-Lattice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Kernels and Counting Transducers\n",
    "\n",
    "* **Kernel Functions**\n",
    "\n",
    ">* $\\kappa (x,x')$: Measures the similarity between two strings\n",
    ">* Typically **symmetric** & **positive**\n",
    "\n",
    "* **Mercer Kernel**\n",
    "\n",
    ">$$ \\kappa (x,x') = \\sum_{s \\in \\Sigma^*} w_s \\phi_s (x) \\phi_s (x')$$\n",
    "\n",
    ">* $\\phi_s (x)$: no. of times a substring $s$ occurs in a string $x$\n",
    ">* Monotonicity constraints relaxed (or removed) / but subsequences in strings should be counted\n",
    "\n",
    "* **Examples**\n",
    "\n",
    ">* **Bag-of-Characters** kernel: $w_s=0$ for $|s|>1$\n",
    ">* **Bag-of-Words** kernel: $w_s=0$ unless $s$ is bounded by white space\n",
    ">* **All-subsequences** kernel: $w_s = 1$\n",
    ">* **K-Spectrum** kernel: $\\kappa(x,x') = \\sum_{s\\in \\Sigma^k} \\phi_s (x) \\phi_s (x')$\n",
    "\n",
    "* **Kernels for Lattices**\n",
    "\n",
    ">\\begin{align}\n",
    "\\text{expected count: } c(A,s) &= \\sum_{x \\in L_A} P_A (x) \\phi_s (x) \\\\\n",
    "\\text{lattice kernel: } \\kappa (A,B) &= \\sum _ {s \\in \\Sigma^*} c(A,s) c(B,s) \\\\\n",
    "&= \\sum_{s \\in \\Sigma^*} \\sum_{x \\in L_A} P_A (x) \\phi_s (x) \\sum_{x' \\in L_B} P_B (x') \\phi_s (x') \\\\\n",
    "&= \\sum_{x \\in L_A} \\sum_{x' \\in L_B} P_A (x) P_B (x') \\kappa(x,x')\n",
    "\\end{align}\n",
    "\n",
    ">* Compares WFSAs as the **weighted similarity** of the strings in their languages\n",
    ">* $A \\cap B = \\phi$ does not imply that $\\kappa(A,B)=0$\n",
    "\n",
    "* **Counting Transducers** (efficiently count n-gram in log semiring)\n",
    "\n",
    ">$$ A \\circ T1 (\\text{or } T2) \\rightarrow \\text{Output Projection} \\rightarrow \\epsilon \\;\\text{Removal} \\rightarrow \\text{Determinization}$$\n",
    "\n",
    ">* **Gappy N-Gram Kernels:** penalty $\\lambda$ for each gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Semirings\n",
    "\n",
    "* **Tropical Semirings - Feature Vectors**\n",
    "\n",
    ">$$s(e) \\overset{i(e)/v(e)}{\\longrightarrow} f(e) \\;\\;\\;\\Rightarrow\\;\\;\\; w(e)=\\theta \\cdot v(e)$$\n",
    "\n",
    ">* $v(e)$: unweighted n-dim feature vector\n",
    ">* $\\theta$: Parameter vector, applied to compute weights\n",
    "\n",
    ">\\begin{align}\n",
    "\\otimes &: \\;\\;\\; v_3 = v_1 + v_2 &\\Rightarrow \\theta \\cdot v_3 = w_1 \\otimes w_2 \\\\\n",
    "\\oplus &: \\;\\;\\; v_3 = \\bigg\\{ \\begin{matrix} v_1 \\;\\;\\; \\text{if} \\;\\;\\; \\theta \\cdot v_1 \\leq \\theta \\cdot v_2 \\\\ v_2 \\;\\;\\; \\text{if} \\;\\;\\; \\theta \\cdot v_2 < \\theta \\cdot v_1 \\end{matrix} &\\Rightarrow \\theta \\cdot v_3 = w_1 \\oplus w_2\n",
    "\\end{align}\n",
    "\n",
    "* **Transducer Composition**\n",
    "\n",
    ">$$\\underset{x,y}{\\min}[\\![ A \\circ B ]\\!] (x,y) =\\underset{x,y}{\\min} \\underset{z}{\\min} ( [\\![ A ]\\!](x,z) [\\![ B ]\\!](z,y))$$\n",
    "\n",
    ">$$x \\in L_{A_1} \\;\\;\\;,\\;\\;\\; z \\in L_{A_2} \\cap L_{B_1} \\;\\;\\;,\\;\\;\\; y \\in L_{B_2}$$\n",
    "\n",
    ">* The weights of each component transducer have their own position in the feature vectors\n",
    ">* $\\Rightarrow$ The contribution of each transducer can be tracked\n",
    "\n",
    "* **Bottleneck Semiring**\n",
    "\n",
    ">* $\\mathbb{K} = \\mathbb{R}, \\;\\; \\bar{0}=-\\infty, \\;\\; \\bar{1}=\\infty$\n",
    ">* $\\otimes = \\min$: **bottleneck** along any particular path\n",
    ">* $\\oplus = \\max$: cost along the path with the most throughput\n",
    ">* Measures maximal **throughput** or **capacity** through a network\n",
    ">* Arc weights: analogous to **pipe widths**\n",
    "\n",
    "* **Possibilistic Semiring**\n",
    "\n",
    ">* $\\mathbb{K} = [0,1], \\;\\; \\bar{0}=-\\infty, \\;\\; \\bar{1}=1$\n",
    ">* $\\otimes = \\times$: probability of success of any particular sequence\n",
    ">* $\\oplus = \\max$: best probability of success\n",
    ">* Measures **maximal possibility** or **maximum reliability**\n",
    "\n",
    "* **Formal Language Semiring**\n",
    "\n",
    ">* $\\mathbb{K} = P(\\Sigma^*)$ (power set)$, \\;\\; \\bar{0}=\\epsilon, \\;\\; \\bar{1}=\\epsilon$\n",
    ">* $\\otimes = \\cup$\n",
    ">* $\\oplus = \\cdot\\;$ (concatenation)\n",
    ">* The distance from the start state to the final state: yields the language of the automata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Applications of Weighted Automata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Acoustic Likelihoods\n",
    "\n",
    "* **HMM Likelihood**\n",
    "\n",
    ">$$P(O,X) = a_{x(0),x(1)} \\prod^T_{t=1} b_{x(t)}(o_t) a_{x(t),x(t+1)}$$\n",
    "\n",
    "* **Conditional Likelihood**\n",
    "\n",
    ">$$[\\![ A ]\\!] (X) = P(O|X) = \\prod^T_{t=1} b_{x(t)} (o_t)$$\n",
    "\n",
    "* **HMM Likelihood with WFSAs in Tropical Semiring**\n",
    "\n",
    ">$$\\log P(O,X) = \\log a_{x(0),x(1)} + \\sum^T_{t=1} \\log b_{x(t)}(o_t) + \\log a_{x(t),x(t+1)}$$\n",
    "\n",
    ">$$\\text{Joint Likelihood: } [\\![ A \\circ B ]\\!] (X) = -\\log P(O,X)$$\n",
    "\n",
    ">\\begin{align}\n",
    "&A: \\text{HMM observation distribution} &(t-1) \\overset{n/-\\log b_n(o_t)}{\\longrightarrow} (t) \\\\\n",
    "&B: \\text{HMM transition probabilities} &(n) \\overset{n'/-\\log a_{n,n'}}{\\longrightarrow} (n') \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Language Models\n",
    "\n",
    "* **Back-off Bigram Language Model**\n",
    "\n",
    ">$$\\hat{P}(w_j|w_i) = \\bigg\\{ \\begin{matrix} p(w_i,w_j) & f(w_i,w_j) > C \\\\ \\alpha(w_i) \\hat{P}(w_j) & \\text{otherwise} \\end{matrix}$$\n",
    "\n",
    ">$$p(w_i,w_j) = d(f(w_i,w_j)) \\frac{f(w_i,w_j)}{f(w_i)}$$\n",
    "\n",
    ">* $f(w_i,w_j)$: no. of times $w_i, w_j$ are observed\n",
    ">* WFSAs: cannot implement **otherwise** $\\rightarrow$ use **failure transition** (e.g. $\\phi$)\n",
    "\n",
    "* **A Small Back-off Bigram Language Model**\n",
    "\n",
    "><img src = 'images/image09.png' width = 600>\n",
    "\n",
    ">* Approximate implementation: uses $\\epsilon$ instead of $\\phi$\n",
    ">* $\\Rightarrow$ The back-off patch can be taken even if a non-back-off path is present\n",
    "\n",
    "* **Building a WFSA for a Bigram Language Model**\n",
    "\n",
    "><img src = 'images/image10.png' width = 600>\n",
    "\n",
    ">* **State** for every word & a unigram back-off state $\\epsilon$\n",
    ">* **Arc** for each pair of words $w$ and $w'$ for which $f(w,w')>C$\n",
    ">* **Back-off Arc** from $w$ to $\\epsilon$ / **Unigram Arc** from $\\epsilon$ to $w'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Lexicons\n",
    "\n",
    "><img src = 'images/image11.png' width = 600>\n",
    "\n",
    ">* Maps **phone sequences** to **word sequences**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. CI-to-CD Transducers\n",
    "\n",
    "* **CI-to-CD transducer**\n",
    "\n",
    ">* Maps **monophone sequence** to **triphone sequence**\n",
    "\n",
    ">$$(p_1,p_2) \\overset{p_3:t=p_1\\text{-}p_2\\text{+}p_3}{\\longrightarrow} (p_2,p_3) $$\n",
    "\n",
    ">* Silence models, monophones, etc must be handled differently\n",
    "\n",
    "* **WFST to Map Triphone Sequences to HMM State Sequences**\n",
    "\n",
    ">* **State-clustering:** share GMM across triphones\n",
    ">* Each triphone state has a pointer to one of the HMM states: $p(t_m)=s_n$\n",
    ">* Transducer maps from HMM state sequences to triphone sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. WFSA ASR\n",
    "\n",
    ">$$\\text{Acoustic Likelihoods} \\overset{A \\circ B}{\\longrightarrow} \\text{HMM State Sequences} \\overset{S}{\\rightarrow}\n",
    "\\text{Triphone Sequences} \\overset{C}{\\rightarrow} $$\n",
    ">$$\\text{Monophone Sequences} \\overset{L}{\\rightarrow} \\text{Word Sequences} \\overset{G}{\\rightarrow} \\text{Language Model Scores}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. Tagging\n",
    "\n",
    "* **Truecasing**\n",
    "\n",
    ">$$B \\circ T_\\text{case} \\circ G_\\text{case}$$\n",
    "\n",
    ">* $B$: acceptor for uncased sentence\n",
    ">* $T_\\text{case}$: maps between all cased and uncased variants found in the text\n",
    ">* $G_\\text{case}$: accpetor for cased N-gram LM\n",
    ">* $P_\\text{case}(W) = \\prod^N_{j=1} P(W_j|W_{j-1},...,W_{j-N+1})$\n",
    ">* `Fstshortestpath` over $B \\circ T_\\text{case} \\circ G_\\text{case}$ to find $\\hat{W} = \\underset{W:lc(W)=w}{\\text{argmax}} \\; P_\\text{case}(W)$\n",
    "\n",
    "* **Part-of-Speech Tagging**\n",
    "\n",
    ">$$\\hat{t}^n_1 = \\underset{t^n_1}{\\text{argmax}} \\; P(t^n_1|w^n_1) \\approx \\prod^n_{i=1} P(w_i|t_i)P(t_i|t_{i-1})$$\n",
    "\n",
    ">* $P(w_i|t_i)$ and $P(t_i|t_{i-1})$: estimated from annotated text collections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7. Keyboards\n",
    "\n",
    ">* **Input modes:** tap typing and gesture typing\n",
    ">* **Challenges:** fat finger errors / ambiguity\n",
    ">* **Desired Features:** autocorrection / word suggestions / alternative suggestions\n",
    ">* **Keyboard Transducers:** handles bi-key sequences in either input mode\n",
    ">* **Transliteration:** Graphemic conversion (i.e., from one script to another)\n",
    ">* **Romanization:** Mapping sequences in the target script to sequences of Latin symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Inference and Computation\n",
    "\n",
    "* Estimate **automata weights** from **data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. MLE\n",
    "\n",
    "* **Likelihood Function**\n",
    "\n",
    ">$$Q(\\mathcal{T}) = \\prod_{x \\in \\mathcal{T}} Q(x) = \\prod_{x \\in \\mathcal{X}} Q(x)^{\\#(x)}$$\n",
    "\n",
    ">* $\\mathcal{X}$: random variable set\n",
    ">* $\\mathcal{T}$: finite training set (i.i.d. samples)\n",
    "\n",
    "* **KL Divergence**\n",
    "\n",
    ">$$D(P||Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}$$\n",
    "\n",
    ">* Define empirical distribution $\\hat{P}(x) = \\#(x)/M$,\n",
    ">* If $D(\\hat{P}||Q_1) \\leq D(\\hat{P}||Q_2)$, then $Q_2(\\mathcal{T}) \\leq Q_1(\\mathcal{T})$\n",
    "\n",
    "* **MLE for Bigram LM**\n",
    "\n",
    ">* Likelihood\n",
    "\n",
    ">$$P(W) = \\prod^N_{n=1} P(w_n|w_{n-1}) = \\prod_w \\prod_{w'} p(w'|w)^{\\#_{w,w'}(W)}$$\n",
    "\n",
    ">* MLE solution\n",
    "\n",
    ">$$\\tilde{p}(w'|w) = \\frac{\\hat{\\#}_{w,w'}}{\\hat{\\#}_{w}} \\;\\;\\;\\text{where}\\;\\;\\; \\hat{\\#}_{w,w'}=\\sum_{W\\in\\mathcal{T}} \\hat{\\#}_{w,w'}(W) \\;\\;\\;\\text{and}\\;\\;\\; \\hat{\\#}_{w}=\\sum_{w'} \\hat{\\#}_{w,w'}$$\n",
    "\n",
    "* **MLE and WFSAs**\n",
    "\n",
    ">* Extend to WFSAs from labelled training sets (**N-Gram**, **Part-of-Speech**, **PCFGs**, ...)\n",
    ">* Unlabelled data $\\rightarrow$ replace empirical counts with expected counts & use **EM algorithm**\n",
    "\n",
    "* **Expectation Semiring** (E-step in EM)\n",
    "\n",
    ">* **Step 1:** Define a **value function** $v(e) \\in \\mathbb{R}^n$ $\\Rightarrow$ value of path $v(\\pi)=\\sum^n_{i=1} v(e_i)$\n",
    ">* **Step 2:** Introduce paired weights $(p,v) \\in \\mathbb{R} \\times \\mathbb{R}^n$ \n",
    ">* **Step 3:** Define the semiring as \n",
    "\n",
    ">$$(p_1,v_1) \\oplus (p_2,v_2) = (p_1+p_2, v_1+v_2)$$\n",
    "\n",
    ">$$(p_1,v_1) \\otimes (p_2,v_2) = (p_1p_2, p_1v_2+p_2v_1)$$\n",
    "\n",
    ">$$\\bar{0} = (0,0) \\;\\;\\; , \\;\\;\\; \\bar{1} = (1,0)$$\n",
    "\n",
    ">* **Step 4:** Suppose $\\pi = \\pi_1 \\pi_2$ (prefix + suffix)\n",
    "\n",
    ">$$P(\\pi) = P(\\pi_1) P(\\pi_2) \\;\\;\\; \\text{and} \\;\\;\\; v(\\pi) = v(\\pi_1) + v(\\pi_2)$$\n",
    "\n",
    ">\\begin{align}\n",
    "(P(\\pi_1),v(\\pi_1)) \\otimes (P(\\pi_2),v(\\pi_2)) &= (P(\\pi_1)P(\\pi_2), P(\\pi_1)P(\\pi_2)(v(\\pi_1)+v(\\pi_2))) \\\\\n",
    "&= (P(\\pi), P(\\pi)v(\\pi)) \\\\\n",
    "\\oplus_{\\pi \\in \\prod} (P(\\pi),P(\\pi)v(\\pi)) &= \\left( \\sum_{\\pi \\in \\Pi} P(\\pi), \\sum_{\\pi \\in \\Pi} P(\\pi)v(\\pi) \\right) \\\\\n",
    "&= \\left( P(\\Pi),E[v(\\pi) \\times 1_{\\Pi}(\\pi)] \\right) \\\\\n",
    "E[v(\\pi)|\\Pi] &= E[v(\\pi) \\times 1_{\\Pi}(\\pi)]/P(\\Pi)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. MBR Training\n",
    "\n",
    "* **Baum-Welch** (improves the likelihood of the correct answer)\n",
    "\n",
    ">$$\\underset{\\lambda}{\\text{argmax}} P_\\lambda (O,W_{ref})$$\n",
    "\n",
    "* **MBR: Minimum Bayes Risk estimation** (Generalization of Baum-Welch)\n",
    "\n",
    ">$$\\underset{\\lambda}{\\text{argmin}} \\sum_{W'} L(W_{ref},W') P_\\lambda (W'|O)$$\n",
    "\n",
    ">* $L(W_{ref},W')$: loss function (e.g. no. of word errors)\n",
    "\n",
    "* **Optimizing expected WER via sampling for speech recognition**\n",
    "\n",
    ">* Acoustic model: stacked LSTM network\n",
    ">* Produces logit vector sequence $z_t \\in [0,1]^Q$\n",
    "\n",
    "><img src = 'images/image13_.png' width = 500>\n",
    "\n",
    ">* **Unrolled decoder graph:** $U(z) = S(z) \\circ (C \\circ L \\circ G)$\n",
    "\n",
    ">  * Input: $z$'s / Output: ASR word hypotheses $o(\\pi)$\n",
    "\n",
    ">* For each path $\\pi$ with weight $w(\\pi,z)$, the posterior is $P(\\pi|z,\\lambda)=\\frac{w(\\pi,z)}{\\sum_{\\pi'} w(\\pi',z)}$ so that\n",
    "\n",
    ">$$\\nabla_z \\log P(\\pi|z) = \\nabla_z \\log w(\\pi,z) - \\mathbb{E}_{P(\\pi|z)} \\nabla_z \\log w(\\pi,z)$$\n",
    "\n",
    ">* $\\nabla_z \\log P(\\pi|z)$: needed for back propagatino of the gradient wrt $\\lambda$\n",
    ">* $\\nabla_z \\log w(\\pi,z)$: $T\\times Q$ matrix with $1$ at each $(t,q_t)$ in $\\pi$ and $0$ elsewhere\n",
    "\n",
    "* **Backward filtering - forward sampling**\n",
    "\n",
    ">* **Sample $\\pi_i \\text{ ~ } P(\\pi|z,\\lambda)$ from $U(z)$:**\n",
    ">  * Push the $U(z)$'s weights to make it stochastic\n",
    ">  * Ancestral sampling: sample edges from the initial state to create complete paths $\\pi_i$\n",
    ">  * For any path $\\pi$, compute $L(\\pi)=L(W_{ref},o(\\pi))$\n",
    "\n",
    ">* **Expected loss** (approximated using Monte Carlo approximation)\n",
    "\n",
    ">$$\\mathbb{E}_{P(\\pi|z)} L(\\pi) = \\sum_\\pi P(\\pi|z) L(\\pi) \\approx 1/I \\sum^I_{i=1} L(\\pi_i) = \\overline{L(\\pi_i)}$$\n",
    "\n",
    ">* **MBR gradient**\n",
    "\n",
    ">\\begin{align}\n",
    "\\nabla_z \\mathbb{E}_{P(\\pi|z)} L(\\pi) &= \\sum_\\pi P(\\pi|z) L(\\pi) \\nabla_z \\log P(\\pi|z) \\\\\n",
    "&= \\mathbb{E}_{P(\\pi|z)} L(\\pi) \\nabla_z \\log w(\\pi,z) - \\mathbb{E}_{P(\\pi|z)} L(\\pi) \\mathbb{E}_{P(\\pi|z)} \\nabla_z \\log w(\\pi,z) \\\\\n",
    "&\\approx \\frac{I}{I-1} \\overline{(L(\\pi)-\\overline{L(\\pi_i)}) \\nabla_z \\log w(\\pi_i,z)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Distance Computation\n",
    "\n",
    "* **Tropical Arithmetic and Dynamic Programming**\n",
    "\n",
    ">* Addition and Multiplication\n",
    "\n",
    ">$$x \\oplus y = \\min(x,y) \\;\\;\\;,\\;\\;\\; x \\otimes y = x+y$$\n",
    "\n",
    ">* Exponentiation\n",
    "\n",
    ">$$(x \\oplus y)^n = x^n \\oplus y^n$$\n",
    "\n",
    ">* Matrix and vector operations\n",
    "\n",
    ">$$(u_1,u_2,u_3) \\otimes (v_1,v_2,v_3)^T = u_1 \\otimes v_1 \\oplus u_2 \\otimes v_2 \\oplus u_3 \\otimes v_3 = \\min \\{ u_1+v_1, u_2+v_2, u_3+v_3 \\}$$\n",
    "\n",
    "* **Shortest Paths in a Weighted Directed Graph**\n",
    "\n",
    ">* **Adjacency matrix**\n",
    "\n",
    ">$$D_G = [d_{i,j}] \\;\\;\\;,\\;\\;\\; d_{i,j}:\\text{distance}$$\n",
    "\n",
    ">* $D_G^{\\otimes n-1}$: $n \\times n$ matrix with entries in $\\mathbb{R}_{\\geq 0} \\cup \\{+\\infty\\}$\n",
    ">* **Proposition:** entry of $[D_G^{\\otimes n-1}]_{i,j}$: length of the shortest path from node $i$ to node $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Inference Functions\n",
    "\n",
    "* **Inference function:** maps an observation to an explanation\n",
    "* **The Few Inference Functions Theorem**\n",
    "\n",
    ">* The no. of inference fn. grows polynomially in the complexity of the graphical model. However, very few of the $k^{n(l)^n}$ mappings are inference functions. (e.g. for HMM, there are at most $C_{k,l} n^{k(k+l)}$ explanations)\n",
    "\n",
    "\n",
    "* **Example: HMM**\n",
    "\n",
    ">* A set of HMM parameters $\\theta$ specifies a particular inference fn.\n",
    "\n",
    ">$$\\hat{X} = \\underset{X}{\\text{argmax}} P_\\theta (Y,X)$$\n",
    "\n",
    ">* There are $k^{n(l)^n}$ possible functions mapping strings from $\\Delta^n$ to $\\Sigma^n$\n",
    "\n",
    ">* **Joint Probability:**\n",
    "\n",
    ">\\begin{align}\n",
    "P(W,T) &= \\prod_i p(w_i|t_i)p(t_i|t_{i-1}) \\\\\n",
    "&= \\prod_{w,t,t'} p(w|t)^{\\#_{w,t}(W,T)} p(t|t')^{\\#_{t,t'}(T)} \\\\\n",
    "\\log P(W,T) &= \\sum_{w,t,t'} \\#_{w,t}(W,T) \\log p(w|t) + \\#_{t,t'}(T) \\log p(t|t') \\\\\n",
    "&= \\sum_{w,t,t'} \\theta_{w,t} \\#_{w,t} (W,T) + \\theta_{t,t'} \\#_{t,t'}(T) = \\theta \\cdot \\#(W,T)\n",
    "\\end{align}\n",
    "\n",
    ">* **Condition:** $T_k = \\text{argmax}_T P(W_k,T)$ for all $k=1,...,l^n$\n",
    ">* Or equivalently,\n",
    "\n",
    ">$$\\theta \\cdot [\\#(W_k,T) - \\#(W_k,T_k)] \\leq 0 \\;\\;\\; \\forall T$$\n",
    "\n",
    "* **More Challenging Inference Problem**\n",
    "\n",
    ">* For a pair $(X',Y')$, find all parameters $\\theta$ s.t. $X'=\\text{argmax}_X P_{\\theta} (X,Y')$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
