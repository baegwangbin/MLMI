{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Feed Forward Neural Networks\n",
    "\n",
    "* **Recap on FFNN**\n",
    "\n",
    ">$$y^{(k)} = \\phi(U^{(k)}) = \\phi(W^{(k)} X^{(k)} + b^{(k)})$$\n",
    "\n",
    ">* $\\phi$: element-wise non-linear activation $\\phi_i(U^{(k)}) = \\phi (U^{(k)}_i)$\n",
    ">* Parameters: $\\Theta = \\{ W^{(k)}, b^{(k)} \\}^K_{k=1}$\n",
    "\n",
    "* **FFNN for Symbol Sequence Transduction**\n",
    "\n",
    ">$$y^{(1)} = \\phi \\left( W^{(1)}_{d \\times V_s} [s]_{V_s \\times 1} + b^{(1)}_{d \\times 1} \\right) \\in \\mathbb{R}^d$$\n",
    "\n",
    ">* **One-Hot Representations**\n",
    ">  * Source Vocabulary: $\\mathcal{V}_S = \\{s_1,...,s_{V_S}\\} \\rightarrow [s_i] \\in \\mathbb{R}^{V_S}$  \n",
    ">  * Target Vocabulary: $\\mathcal{V}_T = \\{s_1,...,s_{V_T}\\} \\rightarrow [f_i] \\in \\mathbb{R}^{V_T}$\n",
    "\n",
    "* **FFNN for Sentence Translation Probabilities** (hidden layer: bottleneck)\n",
    "\n",
    "><img src = 'images/image3_01.png' width=500>\n",
    "\n",
    ">$$P(t^J_1 | s^I_1, A) = \\prod_{(i,j) \\in A} P(t_j|s_i) = \\prod_{(i,j)\\in A} y_j (s_i)$$\n",
    "\n",
    ">* **Including Source Context**\n",
    "\n",
    ">$$P(t^J_1 | s^I_1, A) = \\prod_{(i,j) \\in A} P(t_j|s_{i-1},s_i,s_{i+1}) \\equiv \\prod_{(i,j)\\in A} y_j (c_i)$$\n",
    "\n",
    ">* **Including Target Context**\n",
    "\n",
    ">$$P(t^J_1 | s^I_1, A) = \\prod^J_{j=1} \\prod_{i:(i,j) \\in A} P(t_j|t_{j-1},s_{i-1},s_i,s_{i+1}) \\equiv \\prod^J_{j=1} \\prod_{i:(i,j) \\in A} y_j (c_i,j)$$\n",
    "\n",
    "* **Training FFNN**\n",
    "\n",
    ">* Word aligned parallel text $\\rightarrow$ training instances, $\\{(t^p,c^p)\\}^n_{p=1}$\n",
    "\n",
    ">$$E(\\Theta) = - \\sum^n_{p=1} \\log p(t^p|c^p)$$\n",
    "\n",
    ">* Since $[t]$ is one-hot-encoded, $U^{(K)}_j = [t]' U^{(K)} = e^{-[t]'W^{(K)}X^{(K)}}$\n",
    "\n",
    ">$$y_j (c) = \\frac{e^{-U_j^{(K)}}}{\\sum_{j'} e^{-U_{j'}^{(K)}}} = \\frac{e^{-[t]'W^{(K)}X^{(K)}}}{z(c;\\Theta)} = p(t|c)$$\n",
    "\n",
    ">* **Self-normalized softmax:** set $z(c;\\Theta) \\approx 1.0 \\; \\forall c$ or minimize the variance of $z$ (eliminating the need for softmax operation in evaluation, which is computationally expensive)\n",
    "\n",
    ">$$E(\\Theta) = \\sum^n_{p=1} [t^p]' W^{(K)} X^{(K)} + \\alpha [\\log z(c^p;\\Theta)]^2$$\n",
    "\n",
    "* **FFNN LM** (e.g. trigram)\n",
    "\n",
    ">$$P(t^J_1) = \\prod^J_{j=1} p(t_j|t_{j-1},t_{j-2})$$\n",
    "\n",
    ">* **Implement using FFNN** (weight matrix shared for different time-frames)\n",
    "\n",
    "><img src = 'images/image3_02.png' width=500>\n",
    "\n",
    ">* **Implement using WFSA**\n",
    "\n",
    ">$$(t_{j-2},t_{j-1}) \\overset{t_j / p(t_j|t_{j-1},t_{j-2})}{\\longrightarrow} (t_{j-1},t_{j}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. RNN LM\n",
    "\n",
    "* **RNNLM**\n",
    "\n",
    "><img src = 'images/image3_03.png' width=300>\n",
    "\n",
    ">* **Unrolled**\n",
    "\n",
    "><img src = 'images/image3_04.png' width=300>\n",
    "\n",
    ">\\begin{align}\n",
    "P(t_j|t_{<j}) &= \\text{softmax} (Rh_{j-1} + c) \\in [0,1]^{V_T} \\\\\n",
    "h_j &= \\tanh (W[t_j] + Uh_{j-1} + b) \\in \\mathbb{R}^d\n",
    "\\end{align}\n",
    "\n",
    ">* **Bidirectinal RNN** - concatenation of forward & reverse states\n",
    "\n",
    ">$$h(s^{T_s}_1) = \\left( \\begin{matrix} \\overrightarrow{h_{T_s}} \\\\ \\overleftarrow{h_1} \\end{matrix} \\right)$$\n",
    "\n",
    "* **Stacked RNN**\n",
    "\n",
    "><img src = 'images/image3_05.png' width=300>\n",
    "\n",
    ">\\begin{align}\n",
    "P(t_j|t_{<j}) &= \\text{softmax} (Rh^L_{j-1} + c) \\\\\n",
    "h^l_j &= \\tanh (W^l h^{l-1}_j + U^l h^l_{j-1} + b^l) \\\\\n",
    "h^0_j &= [t_j]\n",
    "\\end{align}\n",
    "\n",
    ">* $[h^l_j]$: representation of $t^j_1$\n",
    ">* Single vector $h_J \\in \\mathbb{R}^{d\\times L}$: representation of $t^J_1$\n",
    "\n",
    "* **GRU - Gated Recurrent Units**\n",
    "\n",
    ">* **Reset Gate**\n",
    "\n",
    "><img src = 'images/image3_06.png' width=250>\n",
    "\n",
    ">\\begin{align}\n",
    "r_j &= \\sigma(W_r x_j + U_r h_{j-1} + b_r) \\\\\n",
    "h_j &= \\tanh (W x_j + U_r (r_j \\odot h_{j-1}) + b_r)\n",
    "\\end{align}\n",
    "\n",
    ">* **Update Gate**\n",
    "\n",
    "><img src = 'images/image3_07.png' width=300>\n",
    "\n",
    ">\\begin{align}\n",
    "u_j &= \\sigma(W x_j + U_u h_{j-1} + b_u) \\\\\n",
    "h_j &= u_j \\odot \\tilde{h}_{j-1} + (1-u_j) \\odot h_{j-1} \\\\\n",
    "\\tilde{h}_j &= \\tanh (W x_j + U(r_j \\odot h_{j-1}) + b)\n",
    "\\end{align}\n",
    "\n",
    "* **LSTMs - Long Short-Term Memory units**\n",
    "\n",
    "><img src = 'images/image3_08.png' width=250>\n",
    "\n",
    ">\\begin{align}\n",
    "h^l_j &= \\mathcal{F}(h^l_{j-1}, h^{l-1}_j) \\\\\n",
    "c^l_j &= f \\odot c^l_{j-1} + i \\odot g \\\\\n",
    "h^l_j &= \\tanh(c^l_j) \\odot o\n",
    "\\end{align}\n",
    "\n",
    ">* Sigmoids: i,o,g,f\n",
    ">* $g\\approx 1, f\\approx 0, o\\approx 1 \\rightarrow$ RNN unit\n",
    ">* **LSTM with Dropout:** $g=\\sigma (WD(h^{l-1}_j) + uh^l_{j-1} + b)$\n",
    ">  * Applied to non-recurrent connections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
