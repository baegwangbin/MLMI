{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Statistical Models for Word Alignment in Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Word Alignment Models\n",
    "\n",
    "* **Word Alignment**\n",
    "\n",
    ">* **Alignment link:** $(i,j) \\Leftrightarrow e_i \\leftrightarrow f_j\\;\\;\\;$ ($e_0$: NULL)\n",
    ">* **Alignment process:** $a_j = i \\Leftrightarrow e_{a_j} \\leftrightarrow f_j$\n",
    ">* **NULL:** used when simple word-to-word translation is inadequate\n",
    "\n",
    "* **Statistical Model**\n",
    "\n",
    ">$$P(f^J_1, a^J_1, J|e^I_0) = \\underset{\\text{(word translation distribution)}}{\\prod^J_{j=1} p_T (\\;f_j|e_{a_j})} \\times \\underset{\\text{(alignment distribution)}}{P_A (a^J_1 | J,I)} \\times \\underset{\\text{(sentence length distribution)}}{p_L(J|I)}$$\n",
    "\n",
    ">* $p_L(J|I)$: simple but informative / tuned to language pairs and translation domains\n",
    ">* $p_T(f|e)$: huge table of probabilities / back-off prob. distributed equally over remaining words\n",
    ">* $P_A(a^J_1|J,I)$: completely unlexicalised / many models exist\n",
    ">* **Automatic Alignments:** $\\hat{a}_1^J = \\text{argmax}_{a^J_1} P(f^J_1,a^J_1,J|e^I_1)$\n",
    ">* **Translation Probability:** $P(f^J_1|e^I_1) = \\sum_{a^J_1} P(f^J_1,a^J_1,J|e^I_1)$\n",
    ">  * **Viterbi procedure** for Model-1 & Model-2\n",
    ">  * **Forward-backward algorithm** for HMM\n",
    "\n",
    "* **IBM Model-1** (flat)\n",
    "\n",
    ">$$p_{M2}(a_j|j,J,I) = \\frac{1}{I} \\;\\;\\;\\rightarrow\\;\\;\\; P_A(a^J_1|J,I) = \\frac{1}{I^J}$$\n",
    "\n",
    "* **IBM Model-2** (alignment links are independent)\n",
    "\n",
    ">$$p_{M2}(a_j|j,J,I) \\text{: table of dimension } I\\times J\\times I \\;\\;\\;\\rightarrow\\;\\;\\; P_A(a^J_1|J,I) = \\prod^J_{j=1} p_{M2}(a_j|j,J,I)$$\n",
    "\n",
    "* **HMM Model** (1st order Markov Process)\n",
    "\n",
    ">$$P_A (a^J_1|J,I) = \\prod^J_{j=1} P(a_j|a^{j-1}_1, J, I) \\approx \\prod^J_{j=1} p_{HMM} (a_j|a_{j-1},I)$$\n",
    "\n",
    ">* $p_{HMM} (\\cdot)$: Markov Position Alignment Distribution\n",
    "\n",
    "><img src = 'images/image4_01.png' width=400>\n",
    "\n",
    "* **IBM Model-4** (implemented in GIZA++ toolkit)\n",
    "\n",
    ">* **Generation Steps**\n",
    ">  * **Step 1:** Create a tablet for each source word (allow **Null translation**)\n",
    ">  * **Step 2:** **Fertility** (no. of target words each source word can generate)\n",
    ">  * **Step 3:** Fill in tablet positions with target words, sampled from **translation table**\n",
    ">  * **Step 4:** **Distortion model** (how target words are distributed throughout the sentence)\n",
    "\n",
    ">* **Strength**\n",
    ">  * Fertility & distortion $\\rightarrow$ capture **phrases** in translation\n",
    ">* **Weakness**\n",
    ">  * Deficiency: the distortion model may assign prob. to non-sentences (prob. over alignments and generated sentences do not sum to one)\n",
    ">  * Parameter estimation & word alignment $\\rightarrow$ difficult to implement (DP-based algorithms not available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Parameter Estimation for Alignment Models\n",
    "\n",
    "* **Translation Sentence Pairs**\n",
    "\n",
    ">$$\\{F^{(r)},E^{(r)}\\}^R_{r=1}$$\n",
    "\n",
    "* **Sentence Length Distribution**\n",
    "\n",
    ">$$P_L(J|I) = \\frac{\\sum^R_{r=1} \\mathbb{1}(J=J^{(r)}, I=I^{(r)})}{\\sum^R_{r=1} \\mathbb{1}(I=I^{(r)})}$$\n",
    "\n",
    "* **Word Translation Distribution** (require word alignment)\n",
    "\n",
    ">$$\\#_T(f\\leftrightarrow e) = \\sum^R_{r=1} \\sum^{J^{(r)}}_{j=1} \\sum^{I^{(r)}}_{i=1}\n",
    "\\mathbb{1}(e=e^{(r)}_i \\mathbb{1}(f=f^{(r)}_j) \\mathbb{1}(a^{(r)}_j = i)$$\n",
    "\n",
    ">$$P_T(f|e) = \\frac{\\#_T(f\\leftrightarrow e)}{\\sum_{f'} \\#_T(f'\\leftrightarrow e)}$$\n",
    "\n",
    "* **Alignment Distribution (Model-2)**\n",
    "\n",
    ">$$\\#_{M2}(i,j,J,I) = \\sum^R_{r=1} \\mathbb{1}(J=J^{(r)},I=I^{(r)}) \\sum^J_{j=1} \\mathbb{1}(i=a^{(r)}_j)$$\n",
    "\n",
    ">$$p_{M2}(i|j,J,I) = \\frac{\\#_{M2}(i,j,J,I)}{\\sum^I_{i'=1} \\#_{M2}(i',j,J,I)}$$\n",
    "\n",
    ">$$P_A(a^J_1|J,I) = \\prod^J_{j=1} p_{M2}(a_j|j,J,I)$$\n",
    "\n",
    "* **Alignment Distribution (HMM)**\n",
    "\n",
    ">$$\\#_{HMM} (i,i',I) = \\sum^R_{r=1} \\mathbb{1}(I=I^{(r)}) \\sum^{J^{(r)}}_{j=1} \\mathbb{1}(i=a^{(r)}_j, i'=a^{(r)}_{j-1}) $$\n",
    "\n",
    ">$$p_{HMM}(i|i',I) = \\frac{\\#_{HMM}(i,i',I)}{\\sum^I_{i''=1} \\#_{HMM}(i'',i',I)}$$\n",
    "\n",
    ">$$P_A (a^J_1|J,I) = \\prod^J_{j=1} P(a_j|a^{j-1}_1, J, I) \\approx \\prod^J_{j=1} p_{HMM} (a_j|a_{j-1},I)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Automatic Word Alignment - Viterbi Training\n",
    "\n",
    "* **Viterbi Alignment**\n",
    "\n",
    ">$$\\hat{a}^J_1 = \\underset{a^J_1}{\\text{argmax}} P(f^J_1, a^J_1 | e^I_0)$$\n",
    "\n",
    "* **Flat Start Training Procedure**\n",
    "\n",
    ">* **1. Model-1**\n",
    "\n",
    ">>* **1.1. Model-1 Initialization** - set $p_T(f|e)$ to be uniform\n",
    ">>* **1.2. Model-1 Viterbi alignment** (to generate word-aligned parallel text)\n",
    ">>* **Repeat** (until some stopping criteria is met)\n",
    ">>  * **1.3. Update $p_T(f|e)$** (from the word-aligned parallel text)\n",
    ">>  * **1.4. Viterbi alignment**\n",
    ">>* **1.5. Output:** Model-1 word-aligned parallel text\n",
    "\n",
    ">* **2. Model-2**\n",
    "\n",
    ">>* **2.1. Model-2 Initialization** - find $p_{M2}(i|j,J,I)$ and $p_T(f|e)$ from **1.5.**\n",
    ">>* **2.2. Model-2 Viterbi alignment**\n",
    ">>* **Repeat**\n",
    ">>  * **2.3. Update $p_{M2}(i|j,J,I)$ and $p_T(f|e)$** \n",
    ">>  * **2.4. Viterbi alignment**\n",
    ">>* **2.5. Output:** Model-2 word-aligned parallel text\n",
    "\n",
    ">* **3. HMM**\n",
    "\n",
    ">>* **3.1. HMM Initialization** - find $p_{HMM}(i|i',I,J)$ and $p_T(f|e)$ from **2.5.**\n",
    ">>* **3.2. HMM Viterbi alignment**\n",
    ">>* **Repeat**\n",
    ">>  * **3.3. Update $p_{HMM}(i|i',I,J)$ and $p_T(f|e)$** \n",
    ">>  * **3.4. Viterbi alignment**\n",
    ">>* **3.5. Output:** HMM parameters & word-aligned parallel text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Iterative Alignment Model Parameter Estimation by EM\n",
    "\n",
    "* Replace hard counts with **link posteriors**\n",
    "\n",
    ">$$\\#(f\\leftrightarrow e) = \\sum^R_{r=1} \\sum^{J^{(r)}}_{j=1} \\sum^{I^{(r)}}_{i=1}\n",
    "\\mathbb{1}(e=e^{(r)}_i \\mathbb{1}(f=f^{(r)}_j) \\underset{\\text{(link posterior)}}{P(a^{(r)}_j = i | E^{(r)},F^{(r)})}$$\n",
    "\n",
    ">$$P(f|e) = \\frac{\\#(f\\leftrightarrow e)}{\\sum_{f'} \\#(f'\\leftrightarrow e)}$$\n",
    "\n",
    "* **Model-2**\n",
    "\n",
    ">\\begin{align}\n",
    "\\text{sentence-level posterior:}\\;\\;\\; P(f^J_1|e^J_1) &= \\prod^J_{j=1} \\sum^I_{i=0} p_{M2}(i|j,I,J) p_T(f_j|e_i) \\\\\n",
    "\\text{link posterior:}\\;\\;\\; P(a_j=i|f^J_1,e^I_1) &= \\frac{p_{M2}(i|j,I,J)p_T(f_j|e_i)}{\\sum^I_{i'=0} p_{M2}(i'|j,I,J)p_T(f_j|e_{i'})} \\equiv A_j(a_j) \\\\\n",
    "P(a^J_1|f^J_1,e^I_1) &= \\prod^J_{j=1} A_j(a_j)\n",
    "\\end{align}\n",
    "\n",
    "* **Model-1** (special case of Model-2)\n",
    "\n",
    ">\\begin{align}\n",
    "P(f^J_1,a^J_1|e^I_1) &= \\frac{1}{(I+1)^J} \\prod^J_{j=1} p_T(f_j|e_i) \\\\\n",
    "P(a_j=i|e^I_1,f^J_1) &= \\frac{p_T(f_j|e_i)}{\\sum^I_{i'=0} p_T(f_j|e_i')}\n",
    "\\end{align}\n",
    "\n",
    "* **HMM**\n",
    "\n",
    ">$$P(f^J_1,a^J_1|e^I_1,J) = \\prod^J_{j=1} p_T(f_j|e_{a_j}) p_{HMM}(a_j|a_{j-1},I)$$\n",
    "\n",
    ">* **Forward Probabilities**\n",
    "\n",
    ">\\begin{align}\n",
    "\\alpha_j(i) &= P(a_j=i,f^j_1|e^I_1) \\\\\n",
    "&= \\sum_{i'} p_T(f_j|e_i) p_{HMM}(a_j=i|a_{j-1}=i') \\alpha_{j-1}(i')\n",
    "\\end{align}\n",
    "\n",
    ">* **Backward Probabilities**\n",
    "\n",
    ">\\begin{align}\n",
    "\\beta_j(i) &= P(f^J_{j+1}|a_j=i,e^I_1) \\\\\n",
    "&= \\sum_{i'} \\beta_{j+1}(i') p_T(f_{j+1}|e_{i'}) p_{HMM}(a_{j+1}=i'|a_j=i)\n",
    "\\end{align}\n",
    "\n",
    ">* **Link Posteriors**\n",
    "\n",
    ">$$P(a_j=i,f^J_1|e^I_1) = P(a_j=i,f^j_1|e^I_1) P(f^J_{j+1}|a_j=i,e^I_1) = \\alpha_j(i) \\beta_j(i)$$\n",
    "\n",
    ">\\begin{align}\n",
    "P(a_j=i,a_{j-1}=i'|f^J_1,e^I_1) &= \\frac{P(a_j=i,a_{j-1}=i',f^J_1,e^I_1)}{P(f^J_1|e^I_1)} \\\\\n",
    "&= \\frac{\\alpha_{j-1}(i') p_{HMM}(i|i') p_T(f_j|e_i) \\beta_j(i) }{ \\sum_i \\alpha_J(i)}\n",
    "\\end{align}\n",
    "\n",
    "* **Model 4**\n",
    "\n",
    ">* No Viterbi algorithm / No EM algorithm\n",
    ">* There are computationally tractable HMMs that capture Model-4 features of fertility and word-to-phrase translation\n",
    "\n",
    "* **Parallel EM - Hadoop/MapReduce**\n",
    "\n",
    ">* EM algorithm: easily parallelizable\n",
    ">* **Mappers:** compute counts (**E-steps**)\n",
    ">* **Reducers:** merge counts & produce probabilities (**M-steps**)\n",
    ">* Uses many CPU $\\rightarrow$ better than using 1 GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Document and Sentence Alignment\n",
    "\n",
    "* **Segment Alignment** specified by a path on **Alignment Grid**\n",
    "\n",
    "><img src = 'images/image4_02.png' width=300>\n",
    "\n",
    ">* **Score 1:** Prob. distribution defined over alignment paths\n",
    ">  * Paths near the diagonal with shorter segments are preferred\n",
    ">* **Score 2:** Translation prob. defined over aligned segment pairs\n",
    ">  * Employs simple model such as IBM Model-1\n",
    ">* **Combine two scores** to assign a prob. to a particular segmentation and alignment\n",
    ">* Assume **monotonic alignment** $\\rightarrow$ reduce search space\n",
    "\n",
    "* **Binary Divisive Clustering**\n",
    "\n",
    ">* From coarse to fine / allow reordering\n",
    ">* At each iteration, choose the single most likely splitting point"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
