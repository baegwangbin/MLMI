{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. HMM Speech Recognition - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Hidden Markov Models\n",
    "\n",
    "* **HMM: Generative Model of Speech**\n",
    "\n",
    ">$$p(\\mathbf{O},\\mathbf{X}|\\lambda) = a_{x(0), x(1)} \\prod^T_{t=1} b_{x(t)}(\\mathbf{o}_t) a_{x(t),x(t+1)}$$\n",
    "\n",
    ">* $N$ states: **entry** state, $N-2$ emitting states, **exit** state\n",
    ">* Common choise: $b_j(\\mathbf{o}) = \\mathcal{N} (\\mathbf{o}; \\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)$ $\\rightarrow$ $\\boldsymbol{\\Sigma}$ is not diagonal (high correlation)\n",
    ">* Observed data: $\\mathbf{O} = \\{\\mathbf{o}_1,...,\\mathbf{o}_T\\}$\n",
    ">* Hidden state sequence: $\\mathbf{X} = \\{x(1),...,x(T)\\}$ / $x(0)$: entry / $x(T+1)$: exit\n",
    "\n",
    "* **HMM - Likelihood**\n",
    "\n",
    ">$$p(\\mathbf{O}|\\lambda) = \\sum_{\\mathbf{X}} p(\\mathbf{O}, \\mathbf{X} | \\lambda)$$\n",
    "\n",
    ">* Sum over all possible state sequences $\\rightarrow$ impractical\n",
    ">* Alternatives: **forward-backward algorithm** and **Viterbi algorithm**\n",
    "\n",
    "* **HMM - Extensions**\n",
    "\n",
    ">* GMM-HMM & DNN-HMM\n",
    ">* Continuous Density HMM & Discrete Density HMM\n",
    "\n",
    "* **Delta & Delta-Delta parameters**\n",
    "\n",
    ">$$\\boldsymbol{\\Delta y}_t = \\frac{\\sum^D_{\\tau=1} \\tau (\\boldsymbol{y}_{t+\\tau} - \\boldsymbol{y}_{t-\\tau})}{2 \\sum^D_{\\tau=1} \\tau^2}$$\n",
    "\n",
    ">$$\\boldsymbol{\\Delta}^2\\boldsymbol{y}_t = \\frac{\\sum^D_{\\tau=1} \\tau (\\boldsymbol{\\Delta y}_{t+\\tau} - \\boldsymbol{\\Delta y}_{t-\\tau})}{2 \\sum^D_{\\tau=1} \\tau^2}$$\n",
    "\n",
    ">* $e_t$: **Normalised log-energy**\n",
    ">* **12 MFCCs $\\rightarrow$ 39-dim feature vector**\n",
    "\n",
    "* **Isolated Word Recognition**\n",
    "\n",
    ">$$\\boldsymbol{O} \\rightarrow p(\\boldsymbol{O}|\\mathcal{M}_i) \\rightarrow \\underset{i}{\\text{argmax}} \\; p(\\boldsymbol{O}|\\mathcal{M}_i)$$\n",
    "\n",
    ">* In practice, $p(\\boldsymbol{O}|\\mathcal{M}_i)$ is estimated based on the **most likely state sequence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Composite HMMs\n",
    "\n",
    "* **Composite HMMs**\n",
    "\n",
    ">* Smaller HMMs $\\rightarrow$ Composition & Concatenation $\\rightarrow$ Larger HMM\n",
    ">* Sub-word units $\\rightarrow$ Word models $\\rightarrow$ Sentence models\n",
    "\n",
    "* **Recognition with a Composite Model**\n",
    "\n",
    ">* **Isolated:** find the optimal state sequence and hence find the model\n",
    ">* **Continuous:** create **sentence generators** using composite HMMs / use **token passing** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Viterbi Algorithm\n",
    "\n",
    "* **Viterbi Algorithm**\n",
    "  * Efficient (search time linear in $T$)\n",
    "  * Prevent numerical overflow by using $\\log \\phi_j (t)$\n",
    "\n",
    ">* $\\phi_j (t)$: probability of 'best' partial path of length $t$ through the trellis ending in state $j$\n",
    ">* $X^{(t-1)}$: set of all partial paths of length $t-1$\n",
    "\n",
    ">$$\\phi_j (t) = \\underset{X^{(t-1)}}{\\max} \\{ p(\\boldsymbol{o}_{1:t}, x(t)=j | \\mathcal{M}_i) \\} = \\underset{i}{\\max} \\{ \\phi_i (t-1) a_{ij} b_j (\\boldsymbol{o}_t) \\}$$\n",
    "\n",
    ">* **Step 1: Initialisation**\n",
    "\n",
    ">$$\\phi_1(0) = 1.0 \\;\\;\\; \\phi_{1<j<N}(0) = 0.0 \\;\\;\\; \\phi_1(1 \\leq t \\leq T) = 0.0$$\n",
    "\n",
    ">* **Step 2: Recursion**\n",
    "\n",
    ">`for t = 1,...,T`\n",
    "\n",
    ">`for j = 2,...,N-1`\n",
    "\n",
    ">`compute` $\\phi_j (t) = \\underset{1 \\leq k < N}{\\max} [\\phi_k (t-1) a_{kj}] b_j (\\boldsymbol{o}_t)$\n",
    "\n",
    ">* **Step 3: Termination**\n",
    "\n",
    ">$$p(\\boldsymbol{O}, \\boldsymbol{X}^* | \\lambda) = \\underset{1<k<N}{\\max} \\phi_k (T) a_{kN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Viterbi Training\n",
    "\n",
    "* **MLE** (find the HMM model parameters)\n",
    "\n",
    ">$$\\hat{\\lambda} = \\underset{\\lambda}{\\text{argmax}} \\Big\\{ \\prod^R_{r=1} p(\\boldsymbol{O}^{(r)}|\\lambda) \\Big\\}$$\n",
    "\n",
    "* **Viterbi** algorithm (based on **best state sequence**, $\\boldsymbol{X}^*$)\n",
    "\n",
    ">$$p(\\boldsymbol{O}|\\lambda) \\approx p(\\boldsymbol{O},\\boldsymbol{X}^*|\\lambda)$$\n",
    "\n",
    "* **Transition Parameters**\n",
    "\n",
    ">$$a_{ij} = \\frac{\\text{No. of transitions } i \\rightarrow j}{\\text{No. of transitions from } i}$$\n",
    "\n",
    "* **Gaussian Parameter Estimation**\n",
    "\n",
    ">\\begin{align}\n",
    "\\hat{\\boldsymbol{\\mu}}_j &= \\frac{ \\sum^R_{r=1} \\sum^{T^{(r)}}_{t=1} \\delta (x^{(r)^*} (t) = j) \\boldsymbol{o}_t^{(r)}}{\\sum^R_{r=1} \\sum^{T^{(r)}}_{t=1} \\delta (x^{(r)^*} (t) = j)} \\\\\n",
    "\\hat{\\boldsymbol{\\Sigma}}_j &= \\frac{ \\sum^R_{r=1} \\sum^{T^{(r)}}_{t=1} \\delta (x^{(r)^*} (t) = j) (\\boldsymbol{o}_t^{(r)}-\\hat{\\boldsymbol{\\mu}}_j) (\\boldsymbol{o}_t^{(r)}-\\hat{\\boldsymbol{\\mu}}_j)' }{\\sum^R_{r=1} \\sum^{T^{(r)}}_{t=1} \\delta (x^{(r)^*} (t) = j)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "* **Iterative Training**\n",
    "\n",
    ">* **Step 1:** Viterbi algorithm (find best state sequence)\n",
    ">* **Step 2:** Update HMM parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
