{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLMI2: Speech Recognition\n",
    "\n",
    "Lecturer: Prof. Phil Woodland\n",
    "\n",
    "----\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    ">## 1. Introduction\n",
    "* 1.1. Human Speech\n",
    "* 1.2. ASR\n",
    "* 1.3. Recognition Model Architecture\n",
    "\n",
    ">## 2. Speech Signal Processing\n",
    "* 2.1. Quasi-Stationary Assumption\n",
    "* 2.2. Digital Signals\n",
    "* 2.3. Windowing\n",
    "* 2.4. Fourier Transform\n",
    "* 2.5. DFT - Discrete Fourier Transform\n",
    "* 2.6. Complex Formulation of DFT\n",
    "* 2.7. Spectral Properties of Speech\n",
    "* 2.8. Spectral Features of Sounds\n",
    "* 2.9. Spectrograms\n",
    "\n",
    ">## 3. Linear Prediction and Cepstral Analysis\n",
    "* 3.1. Source-Filter Model\n",
    "* 3.2. General Linear Constant Coefficient Difference Equation\n",
    "* 3.3. Linear Prediction\n",
    "* 3.4. Autocorrelation Method\n",
    "* 3.5. Inverse Filtering\n",
    "* 3.6. LP Spectrum\n",
    "* 3.7. Cepstral Analysis\n",
    "\n",
    ">## 4. HMM Speech Recognition - Introduction\n",
    "* 4.1. Hidden Markov Models\n",
    "* 4.2. Composite HMMs\n",
    "* 4.3. Viterbi Algorithm\n",
    "* 4.4. Viterbi Training\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Human Speech\n",
    "\n",
    "* **Speech Waveform**\n",
    "\n",
    ">1. **Non-stationary** \n",
    ">2. Mixture of **pseudo-periodic** $+$ **random** components\n",
    "\n",
    "* **Human Speech Production Mechanism**\n",
    "\n",
    ">1. **Acoustic tube**: variably-shaped / responsible for detailed sounds\n",
    ">2. **Excitation source**: responsible for broad distinctions in speech sound\n",
    ">  1. **Voiced sounds** $\\leftarrow$ Vocal cords which vibrate when air from the lungs is forced through them\n",
    ">  2. **Fricative sounds** $\\leftarrow$ Turbulence caused by forcing air through constrictions formed by raising the tongue to narrow the acoustic tube\n",
    ">  3. **Plosive sounds** $\\leftarrow$ Turbulence caused by the release of air following a complete closure of the acoustic tube\n",
    "\n",
    "* **Elements of Human Speech**\n",
    "\n",
    ">* **Speech**: Sequence of **phones**\n",
    ">  * **Phones**: Directly associated with **phonemes**\n",
    ">    * **Phonemes**: Abstraction of phones / basic units of speech\n",
    ">      * **ARPAbet**: American English phone set\n",
    ">* **Consonant Classification**\n",
    ">  * 5 classes based on the type of vocal tract constriction\n",
    ">  * $\\Rightarrow$ plosive(stops), fricatives, affricates, liquids(semi-vowels), nasals\n",
    ">* **Vowel Classification**\n",
    ">  * Based on **tongue position** (front to back) & **jaw position** (low to high)\n",
    ">  * $\\Rightarrow$ Represented by **vowel quadrilateral** (use arrows for **diphthongs**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. ASR\n",
    "\n",
    ">$$\\text{Changing time signal} \\rightarrow \\text{Corresponding symbol sequence}\\;(\\textbf{string of words})$$\n",
    ">* We need a **sequence-to-sequence** model\n",
    ">* This is **NOT** about understanding the input\n",
    "\n",
    "* **Several Styles of ASR**\n",
    "\n",
    ">||Input Mode|Vocab Size|Basic Unit|Grammar|\n",
    "|-|-|-|-|-|\n",
    "|**Isolated**|discrete|small|word|none|\n",
    "|**Continuous**|continuous|medium|phone|FS/N-gram|\n",
    "|**Discrete large vocab**|discrete|large|phone|N-gram|\n",
    "|**Continuous large vocab**|continuous|large|phone|N-gram|\n",
    "\n",
    "* **Performance-metric for ASR**\n",
    "\n",
    ">* **WER(Word Error Rate)** for isolated word recognition\n",
    ">$$\\text{%WER}=100\\times\\frac{N(\\text{correct})}{N(\\text{reference})}$$\n",
    ">$$\\;$$\n",
    ">* **WER** for continuous speech recognition\n",
    ">$$\\text{%WER}=100\\times\\frac{N(\\text{ins.})+N(\\text{del.})+N(\\text{sub.})}{N(\\text{reference})}$$\n",
    "\n",
    ">* **User Satisfaction**\n",
    "\n",
    "* **The performance of ASR is affected by**\n",
    "\n",
    ">* **Inter-speaker variability** (important to develop **recognizer robustness**)\n",
    ">* **Intra-speaker variability** (e.g. situation, aging, ...)\n",
    ">* **Microphone** (close vs far / fixed vs variable / bandwidth)\n",
    ">* **Background noise** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Recognition Model Architecture\n",
    "\n",
    "* **Feature Extraction**\n",
    "\n",
    ">* Normally) use a **fixed duration** of speech signal (**frame**)\n",
    ">* Each frame corresponds to a **feature vector**\n",
    ">* Spectral information is often encoded as a **cepstral representation** (e.g. MFCCs)\n",
    "\n",
    "* **Generative model approach**\n",
    "\n",
    "\n",
    ">$$\\hat{W}=\\underset{W}{\\arg\\max}\\;{P(\\textbf{O}|W)P(W)}$$\n",
    ">* $\\textbf{O}$: Stream of feature vectors describing the utterance\n",
    ">* $P(\\textbf{O}|W)$: given by acoustic model\n",
    ">* $P(W)$: given by language model\n",
    "\n",
    "* **Hidden Markov Model (HMM)**\n",
    "\n",
    ">* **GMM-HMM**: uses Gaussian Mixture Models\n",
    ">* **DNN-HMM**: uses Deep Neural Networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
