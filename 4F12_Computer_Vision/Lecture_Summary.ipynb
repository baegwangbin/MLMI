{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. What is Computer Vision?\n",
    ">$$\\textbf{Images of a real scene} \\rightarrow \\textbf{Useful information (3R's)}$$\n",
    ">\n",
    ">$$\\text{Images} \\rightarrow \\text{Representation}$$\n",
    ">\n",
    ">$$\\text{Perception} \\rightarrow \\text{Actions}$$\n",
    "\n",
    ">* **3R's**: Registration, Recognition, and Reconstruction\n",
    ">* It is different from **Image Processing** or **Pattern Recognition**\n",
    ">* **Applications**: autonomous vehicles, medical diagnosis, augmented reality, face recognition, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Human Eye vs. Camera\n",
    ">* The numbers are rough estimate\n",
    "\n",
    ">||Human Eye|Typical CCD|\n",
    "|-|-|-|\n",
    "|Measures|$1000\\text{ mm}^2$|$24\\text{ mm }\\times16\\text{ mm }$|\n",
    "|No. of Sensors|$2 \\text{ Eyes}$|$1 \\text{ CCD Sensor}$|\n",
    "|Sampling Elements|$10^8\\text{ rods, and } 10^6\\text{ cones}$|$6\\times10^6 \\text{ pixels}$|\n",
    "|Spatial Resolution|$0.01^\\text{o} \\text{ over } 150^\\text{o} \\text{ field of view}$|$\\text{Lens-dependent}$|\n",
    "|Temporal Resolution|$100\\text{ ms}$|$40\\text{ ms}$|\n",
    "|Intensity Resolution|$11\\text{ bits/element}$|$8\\text{ bits/element for each colour channel}$|\n",
    "|Spectral Resolution|$2\\text{ bits/element}$|$8\\text{ bits/element for each colour channel}$|\n",
    "|Data Rate|$3\\text{GB/s}$|$400\\text{MB/s}$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Problems in Computer Vision\n",
    "\n",
    ">**1. Image Formation**\n",
    ">  * Many-to-one mapping, Encodes nothing about the depth\n",
    ">  * The inverse imaging problem has no unique solution\n",
    "\n",
    ">**2. Vision as Information Processing**\n",
    ">  * Images $\\rightarrow$ Generic salient features $\\rightarrow$ Representations and Actions\n",
    ">  * $10 \\text{ MB/s} \\rightarrow 10 \\text{ KB/s} \\rightarrow 1\\text{~}10\\text{ bits/s}$ (Data reduction process)\n",
    "\n",
    ">**3. Feature Extraction**\n",
    ">  * Reduce the data content while preserving the useful information (e.g. **Edge & Corner**)\n",
    "\n",
    ">**4. Camera Models**\n",
    ">  * Account for the camera position, perspective projection, and CCD imaging\n",
    ">  * Predict how known objects will appear in an image $\\rightarrow$ **Object recognition**\n",
    "\n",
    ">**5. Stereo Vision**\n",
    ">  * Two cameras $\\rightarrow$ Match the two images (**Correspondence problem**) $\\rightarrow$ Obtain depth\n",
    ">  * Possible to infer information even when the cameras are **not calibrated**\n",
    "\n",
    ">**6. Structure from Motion**\n",
    ">  * Camera moves $\\rightarrow$ Track features $\\rightarrow$ Infer the structures in the scene & the motion of camera\n",
    ">  * Sensitive to independently moving objects\n",
    "\n",
    ">**7. Shape from Texture**\n",
    ">  * Assume **homogeneous** or **isotropic** texture $\\rightarrow$ infer the orientation of surfaces\n",
    "\n",
    ">**8. Shape from Line Drawing**\n",
    "\n",
    ">**9. Shape from Contour**\n",
    ">  * Each **apparent contour** defines a set of **tangent planes** from the camera to the surface\n",
    ">  * Analyse the deformation of the apparent contours in the image $\\rightarrow$ Infer the shape\n",
    "\n",
    ">**10. Shape from Shading**\n",
    ">  * Assume Lambertian light source, isotropic surface reflectance, and a top-lit scene $\\rightarrow$ Infer the shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Extraction and Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Objectives\n",
    ">**1. Reduce** the amount of data\n",
    "\n",
    ">**2. Preserve** the useful information (e.g. edge, corner, shape)\n",
    "\n",
    ">**3. Discard** the redundant information (e.g. lighting conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. 1D Edge Detection\n",
    "* **Step 1. Smooth the signal to suppress noise**\n",
    "\n",
    "  * Convolve the signal $I(x)$ with a Gaussian kernel $g_\\sigma(x)$\n",
    "  * **Small $\\sigma \\rightarrow$** Large cutoff frequency $\\rightarrow$ **Preserve detail**\n",
    "  * **Large $\\sigma \\rightarrow$** Small cutoff frequency $\\rightarrow$ **Suppress detail**\n",
    "\n",
    ">$$g_\\sigma(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)$$\n",
    ">$$\\;$$\n",
    ">\\begin{align}\n",
    "s(x)=g_\\sigma(x) * I(x)&=\\int_{-\\infty}^{\\infty}g_\\sigma(u)I(x-u)du\\\\\n",
    "&=\\int_{-\\infty}^{\\infty}g_\\sigma(x-u)I(u)du\n",
    "\\end{align}\n",
    "\n",
    "* **Step 2-a. Compute $s'(x)$ and look for maxima & minima**\n",
    "\n",
    "  * Instead of doing two convlutions,\n",
    "  * Using the derivative theorem of convolution,\n",
    "\n",
    ">$$s'(x)=\\frac{d}{dx} [ g_\\sigma(x) * I(x) ] = g'_\\sigma(x)*I(x)$$\n",
    "\n",
    "* **Step 2-b. Compute $s''(x)$ and look for zero-crossings**\n",
    "\n",
    "  * The signal is convolved with the **Laplacian of a Gaussian**\n",
    "\n",
    ">$$s''(x)=g''_\\sigma(x)*I(x)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. 2D Edge Detection - Canny (1986)\n",
    "* **Step 1. Convolve with a 2D Gaussian $G_\\sigma(x,y)$**\n",
    "\n",
    ">\\begin{align}\n",
    "G_\\sigma(x,y)&=\\frac{1}{2\\pi\\sigma^2}\\exp\\left(-\\frac{x^2+y^2}{2\\sigma^2}\\right)\\\\\n",
    "\\\\\n",
    "S(x,y)&=G_\\sigma(x,y)*I(x,y)\\\\\n",
    "\\\\\n",
    "&=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}G_\\sigma(u,v)I(x-u,y-v)du dv\n",
    "\\end{align}\n",
    "\n",
    "* **Step 2. Find the Gradient of $S(x,y)$**\n",
    "\n",
    ">\\begin{align}\n",
    "\\nabla S &=\\nabla (G_\\sigma*I) \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial (G_\\sigma *I)}{\\partial x} \\\\\n",
    "    \\frac{\\partial (G_\\sigma *I)}{\\partial y} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial G_\\sigma}{\\partial x} *I \\\\\n",
    "    \\frac{\\partial G_\\sigma}{\\partial y} *I \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "* **Step 3. Non-maximal Suppression**\n",
    "\n",
    ">$$\\text{Edge elements (edgels) are placed where } \\mid \\nabla S \\mid \\text{ is greater than local values in the direction of } \\pm \\nabla S $$\n",
    "  \n",
    "* **Step 4. Threshold the Edgels**\n",
    "\n",
    ">$$\\text{Output: edgel positions, each with strength } \\mid \\nabla S \\mid \\text{ and orientation } \\nabla S \\text{/} \\mid \\nabla S \\mid$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. 2D Edge Detection - Marr and Hildreth (1980)\n",
    "* Unlike Canny edge detector, Marr-Hildreth operator is **isotropic**\n",
    "* **Algorithm: Find the zero-crossings of:**\n",
    "\n",
    ">$$\\nabla^2G_\\sigma*I$$\n",
    ">$$\\;$$\n",
    ">$$\\text{where } \\nabla^2G_\\sigma=\\left( \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2} \\right) G_\\sigma \\text{ is the Laplacian of } G_\\sigma$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Implementation Details\n",
    "* **Convolution**\n",
    "\n",
    ">$$S(x,y)=\\sum_{u=-n}^{n} \\sum_{v=-n}^{n} G_\\sigma(u,v)I(x-u,y-v)$$\n",
    ">\n",
    ">* Kernels are trucated so that the discarded samples are less than $1/1000$ of the peak value\n",
    ">* $\\pm 3\\sigma$ is often enough\n",
    ">* Smaller $n$ leads to sharp discontinuity (avoid!)\n",
    ">\n",
    "|$\\sigma$|1.0|1.5|3|6|\n",
    "|-|-|-|-|-|\n",
    "|$2n$ $+1$|7|11|23|45|\n",
    ">\n",
    ">* 2D Convolution: computationally expensive!\n",
    ">* Decompose it into two 1D convolutions\n",
    ">$$G_\\sigma(x,y)*I(x,y)=g_\\sigma(x)*[g_\\sigma(y)*I(x,y)]$$\n",
    ">\n",
    ">$$\\mathcal{O} \\left((2n+1)^2 \\right) \\text{ vs. } \\mathcal{O} \\left(2(2n+1)\\right)$$\n",
    "\n",
    "* **Differentiation** \n",
    "\n",
    ">$$\\frac{\\partial S}{\\partial x} \\approx \\frac{S(x+1,y)-S(x-1,y)}{2}$$\n",
    ">\n",
    ">* The above is the result of the **Taylor Expansion** of $S(x,y)$\n",
    ">* This is equivalent to convolving the rows of image samples with the kernel $(1/2, 0, -1/2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Corner Detection - Harris (1987)\n",
    "\n",
    "* **Corner Detection** - Important in **tracking** objects and **matching** stereo pairs\n",
    "* Important in understanding **kinetic effect** and **ego-motion**\n",
    "* **Step 1. Calculate the change in intensity in direction $\\textbf{n}$**\n",
    "\n",
    "  * Here, $I$ is already smoothed\n",
    "\n",
    ">\\begin{align}\n",
    "I_n &\\equiv \\nabla I(x,y) \\cdot \\widehat{\\textbf{n}} \\equiv \\begin{bmatrix} I_x & I_y \\end{bmatrix}^T \\cdot \\widehat{\\textbf{n}}\\\\\n",
    "\\;\\\\\n",
    "I_n^2 &= \\frac{\\textbf{n}^T \\nabla I \\nabla I^T \\textbf{n}}{\\textbf{n}^T\\textbf{n}}\\\\\n",
    "\\;\\\\\n",
    "&= \\frac{\\textbf{n}^T \\begin{bmatrix} \n",
    "I_x^2 & I_xI_y \\\\\n",
    "I_xI_y & I_y^2\n",
    "\\end{bmatrix}\n",
    "\\textbf{n}}{\\textbf{n}^T\\textbf{n}}\n",
    "\\end{align}\n",
    ">$$\\;$$\n",
    ">$$\\text{where } I_x \\equiv \\frac{\\partial I}{\\partial x} \\text{ and } I_y \\equiv \\frac{\\partial I}{\\partial y}$$\n",
    "\n",
    "* **Step 2. Smooth $I_n^2$ by convolution with a Gaussian kernel**\n",
    "\n",
    ">\\begin{align}\n",
    "C_n(x,y) &= G_\\sigma (x,y)*I^2_n \\\\\n",
    "\\;\\\\\n",
    "&= \\frac{\\textbf{n}^T \\begin{bmatrix} \n",
    "\\left\\langle I_x^2 \\right\\rangle & \\left\\langle I_xI_y \\right\\rangle\\\\\n",
    "\\left\\langle I_xI_y \\right\\rangle & \\left\\langle I_y^2 \\right\\rangle\\\\\n",
    "\\end{bmatrix}\n",
    "\\textbf{n}}{\\textbf{n}^T\\textbf{n}}\n",
    "\\end{align}\n",
    ">$$\\;$$\n",
    ">$$\\text{where } \\left\\langle \\right\\rangle \\text{ is the smoothed value}$$\n",
    "\n",
    "* **Step 3. Use the eigenvalues of $\\text{A}$ to determine the structure**\n",
    "\n",
    ">$$C_n(x,y)=\\frac{\\textbf{n}^T \\text{A} \\textbf{n}}{\\textbf{n}^T\\textbf{n}}$$\n",
    ">$$\\;$$\n",
    ">$$\\text{where,} \\;\\;\\; \\text{A}=\\begin{bmatrix} \n",
    "\\left\\langle I_x^2 \\right\\rangle & \\left\\langle I_xI_y \\right\\rangle\\\\\n",
    "\\left\\langle I_xI_y \\right\\rangle & \\left\\langle I_y^2 \\right\\rangle\\\\\n",
    "\\end{bmatrix}$$\n",
    ">$$\\;$$\n",
    ">$$\\text{then,} \\;\\;\\; \\lambda_1 \\leq C_n(x,y) \\leq\\lambda_2$$\n",
    ">\n",
    ">* **$\\text{A}$**: Structure Tensor\n",
    ">* **No structure**(smooth): $\\lambda_1 \\approx \\lambda_2 \\approx 0$\n",
    ">* **1D structure**(edge): $\\lambda_1 \\approx 0$ (direction of edge), $\\lambda_2$ is large (normal to edge)\n",
    ">* **2D structure**(corner): $\\lambda_1$ and $\\lambda_2$ both large and distinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Setting Threshold for Corner Detection\n",
    "* **If $M_c$ exceeds some threshold $\\rightarrow$ Mark corners**\n",
    "\n",
    ">$$M_c=\\lambda_1 \\lambda_2-\\kappa(\\lambda_1+\\lambda_2)^2$$\n",
    ">$$\\;$$\n",
    ">$$M_c=\\det{\\text{A}} - \\kappa \\text{ tr A}$$\n",
    ">* The second form is preferable (low computational cost)\n",
    ">* $\\kappa$: generally range from $0.04$ to $0.15$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8. Blobs\n",
    "* Blob: area of uniform intensity in the image\n",
    "* They are localised in the middle of areas of similar intensity\n",
    "* Convolve with the **Laplacian of the Gaussian** $\\rightarrow$ Locate minimum\n",
    "* As $\\sigma$ increases, larger image features are detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9. Scale Space\n",
    "* Achieve scale independence by looking at different resolutions\n",
    "* **Scale Space**\n",
    "\n",
    ">\\begin{align}\n",
    "L(x,y,t)&=G(x,y,t)*I(x,y)\\\\\n",
    "\\;\\\\\n",
    "G(x,y,t)&=\\frac{1}{2\\pi t}\\exp{-\\frac{x^2+y^2}{2t}}\\\\\n",
    "\\;\\\\\n",
    "t&=\\sigma^2\n",
    "\\end{align}\n",
    "\n",
    "* **Choose discrete set of low-pass filter**\n",
    "\n",
    ">$$\\sigma_i=2^\\frac{1}{s}\\sigma_{i-1}=2^\\frac{i}{s}\\sigma_0$$\n",
    ">\n",
    ">* $\\sigma$ doubles after $s$ intervals ($=$ an **octave**)\n",
    ">* Avoid blurring with large scales by **subsampling** the image after each octave\n",
    ">* $\\Rightarrow$ **Image Pyramid**\n",
    "\n",
    "* **Within each octave, we convolve repeatedly**\n",
    "\n",
    ">$$G(\\sigma_1)*G(\\sigma_2)=G\\left(\\sqrt{\\sigma^2_1+\\sigma^2_2}\\right)$$\n",
    ">\n",
    ">* The following should be satisfied:\n",
    ">\n",
    ">$$G(\\sigma_{i+1})=G(\\sigma_i)*G(\\sigma_k)$$\n",
    ">\n",
    ">* $\\sigma_k$ can be calculated\n",
    ">\n",
    ">\\begin{align}\n",
    "\\sigma_k&=\\sqrt{\\sigma_{i+1}^2-\\sigma_i^2}\\\\\n",
    "\\sigma_{i+1}&=2^{\\frac{1}{s}}\\sigma_i\\\\\n",
    "\\sigma_k&=\\sigma_i\\sqrt{2^{\\frac{2}{s}}-1}\n",
    "\\end{align}\n",
    "\n",
    "* **Ideal scale for a keypoint is located at the maximum of the scale space**\n",
    "  * $\\Rightarrow$ The largest value of the samples in the pyramid is obtained and interpolated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10. DoG - Difference of Gaussians\n",
    "* **DoG: Blob Detector**\n",
    "  * Calculated as the difference of two Gaussians (small $\\sigma$ - high $\\sigma$)\n",
    "  * This approximates the Laplacian of a Gaussian\n",
    "* **Blobs are Important**\n",
    "  * Blobs are usually found inside of objects (as opposed to edges)\n",
    "  * Thus they are less likely to contain background in queries\n",
    "  * +) stability, repeatability, definite optimal scale, ...\n",
    "* **Scale Space Pyramid**\n",
    "  * Subtract one member of a pyramid level from the one directly above it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11. Zero Normalised Patches\n",
    "* **Matching intensity patches**\n",
    "\n",
    ">$$CC(P_1, P_2)=\\sum_i^N{P_1[i]P_2[i]}$$\n",
    ">\n",
    ">* Not robust to changes (brightness & contrast)\n",
    "\n",
    "* **Zero Normalised Patches**\n",
    "\n",
    ">$$ZN(x,y)=\\frac{Z(x,y)}{\\sigma}=\\frac{I(x,y)-\\mu}{\\sigma}$$\n",
    ">\n",
    ">* Robust to changes\n",
    ">* Patches can be matched using simple cross-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12. Matching Patches using Intensity Edges\n",
    "* **Orientation Histograms**\n",
    "\n",
    ">1. Find every edge in a patch of pixels\n",
    ">2. Weight them by the strength of the edge\n",
    ">3. Bin them together into an orientation histogram\n",
    "\n",
    "* **Advantage**\n",
    "\n",
    ">1. Robust to brightness and contrast changes\n",
    ">2. Incorporates orientation data $\\rightarrow$ robust to orientation\n",
    "\n",
    "* **SIFT**(Scale Invariant Feature Transform)** interest point descriptor**\n",
    "\n",
    ">1. $N\\times N$ patch (typically, $N=16$)\n",
    ">2. Split this patch into $c$ cells (typically $c=16$)\n",
    ">3. In each cell, obtain the orientation histogram\n",
    ">4. Weight them with a Gaussian window ($\\sigma=0.5\\;\\times$ the scale of the feature centered on the patch)\n",
    ">5. The resulting descriptor is $d\\times c$ vector (typically $\\textbf{128}\\text{D}$)\n",
    ">6. Normalise the descriptor vector (invariance to gradient magnitude change)\n",
    ">7. Threshold the elements ($0.2$) $\\rightarrow$ Renormalise (reduce the effect of non-affine lighting changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.13. Texture\n",
    "* **What is Texture?**\n",
    "\n",
    ">* **Image Texture** consists of organised patterns of regular sub-elements called **textons**\n",
    "\n",
    "* **Characterising Texture**\n",
    "\n",
    ">* **Filter bank**:\n",
    ">  * $8$ Lablacian of Gaussian filters\n",
    ">  * $4$ Gaussian filters at different scales\n",
    ">  * $36$ Oriented filters\n",
    ">    * $6$ Different angles\n",
    ">    * $3$ Different scales\n",
    ">    * $2$ Different phases ($1^{st} \\text{&}\\; 2^{nd}$ derivatives of Gaussians on the minor axis)\n",
    ">* **Descriptor**:\n",
    ">  * Simply the concatenated responses of all the filters at a pixel\n",
    ">  * This is innately immune to most changes in an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Orthographic Projection\n",
    "\n",
    "* **Orthographic Projection:** Projection using ***parallel*** rays\n",
    "\n",
    "><img src=\"images/image01.png\" width=250>\n",
    "\n",
    ">$$\\mathbf{x}=\\mathbf{X}-(\\mathbf{X\\cdot k})\\mathbf{k} = (\\mathbf{k \\times X})\\times\\mathbf{k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Perspective Projection\n",
    "\n",
    "* **Perspective Projection:** Inspired by the **pin-hold camera**\n",
    "  * $\\mathbf{X}_c=(X_c, Y_c, Z_c)$: world points\n",
    "  * $\\mathbf{x}=(x,y)$: image plane points\n",
    "\n",
    "><img src=\"images/image02.png\" width=400>\n",
    "\n",
    ">$$\\frac{x}{f}=\\frac{X_c}{Z_c} \\;\\;\\; \\Leftrightarrow \\;\\;\\; x=\\frac{fX_c}{Z_c} \\;\\;\\; \\text{similarly,} \\;\\;\\; y=\\frac{fY_c}{Z_c}$$\n",
    ">\n",
    ">* **Ratio & Symmetry** $\\Rightarrow$ not preserved under perspective projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Vanishing Points\n",
    "* **Vanishing Point:** Point where parallel lines appear to meet\n",
    "  * Each set of parallel lines have a ***different*** vanishing point\n",
    "\n",
    "><img src=\"images/image03.png\" width=300>\n",
    ">\n",
    ">* **Example:** Vanishing point of a line\n",
    ">\n",
    ">$$\\mathbf{X}_c=\\mathbf{a}+\\lambda \\mathbf{b} \\;\\Rightarrow\\; \\mathbf{x} = f \\left( \\frac{a_x+\\lambda b_x}{a_z+\\lambda b_z}, \\frac{a_y+\\lambda b_y}{a_z+\\lambda b_z} \\right) \\;\\Rightarrow\\; \\mathbf{x}_vp=f \\left( \\frac{b_x}{b_z},\\frac{b_y}{b_z} \\right)$$\n",
    "\n",
    "* **Horizon Line:** Line where parallel planes appear to meet\n",
    "  * Any set of parallel lines ***lying on*** these planes will have a vanishing point ***on the horizon line***\n",
    "\n",
    "><img src=\"images/image04.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Full Camera Model\n",
    "\n",
    "* $\\mathbf{X} \\rightarrow \\mathbf{X}_c$ \n",
    "\n",
    "><img src=\"images/image05.png\" width=300>\n",
    "\n",
    ">\\begin{align}\n",
    "\\mathbf{X}_c &= \\mathbf{RX}+\\mathbf{T}\\\\\n",
    "\\begin{bmatrix} X_c \\\\ Y_c \\\\ Z_c \\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ r_{21} & r_{22} & r_{23} \\\\ r_{31} & r_{32} & r_{33} \\end{bmatrix} \n",
    "\\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix}\n",
    "+ \\begin{bmatrix} T_x \\\\ T_y \\\\ T_z \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "* $\\mathbf{x}=(x,y) \\rightarrow \\mathbf{w}=(u,v)$\n",
    "\n",
    "><img src=\"images/image06.png\" width=300>\n",
    "\n",
    ">$$u=u_0+k_ux \\;\\;\\;,\\;\\;\\; v=v_0+k_vy$$\n",
    "\n",
    "* **Combining the Two**\n",
    "\n",
    ">\\begin{align}\n",
    "u&=u_0+\\frac{k_ufX_c}{Z_c}=u_0+\\frac{k_uf(r_{11}X+r_{12}Y+r_{13}Z+T_x)}{r_{31}X+r_{32}Y+r_{33}Z+T_z}\\\\\n",
    "\\;\\\\\n",
    "v&=v_0+\\frac{k_vfY_c}{Z_c}=v_0+\\frac{k_vf(r_{21}X+r_{22}Y+r_{23}Z+T_x)}{r_{31}X+r_{32}Y+r_{33}Z+T_z}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Homogeneous Coordinates\n",
    "* **Homogeneous Coordinates:**\n",
    "  * More natural framework for projective geometry\n",
    "  * Using HC, imagining process $\\rightarrow$ linear matrix operation\n",
    "  * Using HC, series of projections $\\rightarrow$ single matrix operation\n",
    "  * Any equation in $(x,y)$ is equivalent to a ***homogeneous*** equation in $(x_1,x_2,x_3)$\n",
    "\n",
    "* **Homogenous $\\rightarrow$ Cartesian**\n",
    "\n",
    ">$$\\widetilde{\\mathbf{X}}=(x_1,x_2,x_3,x_4) \\rightarrow \\mathbf{X}=\\left( \\frac{x_1}{x_4}, \\frac{x_2}{x_4}, \\frac{x_3}{x_4} \\right)$$\n",
    "\n",
    "* **Cartesian $\\rightarrow$ Homogeneous**\n",
    "\n",
    "  * By convention, $\\lambda$ is set to $1$\n",
    "\n",
    ">$$\\mathbf{X}=(X,Y,Z) \\rightarrow \\widetilde{\\mathbf{X}}=(\\lambda X,\\lambda Y,\\lambda Z,\\lambda) $$\n",
    "\n",
    "* **Perspective Projection**\n",
    "\n",
    ">\\begin{align}\n",
    "\\widetilde{\\mathbf{x}}&=\\mathbf{P}_p \\widetilde{\\mathbf{X}}_c\\\\\n",
    "\\begin{bmatrix} sx \\\\ sy \\\\ s \\end{bmatrix} &=\n",
    "\\begin{bmatrix} f&0&0&0 \\\\ 0&f&0&0 \\\\ 0&0&1&0 \\end{bmatrix}\n",
    "\\begin{bmatrix} \\lambda X_c \\\\ \\lambda Y_c \\\\ \\lambda Z_c \\\\ \\lambda \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    ">* $s \\;\\text{&}\\; \\lambda$ has no effect on the projection\n",
    ">* Same projection is achieved with $\\mu\\mathbf{P}_p \\;\\;\\; (\\mu\\neq0)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Full Camera Model in Homogeneous Coordinates\n",
    "\n",
    "* **Step 1. Rigid Body Transformation** $(\\widetilde{\\mathbf{X}}\\rightarrow\\widetilde{\\mathbf{X}}_c)$\n",
    "\n",
    ">\\begin{align}\n",
    "\\widetilde{\\mathbf{X}}_c &= \\mathbf{P}_r\\widetilde{\\mathbf{X}}\\\\\n",
    "\\begin{bmatrix} X_c \\\\ Y_c \\\\ Z_c \\\\ 1 \\end{bmatrix} &=\n",
    "\\begin{bmatrix} r_{11} & r_{12} & r_{13} & T_x \\\\ r_{21} & r_{22} & r_{23} & T_y \\\\ r_{31} & r_{32} & r_{33} & T_z \\\\ 0&0&0&1 \\end{bmatrix}\n",
    "\\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ 1 \\end{bmatrix}\n",
    "\\end{align}\n",
    ">* $\\mathbf{P}_r$: **rigid body transformation matrix**\n",
    "\n",
    "* **Step 2. Perspective Projection** $(\\widetilde{\\mathbf{X}}_c\\rightarrow\\widetilde{\\mathbf{x}}_c)$\n",
    "\n",
    ">\\begin{align}\n",
    "\\widetilde{\\mathbf{x}}_c &= \\mathbf{P}_p\\widetilde{\\mathbf{X}}_c\\\\\n",
    "\\begin{bmatrix} sx \\\\ sy \\\\ s \\end{bmatrix} &=\n",
    "\\begin{bmatrix} f&0&0&0 \\\\ 0&f&0&0 \\\\ 0&0&1&0 \\end{bmatrix}\n",
    "\\begin{bmatrix} X_c \\\\ Y_c \\\\ Z_c \\\\ 1 \\end{bmatrix}\n",
    "\\end{align}\n",
    ">* $\\mathbf{P}_p$: **perspective projection matrix**\n",
    "\n",
    "* **Step 3. CCD imaging**\n",
    "\n",
    ">\\begin{align}\n",
    "\\widetilde{\\mathbf{w}} &= \\mathbf{P}_c\\widetilde{\\mathbf{x}}\\\\\n",
    "\\begin{bmatrix} su \\\\ sv \\\\ s \\end{bmatrix} &=\n",
    "\\begin{bmatrix} k_u&0&u_0 \\\\ 0&k_v&v_0 \\\\ 0&0&1 \\end{bmatrix}\n",
    "\\begin{bmatrix} sx \\\\ sy \\\\ s \\end{bmatrix}\n",
    "\\end{align}\n",
    ">* $\\mathbf{P}_c$: **CCD calibration matrix**\n",
    "\n",
    "* **Perspective Camera**\n",
    "\n",
    ">\\begin{align}\n",
    "\\widetilde{\\mathbf{w}} &= \\mathbf{P}_{ps}\\widetilde{\\mathbf{X}}\\\\\n",
    "\\mathbf{P}_{ps} &= \\mathbf{P}_{c}\\mathbf{P}_{p}\\mathbf{P}_{r}\\\\\n",
    "&=\n",
    "\\begin{bmatrix} k_u&0&u_0 \\\\ 0&k_v&v_0 \\\\ 0&0&1 \\end{bmatrix}\n",
    "\\begin{bmatrix} f&0&0&0 \\\\ 0&f&0&0 \\\\ 0&0&1&0 \\end{bmatrix}\n",
    "\\begin{bmatrix} r_{11} & r_{12} & r_{13} & T_x \\\\ r_{21} & r_{22} & r_{23} & T_y \\\\ r_{31} & r_{32} & r_{33} & T_z \\\\ 0&0&0&1 \\end{bmatrix}\n",
    "\\end{align}\n",
    ">* $\\mathbf{P}_{ps}$: **Camera Projection Matrix** for a **Perspective Camera**\n",
    ">  * $10$ d.o.f. $=3$ for $\\mathbf{R} + 3$ for $\\mathbf{T} + 2$ for $(f,k_u,k_v) + u_0 + v_0$\n",
    ">  * $\\mathbf{P}_c\\mathbf{P}_p$: accounts for ***intrinsic** parameters\n",
    ">  * $\\mathbf{P}_r$: accounts for **extrinsic** parameters\n",
    "\n",
    "* **Alternative form of $\\mathbf{P}_{ps}$**\n",
    "\n",
    ">\\begin{align}\n",
    "\\mathbf{P}_{ps} &= \\mathbf{K}\\left[\\;\\mathbf{R\\;|\\;T}\\;\\right]\\\\\n",
    "&=\n",
    "\\begin{bmatrix} \\alpha_u&0&u_0 \\\\ 0&\\alpha_v&v_0 \\\\ 0&0&1 \\end{bmatrix}\n",
    "\\begin{bmatrix} r_{11} & r_{12} & r_{13} & T_x \\\\ r_{21} & r_{22} & r_{23} & T_y \\\\ r_{31} & r_{32} & r_{33} & T_z \\\\ 0&0&0&1 \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    ">* $\\alpha_u=fk_u$ and $\\alpha_v=fk_v$: **scaling factors**\n",
    ">* $\\alpha_v/\\alpha_u$: **aspect ratio**\n",
    "\n",
    "* **Projective Camera**\n",
    "\n",
    ">\\begin{align}\n",
    "\\widetilde{\\mathbf{w}} &= \\mathbf{P}\\widetilde{\\mathbf{X}}\\\\\n",
    "\\mathbf{P} &=\n",
    "\\begin{bmatrix} p_{11}&p_{12}&p_{13}&p_{14} \\\\ p_{21}&p_{22}&p_{23}&p_{24} \\\\ p_{31}&p_{32}&p_{33}&p_{34} \\end{bmatrix}\n",
    "\\end{align}\n",
    ">\n",
    ">* $11$ d.o.f. (since overall scale of $\\mathbf{P}$ does not matter\n",
    ">* Projective camera is more convenient since we do not have to worry about any non-linear constraints on the elements of $\\mathbf{P}$\n",
    ">* Perspective camera is a special case of projective camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7. Viewing a Plane\n",
    "\n",
    "><img src=\"images/image07.png\" width=500>\n",
    "\n",
    "* **Perspective Camera**\n",
    "\n",
    ">\\begin{align}\n",
    "\\widetilde{\\mathbf{w}} &= \\mathbf{P}^p_{ps}\\widetilde{\\mathbf{X}}^p\\\\\n",
    "&= \\mathbf{P}_{c}\\mathbf{P}_{p}\\mathbf{P}^p_{r}\\widetilde{\\mathbf{X}}^p\\\\\n",
    "&=\n",
    "\\begin{bmatrix} k_u&0&u_0 \\\\ 0&k_v&v_0 \\\\ 0&0&1 \\end{bmatrix}\n",
    "\\begin{bmatrix} f&0&0&0 \\\\ 0&f&0&0 \\\\ 0&0&1&0 \\end{bmatrix}\n",
    "\\begin{bmatrix} r_{11} & r_{12} & T_x \\\\ r_{21} & r_{22} & T_y \\\\ r_{31} & r_{32} & T_z \\\\ 0&0&1 \\end{bmatrix}\n",
    "\\begin{bmatrix} X \\\\ Y \\\\ 1 \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "* **Projective Camera**\n",
    "  * Relax the contraints to obtain more tractable model\n",
    "  \n",
    ">\\begin{align}\n",
    "\\widetilde{\\mathbf{w}} &= \\mathbf{P}^p\\widetilde{\\mathbf{X}}^p\\\\\n",
    "&=\n",
    "\\begin{bmatrix} p_{11} & p_{12} & p_{13} \\\\ p_{21} & p_{22} & p_{23} \\\\ p_{31} & p_{32} & p_{33} \\end{bmatrix}\n",
    "\\begin{bmatrix} X \\\\ Y \\\\ 1 \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    ">* Transformation between $\\widetilde{\\mathbf{w}}$ and $\\widetilde{\\mathbf{X}}^p$ is called:\n",
    "\n",
    ">  * **Planar projective transformation** or **homography** or **collineation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8. Viewing a Line\n",
    "\n",
    "* **Perspective Camera**\n",
    "\n",
    ">\\begin{align}\n",
    "\\widetilde{\\mathbf{w}} &= \\mathbf{P}^l_{ps}\\widetilde{\\mathbf{X}}^l\\\\\n",
    "\\mathbf{P}^l_{ps} &=\n",
    "\\begin{bmatrix} k_u&0&u_0 \\\\ 0&k_v&v_0 \\\\ 0&0&1 \\end{bmatrix}\n",
    "\\begin{bmatrix} f&0&0&0 \\\\ 0&f&0&0 \\\\ 0&0&1&0 \\end{bmatrix}\n",
    "\\begin{bmatrix} r_{11} & T_x \\\\ r_{21} & T_y \\\\ r_{31} & T_z \\\\ 0&1 \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "* **Projective Camera**\n",
    "  \n",
    ">\\begin{align}\n",
    "\\widetilde{\\mathbf{w}} &= \\mathbf{P}^l\\widetilde{\\mathbf{X}}^l\\\\\n",
    "\\mathbf{P}^l &=\n",
    "\\begin{bmatrix} p_{11} & p_{12} \\\\ p_{21} & p_{22} \\\\ p_{31} & p_{32} \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9. Camera Calibration\n",
    "* **Camera Calibration**: process of discovering the ***projection matrix (and its decomposition*** into camera matrix and the position and orientation of the camera) from an image of a controlled scene\n",
    "\n",
    "* **Camera Calibration: 3D**\n",
    "\n",
    ">$$\\begin{bmatrix} su \\\\ sv \\\\ s \\end{bmatrix} =\n",
    "\\begin{bmatrix} p_{11}&p_{12}&p_{13}&p_{14} \\\\ p_{21}&p_{22}&p_{23}&p_{24} \\\\ p_{31}&p_{32}&p_{33}&p_{34} \\end{bmatrix}\n",
    "\\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ 1 \\end{bmatrix}$$\n",
    ">\n",
    ">* **Step 1. 11 Parameters** to estimate $\\rightarrow$ **6 Observations** required\n",
    ">* **Step 2.** Solve the equations using linear least squares\n",
    ">* **Step 3.** Decompose the projection matrix, $\\mathbf{P}_{ps} = \\mathbf{K}\\left[\\;\\mathbf{R\\;|\\;T}\\;\\right]$\n",
    "\n",
    ">  * $\\mathbf{K}$ and $\\mathbf{R}$ using **QR decomposition**,\n",
    ">  * and $\\mathbf{T}$ using $\\mathbf{T}=\\mathbf{K}^{-1}(p_{14},p_{24},p_{34})^T$\n",
    "\n",
    "* **Camera Calibration: 2D**\n",
    "\n",
    ">$$\\begin{bmatrix} su \\\\ sv \\\\ s \\end{bmatrix} =\n",
    "\\begin{bmatrix} p_{11}&p_{12}&p_{13} \\\\ p_{21}&p_{22}&p_{23} \\\\ p_{31}&p_{32}&p_{33} \\end{bmatrix}\n",
    "\\begin{bmatrix} X \\\\ Y \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    ">* **8 Parameters** to estimate $\\rightarrow$ **4 Observations**\n",
    "\n",
    "* **Camera Calibration: 1D**\n",
    "\n",
    ">$$\\begin{bmatrix} su \\\\ sv \\\\ s \\end{bmatrix} =\n",
    "\\begin{bmatrix} p_{11}&p_{12} \\\\ p_{21}&p_{22} \\\\ p_{31}&p_{32} \\end{bmatrix}\n",
    "\\begin{bmatrix} X \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    ">* **5 Parameters** to estimate $\\rightarrow$ **3 Observations**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10. Recovery of World Position\n",
    "\n",
    "* **1D case**\n",
    "\n",
    ">$$\\begin{bmatrix} u \\\\ 1 \\end{bmatrix} =\n",
    "\\begin{bmatrix} p_{11}&p_{12} \\\\ p_{31}&p_{32} \\end{bmatrix}\n",
    "\\begin{bmatrix} \\lambda X \\\\ \\lambda \\end{bmatrix}$$\n",
    ">$$\\;$$\n",
    ">$$X=\\frac{p_{32}u-p_{12}}{-p_{31}u+p_{11}}$$\n",
    "\n",
    "* **2D case**\n",
    "\n",
    ">$$\\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} =\n",
    "\\begin{bmatrix} p_{11}&p_{12}&p_{13} \\\\ p_{21}&p_{22}&p_{23} \\\\ p_{31}&p_{32}&p_{33} \\end{bmatrix}\n",
    "\\begin{bmatrix} \\lambda X \\\\ \\lambda Y \\\\ \\lambda \\end{bmatrix}$$\n",
    ">$$\\;$$\n",
    ">$$X=\\frac{p^i_{11}u+p^i_{12}v+p^i_{13}}{p^i_{31}u+p^i_{32}v+p^i_{33}}\\;\\;\\;,\\;\\;\\;Y=\\frac{p^i_{21}u+p^i_{22}v+p^i_{23}}{p^i_{31}u+p^i_{32}v+p^i_{33}}$$\n",
    "\n",
    "* **3D case**\n",
    "\n",
    ">$$\\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} =\n",
    "\\begin{bmatrix} p_{11}&p_{12}&p_{13}&p_{14} \\\\ p_{21}&p_{22}&p_{23}&p_{24} \\\\ p_{31}&p_{32}&p_{33}&p_{34} \\end{bmatrix}\n",
    "\\begin{bmatrix} \\lambda X \\\\ \\lambda Y \\\\ \\lambda Z \\\\ \\lambda \\end{bmatrix}$$\n",
    ">$$\\;$$\n",
    ">$$\\text{defines the light ray}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.11. Affine Camera\n",
    "* **Weak Perspective**\n",
    "  * When $\\Delta Z_c$ is small compared to $Z_c$ $\\rightarrow$ Assume $Z_c=Z^{av}_c$\n",
    "  * Then, the projection becomes,\n",
    "  \n",
    ">\\begin{align}\n",
    "\\widetilde{\\mathbf{x}}_c &= \\mathbf{P}_{pll}\\widetilde{\\mathbf{X}}_c\\\\\n",
    "\\begin{bmatrix} sx \\\\ sy \\\\ s \\end{bmatrix} &=\n",
    "\\begin{bmatrix} f&0&0&0 \\\\ 0&f&0&0 \\\\ 0&0&0&Z^{av}_c \\end{bmatrix}\n",
    "\\begin{bmatrix} X_c \\\\ Y_c \\\\ Z_c \\\\ 1 \\end{bmatrix}\n",
    "\\end{align}  \n",
    "\n",
    "* **Overal Projection Matrix**\n",
    "  \n",
    ">$$\\mathbf{P}_{wp}=\\mathbf{P}_{c}\\mathbf{P}_{pll}\\mathbf{P}_{r}\n",
    "=\\begin{bmatrix} fk_ur_{11}&fk_ur_{12}&fk_ur_{13}&fk_uT_x+u_0Z^{av}_c \\\\ fk_ur_{21}&fk_ur_{22}&fk_ur_{23}&fk_uT_y+v_0Z^{av}_c \\\\ \n",
    "0&0&0&Z^{ac}_c \\end{bmatrix}$$\n",
    "\n",
    "* **Projection Matrix for Affine Camera**\n",
    "  * Discard nonlinear constraints\n",
    "  \n",
    ">$$\\mathbf{P}_{aff}=\n",
    "\\begin{bmatrix} p_{11}&p_{12}&p_{13}&p_{14} \\\\ p_{21}&p_{22}&p_{23}&p_{24} \\\\ 0&0&0&p_{34} \\end{bmatrix}$$\n",
    ">* **8 Parameters** to estimate $\\rightarrow$ **4 Observations** required\n",
    ">* **2D case:** 6 Parameters $\\rightarrow$ 3 Observations\n",
    ">* **1D case:** 4 Parameters $\\rightarrow$ 2 Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.12. Invariants\n",
    "* **Definition**\n",
    "\n",
    ">$$f(\\text{image}): \\text{robust across viewpoints, change across objects}$$\n",
    "\n",
    "* **Invariants of Different Cameras viewing planar scenes**\n",
    "\n",
    ">* **1. Euclidean Invariants**\n",
    ">  * **Condition:** Image is parallel & a fixed distance from the world plane\n",
    ">  * **Invariants: Lengths / Areas**\n",
    "\n",
    "><img src=\"images/image08.png\" width=300>\n",
    "\n",
    ">* **2. Similarity Invariants**\n",
    ">  * **Condition:** Euclidean + varying distance\n",
    ">  * **Invariants: Ratio of Lengths / Ratio of Angles**\n",
    "\n",
    "><img src=\"images/image09.png\" width=300>\n",
    "\n",
    ">* **3. Affine Invariants**\n",
    ">  * **Invariants: Parallelism / Ratio of Areas**\n",
    ">    * Parallelism: ratios of lengths along collinear or parallel lines\n",
    "\n",
    "><img src=\"images/image10.png\" width=300>\n",
    "\n",
    ">* **4. Projective Invariants**\n",
    ">  * **Invariants: concurrency / collinearity / tangent discontinuities / cusps / order of contact**\n",
    ">    * Order of Contact: **intersection** (1 point of contact), **tangency** (2 poc), **inflection** (3 poc)\n",
    "\n",
    "><img src=\"images/image11.png\" width=300>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.13. Cross-Ratio (Perspective Camera)\n",
    "* **4 Collinear Points**\n",
    "\n",
    "><img src=\"images/image12.png\" width=300>\n",
    "\n",
    ">$$\\begin{bmatrix} sl \\\\ s \\end{bmatrix} = \\begin{bmatrix} p & q \\\\ r & 1 \\end{bmatrix} \\begin{bmatrix} X \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    ">$$l_i=\\frac{pX_i+q}{rX_i+1}$$\n",
    "\n",
    ">$$\\textbf{cross-ratio:} \\;\\;\\; \\frac{(l_d-l_a)(l_c-l_b)}{(l_d-l_b)(l_c-l_a)}=\\frac{(X_d-X_a)(X_c-X_b)}{(X_d-X_b)(X_c-X_a)}$$\n",
    "\n",
    "* **5 Coplanar Points**\n",
    "  * ***Find 4 more distinguished points $\\rightarrow$ 2 sets of 4 collinear points***\n",
    "\n",
    "><img src=\"images/image13.png\" width=500>\n",
    "\n",
    ">$$\\delta_1=\\text{cross-ratio of}\\; \\left\\{a,e_2,b,f\\right\\}$$\n",
    ">$$\\delta_2=\\text{cross-ratio of}\\; \\left\\{a,e_1,d,g\\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.14. Canonical Views\n",
    "* **5 Coplanar Points**\n",
    "  * **Idea:** Any two views $\\widetilde{\\mathbf{w}}$ and $\\widetilde{\\mathbf{w}}'$ are related by a projectivity $\\mathbf{P}^{p''}$\n",
    "  \n",
    "><img src=\"images/image14.png\" width=500>\n",
    ">$$\\;$$\n",
    ">\\begin{align}\n",
    "\\widetilde{\\mathbf{w}}&=\\mathbf{P}^p\\widetilde{\\mathbf{X}}^p \\\\\n",
    "\\widetilde{\\mathbf{w}}'&=\\mathbf{P}^{p'}\\widetilde{\\mathbf{X}}^p=\\mathbf{P}^{p'}[\\mathbf{P}^p]^{-1}\\widetilde{\\mathbf{w}}=\\mathbf{P}^{p''}\\widetilde{\\mathbf{w}}\n",
    "\\end{align}\n",
    "\n",
    ">* **Step 1:** Distinguish 4 points\n",
    ">* **Step 2:** Find $\\mathbf{P}^p$ which maps them onto the corners of the unit square in the canonical view\n",
    ">* **Step 3:** Apply $\\mathbf{P}^p$ to other points $\\rightarrow$ **invariant signature**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
