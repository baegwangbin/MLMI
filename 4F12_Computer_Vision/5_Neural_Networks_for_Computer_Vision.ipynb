{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Neural Networks for Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Single Neuron\n",
    "\n",
    "* **Formulation**\n",
    "\n",
    ">$$a = \\sum^D_{d=0} w_d z_d \\;\\;\\;\\rightarrow\\;\\;\\; x(a) = \\frac{1}{1 + \\exp (-a)}$$\n",
    "\n",
    ">* $\\hat{\\mathbf{w}}$: direction of boundary\n",
    ">* $|\\mathbf{w}|$: steepness of boundary\n",
    "\n",
    "* **Training**\n",
    "\n",
    ">$$G (\\mathbf{w}) = - \\sum_n \\big[ t^{(n)} \\log x (\\mathbf{z}^{(n)};\\mathbf{w}) + (1-t^{(n)}) \\log ( 1 - x (\\mathbf{z}^{(n)};\\mathbf{w})) \\big]$$\n",
    "\n",
    ">$$\\frac{d}{d\\mathbf{w}} G(\\mathbf{w}) = \\sum_n \\frac{dG_n(\\mathbf{w})}{dx^{(n)}} \\frac{dx^{(n)}}{d\\mathbf{w}} = - \\sum_n (t^{(n)} - x^{(n)}) \\mathbf{z}^{(n)} \\;\\;\\;\\rightarrow\\;\\;\\; \\mathbf{w}=\\mathbf{w}-\\eta \\frac{d}{d\\mathbf{w}} G(\\mathbf{w})$$\n",
    "\n",
    "* **Regularised Training**\n",
    "\n",
    ">$$E(\\mathbf{w}) = \\frac{1}{2} \\sum_i w_i^2 \\;\\;\\;\\rightarrow\\;\\;\\; M (\\mathbf{w}) = [G (\\mathbf{w}) + \\alpha E(\\mathbf{w})]$$\n",
    "\n",
    ">$$\\frac{d}{d\\mathbf{w}} M(\\mathbf{w}) = - \\sum_n (t^{(n)} - x^{(n)}) \\mathbf{z}^{(n)} + \\alpha \\mathbf{w}$$\n",
    "\n",
    "* **Probabilistic Interpretation**\n",
    "\n",
    ">\\begin{align}\n",
    "p(t|\\mathbf{w},\\mathbf{z}) &= x^t (1-x)^{(1-t)} \\\\\n",
    "p(D|\\mathbf{w},Z) &= \\exp (-G(\\mathbf{w})) \\\\\n",
    "p(\\mathbf{w}|\\alpha) &= \\frac{1}{Z_W(\\alpha)} \\exp (-\\alpha E(\\mathbf{w})) \\\\\n",
    "p(\\mathbf{w}|D,\\alpha) &= \\frac{1}{Z_M} \\exp (-G(\\mathbf{w})-\\alpha E(\\mathbf{w}))\n",
    "\\end{align}\n",
    "\n",
    ">* The result of training: **(locally) most probable weight vector given data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Single Hidden Layer Neural Networks\n",
    "\n",
    "* **Framework**\n",
    "\n",
    "><img src='images/image17.png' width=400>\n",
    "\n",
    "* **Training**\n",
    "\n",
    ">\\begin{align}\n",
    "G(W,\\mathbf{w}) &= - \\sum_n \\big[ t^{(n)} \\log x^{(n)} + (1-t^{(n)}) \\log ( 1 - x^{(n)}) \\big] \\\\\n",
    "E(W,\\mathbf{w}) &= \\frac{1}{2} \\sum_i w_i^2 + \\frac{1}{2} \\sum_{i,j} W_{ij}^2\n",
    "\\end{align}\n",
    "\n",
    "* **Back-Propagation**\n",
    "\n",
    ">$$\\frac{dG(W,\\mathbf{w})}{dW_{ij}} = \\sum_{n,i} \\frac{dG(W,\\mathbf{w})}{dx^{(n)}} \\frac{dx^{(n)}}{da^{(n)}} \\frac{da^{(n)}}{dx_i^{(n)}} \\frac{dx_i^{(n)}}{da_i^{(n)}} \\frac{da_i^{(n)}}{dW_{ij}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Hierarchical Models with Many Hidden Layers\n",
    "\n",
    "* **Justifications**\n",
    "\n",
    ">1. Visual scenes are hierarchically organised\n",
    ">2. Biological vision is hierarchically organised\n",
    ">3. Shallow architectures are inefficient at representing deep functions\n",
    "\n",
    "* **Initialisation Methods**\n",
    "\n",
    ">1. Unsupervised pre-training (e.g. using a restricted Boltzmann machine)\n",
    ">2. Recursively apply back propagation\n",
    ">3. Initialise randomly, but ensure activations have $\\mu=0$ and $\\sigma^2=1$ across the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Convolutional Neural Networks\n",
    "\n",
    "* **Key Ideas** (all three ideas reduce the no. of parameters)\n",
    "\n",
    ">* Image statistics are **translation invariant** \n",
    ">  * $\\rightarrow$ tie weights together\n",
    ">* **Low-level features:** local\n",
    ">  * $\\rightarrow$ allow only **local connectivity**\n",
    ">* **High-level features:** coarse, abstract, invariance to translation, rotation, lighting, ...\n",
    ">  * $\\rightarrow$ **subsample** up the hierarchy\n",
    "\n",
    "* **Framework**\n",
    "\n",
    "><img src = 'images/image19.png' width=500>\n",
    "\n",
    "* **Building Blocks**\n",
    "\n",
    ">1. Convolutional stage: $a_{i,j} = \\sum_{k,l} w_{k,l} z_{i-k,j-l}$\n",
    ">2. Non-linear stage: $y_{i,j}=f(a_{i,j})$ (sigmoid, ReLU, tanh, ...)\n",
    ">3. Pooling stage: $x_{i,j} = \\underset{|k|<\\tau, |l|<\\tau}{\\max} y_{i-k,j-l}$\n",
    "\n",
    "* **Training**\n",
    "\n",
    ">* **Back-propgation:** optimisation over a mini-batch of data\n",
    ">* **Data Augmentation:** shift, rotation, mirroring, local distortion, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
