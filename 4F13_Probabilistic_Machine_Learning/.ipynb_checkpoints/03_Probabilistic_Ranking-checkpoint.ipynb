{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "* **Goal:**\n",
    "\n",
    ">*What is the probability that player 1 defeats player 2?*\n",
    "\n",
    "* **Generative Model for Game Outcomes:**\n",
    "\n",
    ">* Player has **skills** $\\rightarrow$ computer skill difference\n",
    "\n",
    ">$$s = w_1 - w_2$$\n",
    "\n",
    ">* **Add noise**\n",
    "\n",
    ">$$t = s + n \\;\\;\\;\\text{where}\\;\\;\\; n \\text{~} \\mathcal{N}(0,1)$$\n",
    "\n",
    ">* **Computer game outcome**\n",
    "\n",
    ">$$y=\\text{sign}(t)= \\Bigg\\{ \\begin{matrix} +1 \\rightarrow \\text{Player 1 wins}\\\\ -1 \\rightarrow \\text{Player 1 wins}\\end{matrix}$$\n",
    "\n",
    ">* **Probability that player 1 wins**\n",
    "\n",
    ">$$p(t|w_1,w_2) = \\mathcal{N}(t;w_1-w_2,1)$$\n",
    "\n",
    ">$$p(y=1|w_1,w_2)=p(t>0|w_1,w_2)=\\Phi(w_1-w_2)$$\n",
    "\n",
    ">$$\\Phi(x)=\\int^x_{-\\infty} \\mathcal{N}(z;0,1)dz = \\int^\\infty_0 \\mathcal{N}(z;x,1)dz$$\n",
    "\n",
    ">* **Likelihood**\n",
    "\n",
    ">$$p(y|w_1,w_2) = \\Phi(y(w_1-w_2))$$\n",
    "\n",
    "* **TrueSkill: a probabilistic skill rating system:**\n",
    "\n",
    ">* **Prior**\n",
    "\n",
    ">$$p(w_i) = \\mathcal{N}(w_i|\\mu_i,\\sigma^2_i)$$\n",
    "\n",
    ">* **Likelihood**\n",
    ">  * $p(s|w_1,w_2)$: delta fn.\n",
    ">  * $p(y|t)$: step fn.\n",
    "\n",
    ">$$p(y|w_1,w_2)=\\iint p(y|t)p(t|s)p(s|w_1,w_2)dsdt$$\n",
    "\n",
    "\n",
    "\n",
    ">* **Posterior:** no longer Gaussian / does not factorise / looks like a high-dim ball\n",
    "\n",
    ">\\begin{align}\n",
    "p(w_1,w_2|y) &= \\frac{p(w_1)p(w_2)p(y|w_1,w_2)}{\\iint p(w_1)p(w_2)p(y|w_1,w_2)dw_1 dw_2} \\\\\n",
    "&= \\frac{\\mathcal{N}(w_1;\\mu_1,\\sigma_1^2)\\mathcal{N}(w_2;\\mu_2,\\sigma_2^2) \\Phi(y(w_1-w_2))}{\\iint \\mathcal{N}(w_1;\\mu_1,\\sigma_1^2)\\mathcal{N}(w_2;\\mu_2,\\sigma_2^2)\\Phi(y(w_1-w_2)) dw_1dw_2}\n",
    "\\end{align}\n",
    "\n",
    ">* **Normalising constant:** have closed form\n",
    "\n",
    ">$$p(y) = \\Phi \\left( \\frac{y(\\mu_1-\\mu_2)}{\\sqrt{1+\\sigma^2_1+\\sigma^2_1}} \\right) \\;\\;\\;\\rightarrow\\;\\;\\; \\text{smoother version of the likelihood}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gibbs Sampling\n",
    "\n",
    "* **Q. How do we integrate wrt an intractable posterior?**\n",
    "* **The original integral:**\n",
    "\n",
    ">$$\\mathbb{E}_{p(\\mathbf{x})}[\\phi(\\mathbf{x})] = \\bar{\\phi} = \\int \\phi(\\mathbf{x}) p(\\mathbf{x}) \\text{d}\\mathbf{x} \\;\\;\\; , \\;\\;\\; \\mathbf{x} \\in \\mathbb{R}^D$$\n",
    "\n",
    "* **Numerical integram on a grid:** (practical only to $D \\leq 4$)\n",
    "\n",
    ">$$\\int{\\phi(\\mathbf{x})p(\\mathbf{x})\\text{d}\\mathbf{x}} \\approx \\sum^T_{\\tau=1} \\phi(\\mathbf{x}^{(\\tau)}) p(\\mathbf{x}^{(\\tau)}) \\Delta \\mathbf{x}$$\n",
    "\n",
    "* **Monte Carlo:**\n",
    "\n",
    ">$$\\mathbb{E}_{p(\\mathbf{x})} [\\phi(\\mathbf{x})] \\approx \\hat{\\phi} = \\frac{1}{T} \\sum^T_{\\tau=1} \\phi(\\mathbf{x}^{(\\tau)}) \\;\\;\\;,\\;\\;\\; \\mathbf{x}^{(\\tau)} \\text{ ~ } p(\\mathbf{x})$$\n",
    "\n",
    ">* $\\hat{\\phi}$: unbiased estimate with\n",
    "\n",
    ">$$\\mathbb{V}[\\hat{\\phi}] = \\frac{\\mathbb{V}[\\phi]}{T} \\;\\;\\;,\\;\\;\\; \\mathbb{V}[\\phi] = \\int \\left( \\phi(\\mathbf{x})-\\bar{\\phi} \\right)^2 p(\\mathbf{x}) \\text{d} \\mathbf{x}$$\n",
    "\n",
    ">* **NOTE:** the variance in independent of the dimension of $\\mathbf{x}$\n",
    "\n",
    "* **Markov Chain Monte Carlo:**\n",
    "\n",
    ">$$\\mathbf{x} \\rightarrow \\mathbf{x}' \\rightarrow \\mathbf{x}'' \\rightarrow \\mathbf{x}''' \\rightarrow \\cdots$$\n",
    "\n",
    ">$$x'_i \\sim p(x_i|x_1,...,x_{i-1},x_{i+1},...,x_D)$$\n",
    "\n",
    ">* This will eventually generate dependent samples from $p(\\mathbf{x})$\n",
    "\n",
    "* **Gibbs Sampling**\n",
    "\n",
    ">* **Advantage:** Sampling from a joint distribution $\\rightarrow$ sampling from a sequence of univariate conditional distributions\n",
    "\n",
    ">* **Disadvantage:** Initial convergence / Dependence between samples\n",
    ">* **Improvements:** \n",
    ">  * Initial samples are discarded\n",
    ">  * Samples are thinned (e.g.  every 10th or 100th)\n",
    ">  * Change the *effective correlation length*\n",
    ">  * Run several Gibbs samplers (different starting points) to compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
