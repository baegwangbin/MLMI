{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modelling Document Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Introduction\n",
    "\n",
    "* **Bag of Words:** frequency of occurrence of every distinct word\n",
    "* **Zipf's Law:** the frequency of any word is inversely proportional to its rank in the frequency table\n",
    "\n",
    "><img src = 'images/image24.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Discrete Binary Distributions\n",
    "\n",
    "* **Discrete** Distributions\n",
    "\n",
    ">||binary|multi-valued|\n",
    "|-|-|-|\n",
    "|**sequence**|binary categorical, $\\pi^k(1-\\pi)^{n-k}$|categorical, $\\prod^k_{i=1} \\pi_i^{k_i}$|\n",
    "|**counts**|binomial|multinomial|\n",
    "\n",
    "* **Bernoulli** Distribution\n",
    "\n",
    ">$$p(X=1)=\\pi \\;\\;\\;,\\;\\;\\; p(X=0)=1-\\pi$$\n",
    ">$$p(X=x|\\pi) = \\pi^x (1-\\pi)^{1-x}$$\n",
    "\n",
    "* **Binomial** Distribution ($k$ heads out of $n$ tosses)\n",
    "\n",
    ">$$p(k|\\pi, n) = \\left( \\begin{matrix} n \\\\ k \\end{matrix} \\right) \\pi^k (1-\\pi)^{n-k} \\;\\;\\;\\rightarrow\\;\\;\\; \\textbf{MLE: } \\pi = \\frac{k}{n}$$\n",
    "\n",
    ">$$\\left( \\begin{matrix} n \\\\ k \\end{matrix} \\right) = \\frac{n!}{k!(n-k)!}$$\n",
    "\n",
    "* **Beta** Distribution\n",
    "\n",
    ">$$\\text{Beta}(\\pi|\\alpha,\\beta) = \\frac{1}{B(\\alpha,\\beta)} \\pi^{\\alpha-1} (1-\\pi)^{\\beta-1} \\;\\;\\;\\rightarrow\\;\\;\\; E(\\pi) = \\frac{\\alpha}{\\alpha+\\beta}$$\n",
    "\n",
    ">$$\\frac{1}{B(\\alpha,\\beta)} = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\;\\;\\;,\\;\\;\\; \\Gamma(\\alpha)=\\int^\\infty_0 x^{\\alpha-1} e^{-x} dx$$\n",
    "\n",
    "* **Posterior** (Prior: Beta, Likelihood: Binomial)\n",
    "\n",
    ">\\begin{align}\n",
    "p(\\pi|\\mathcal{D}) &= \\frac{p(\\pi|\\alpha,\\beta)p(\\mathcal{D}|\\pi)}{p(\\mathcal{D})} \\\\\n",
    "&\\propto \\text{Beta}(\\pi|\\alpha,\\beta) \\pi^k (1-\\pi)^{n-k} \\\\\n",
    "&\\propto \\text{Beta}(\\pi|\\alpha+k,\\beta+(n-k))\n",
    "\\end{align}\n",
    "\n",
    "* **Making Predictions**\n",
    "\n",
    ">$$p(x_{next}=1 | \\mathcal{D}) = \\int p(x=1|\\pi) p(\\pi|\\mathcal{D}) d\\pi$$\n",
    "\n",
    ">$$p(\\pi > 0.5 | \\mathcal{D}) = \\int^1_{0.5} p(\\pi'|\\mathcal{D}) d\\pi'$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Discrete Categorical Distribution\n",
    "\n",
    "* **Categorical** Distribution: **Multinomial** with one trial\n",
    "* **Multinomial** Distribution\n",
    "\n",
    ">$$p(\\mathbf{k}|\\boldsymbol{\\pi},n) = \\frac{n!}{k_1!...k_m!} \\prod^m_{i=1} \\pi_i^{k_i}$$\n",
    "\n",
    ">$$\\sum^m_{i=1} k_i = n \\;\\;\\;,\\;\\;\\; \\sum^m_{i=1} \\pi_i = 1$$\n",
    "\n",
    "* **Dirichlet** Distribution\n",
    "\n",
    ">$$\\text{Dir}(\\boldsymbol{\\pi}|\\alpha_1,...\\alpha_m) = \\frac{\\Gamma (\\sum^m_{i=1} \\alpha_i)}{\\prod^m_{i=1} \\Gamma(\\alpha_i)} \\prod^m_{i=1} \\pi_i^{\\alpha_i - 1} = \\frac{1}{B(\\boldsymbol{\\alpha})} \\prod^m_{i=1} \\pi_i^{\\alpha_i - 1}$$\n",
    "\n",
    ">$$E(\\pi_j) = \\frac{\\alpha_j}{\\sum^m_{i=1} \\alpha_i}$$\n",
    "\n",
    ">* $\\boldsymbol{\\alpha}=[\\alpha_1,...,\\alpha_m]^T$: shape parameters\n",
    ">* $B(\\boldsymbol{\\alpha})$: multivariate Beta fn.\n",
    ">* Symmetric Dirichlet distribution: $\\alpha_i = \\alpha, \\forall i$ \n",
    ">  * `w=randg(alpha,D,1); bar(w/sum(w));`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Document Models\n",
    "\n",
    "* **Simple Model**\n",
    "\n",
    "><img src='images/image25.png' width = 200>\n",
    "\n",
    ">* $w_{nd} \\sim \\text{Cat}(\\boldsymbol{\\beta})$, where $\\boldsymbol{\\beta}=[\\beta_1,...,\\beta_M]^T$\n",
    ">* $n$-th word in $d$-th document, $M$-words\n",
    "\n",
    "* **Simple Model - MLE**\n",
    "\n",
    ">$$\\log p(\\mathbf{w}|\\boldsymbol{\\beta}) = \\sum^M_{m=1} c_m \\log \\beta_m$$\n",
    "\n",
    ">$$\\text{Cost: } F = \\sum^M_{m=1} c_m \\log \\beta_m + \\lambda \\left( 1-\\sum^M_{m=1} \\beta_m \\right) \\;\\;\\rightarrow\\;\\; \\beta_m = \\frac{c_m}{n}$$\n",
    "\n",
    "* **Mixture of Categoricals Model**\n",
    "\n",
    "><img src='images/image26.png' width = 300>\n",
    "\n",
    ">\\begin{align}\n",
    "p(\\mathbf{w}|\\boldsymbol{\\theta},\\boldsymbol{\\beta}) &= \\prod^D_{d=1} p(\\mathbf{w}_d | \\boldsymbol{\\theta},\\boldsymbol{\\beta}) \\\\\n",
    "&= \\prod^D_{d=1} \\sum^K_{k=1} p(\\mathbf{w}_d,z_d=k|\\boldsymbol{\\theta},\\boldsymbol{\\beta}) \\\\\n",
    "&= \\prod^D_{d=1} \\sum^K_{k=1} p(z_d=k|\\boldsymbol{\\theta})p(\\mathbf{w}_d|z_d=k,\\boldsymbol{\\beta}_k) \\\\\n",
    "&= \\prod^D_{d=1} \\sum^K_{k=1} p(z_d=k|\\boldsymbol{\\theta}) \\sum^{N_d}_{n=1} p(w_{nd}|z_d=k,\\boldsymbol{\\beta}_k) \\\\\n",
    "\\end{align}\n",
    "\n",
    "* **Mixture of Categoricals Model - EM**\n",
    "\n",
    ">$$F(\\mathbf{R},\\boldsymbol{\\theta},\\boldsymbol{\\beta}) = \\sum_{k,d} r_{kd} \\left( \\sum^M_{m=1} c_{md} \\log \\beta_{km} + \\log \\theta_k \\right)$$\n",
    "\n",
    ">$$\\hat{\\theta}_k = \\underset{\\theta_k}{\\text{argmax}} \\left[ F(\\mathbf{R},\\boldsymbol{\\theta},\\boldsymbol{\\beta}) + \\lambda \\left( 1-\\sum^K_{k'=1} \\theta_k' \\right) \\right] = \\frac{\\sum^D_{d=1} r_{kd}}{D}$$\n",
    "\n",
    ">$$\\hat{\\beta}_{km} = \\underset{\\beta_{km}}{\\text{argmax}} \\left[ F(\\mathbf{R},\\boldsymbol{\\theta},\\boldsymbol{\\beta}) + \\sum^K_{k'=1} \\lambda_{k'} \\left( 1-\\sum^M_{m'=1} \\beta_k'm' \\right) \\right] = \\frac{\\sum^D_{d=1} r_{kd} c_{md} }{\\sum^M_{m'=1} \\sum^D_{d=1} r_{kd} c_{m'd}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Gibbs Sampling for Bayesian Mixture\n",
    "\n",
    "* **Bayesian Mixture of Categoricals Model**\n",
    "  * Returns **posterior** distributions of $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\beta}$\n",
    "  \n",
    "><img src='images/image27.png' width = 550>\n",
    "\n",
    "\n",
    "* **Latent Posterior**\n",
    "\n",
    ">$$p(y_n|z_n=k,\\boldsymbol{\\beta}) = p(y_n|\\beta_k) = p(y_n|\\beta_{z_n}) \\;\\;\\;,\\;\\;\\; p(\\boldsymbol{\\beta}|\\gamma) = \\text{Dir}(\\gamma)$$\n",
    "\n",
    ">$$p(z_n = k | \\boldsymbol{\\beta}) = \\theta_k \\;\\;\\;,\\;\\;\\; p(\\boldsymbol{\\theta}|\\alpha) = \\text{Dir}(\\alpha)$$\n",
    "\n",
    ">$$\\therefore \\;\\;\\; p(z_n = k | y_n, \\boldsymbol{\\theta}, \\boldsymbol{\\beta}) \\propto p(z_n=k | \\boldsymbol{\\theta}) p(y_n | z_n=k, \\boldsymbol{\\beta}) \\propto \\theta_k p(y_n | \\beta_{z_n})$$\n",
    "\n",
    "* **Gibbs Sampling**\n",
    "\n",
    ">* Component parameters\n",
    "\n",
    ">$$p(\\beta_k | \\mathbf{y}, \\mathbf{z}) \\propto p(\\beta_k) \\prod_{n:z_n=k} p(y_n|\\beta_k)$$\n",
    "\n",
    ">* Latent allocations\n",
    "\n",
    ">$$p(z_n = k | y_n, \\boldsymbol{\\theta}, \\boldsymbol{\\beta}) \\propto \\theta_k p(y_n|\\beta_{z_n})$$\n",
    "\n",
    ">* Mixing proportions ($c_k$: counts for mixture $k$)\n",
    "\n",
    ">$$p(\\boldsymbol{\\theta}|\\mathbf{z}, \\alpha) \\propto p(\\boldsymbol{\\theta}|\\alpha) p(\\mathbf{z}|\\boldsymbol{\\theta}) = \\text{Dir} \\left( \\frac{c_k + \\alpha_k}{\\sum^K_{j=1} c_j + \\alpha_j} \\right)$$\n",
    "\n",
    "* **Collapsed Gibbs Sampler**\n",
    "\n",
    ">$$$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. Latent Dirichlet Allocation for Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
