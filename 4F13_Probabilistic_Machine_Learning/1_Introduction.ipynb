{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4F13: Probabilistic Machine Learning\n",
    "\n",
    "Lecturer: Prof. Carl Edward Rasmussen\n",
    "\n",
    "----\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    ">## 1. Introduction to Probabilistic Machine Learning\n",
    "* 1.1. Mathematical Models\n",
    "* 1.2. Linear in the Parameters Model\n",
    "* 1.3. Likelihood and Noise\n",
    "* 1.4. Probability Basics\n",
    "* 1.5. Bayesian Inference and Prediction in Finite Regression Models\n",
    "\n",
    ">## 2. Gaussian Process\n",
    "* 2.1. Introduction\n",
    "* 2.2. Gaussian Process\n",
    "* 2.3. Non-Parametric Gaussian Process Models\n",
    "* 2.4. GP Marginal Likelihood and Hyperparameters\n",
    "* 2.5. Linear in the Parameters Models and GP\n",
    "* 2.6. Finite and Infinite Basis GPs\n",
    "* 2.7. Covariance Functions\n",
    "\n",
    ">## 3. Probabilistic Ranking\n",
    "* 3.1. Introduction to Ranking\n",
    "* 3.2. Gibbs Sampling\n",
    "* 3.3. Gibbs Sampling in TrueSkill\n",
    "* 3.4. Factor Graphs and Message Passing\n",
    "* 3.5. Message Passing in TrueSkill\n",
    "\n",
    ">## 4. Modelling Document Collections\n",
    "* 4.1. Introduction\n",
    "* 4.2. Discrete Binary Distributions\n",
    "* 4.3. Discrete Categorical Distribution\n",
    "* 4.4. Document Models\n",
    "* 4.5. Gibbs Sampling for Bayesian Mixture\n",
    "* 4.6. Latent Dirichlet Allocation for Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Probabilistic Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Mathematical Models\n",
    "\n",
    "* **Purpose**\n",
    "\n",
    ">1. Make predictions\n",
    ">2. Generalise from observations (inter/extrapolation)\n",
    ">3. Understand and interpret statistical relationships\n",
    ">4. Evaluate the relative probability of hypothesis about the data\n",
    ">5. Compress or summarise data\n",
    ">6. Generate more data, from a similar distribution\n",
    "\n",
    "* **Originate from**\n",
    "\n",
    ">1. **First Principles** (e.g. Newtonian mechanics)\n",
    ">2. **Observations** (data) $\\Rightarrow$ ***Machine Learning*** significantly relies on data\n",
    "\n",
    "* **Rely on**\n",
    "\n",
    ">1. **Knowledge** (expressed through ***priors***)\n",
    ">2. **Assumptions** (e.g. conditional independence)\n",
    ">3. **Simplifying assumptions** (if they are 'good enough')\n",
    "\n",
    "* **Terminology:**\n",
    "\n",
    "><img src=\"images/image1_01.png\" width=300>\n",
    ">\n",
    ">* $y$: observations\n",
    ">* $x$: unobserved or hidden or latent variables (# grow with data)\n",
    ">* $A$: parameter for transitions / $C$: parameter for emissions (# fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Linear in the Parameters Regression\n",
    "\n",
    "* **Model**\n",
    "\n",
    ">$$f_w(x)=\\sum^{M}_{j=0}{w_j\\Phi_j(x)} \\;\\;\\; \\text{where} \\;\\;\\; \\Phi_j(x)=x^j$$\n",
    ">\n",
    ">$$\\hat{y}=\\Phi \\mathbf{w}$$\n",
    "\n",
    "* **Least Squares Fit**\n",
    "\n",
    ">$$\\text{cost:} \\;\\;\\; \\text{E}(\\mathbf{w})=(y-\\hat{y})^T(y-\\hat{y})=(y-\\Phi \\mathbf{w})^T(y-\\Phi \\mathbf{w})$$\n",
    "\n",
    ">$$\\frac{\\partial \\text{E}(\\mathbf{w})}{\\partial \\mathbf{w}}=0 \\;\\;\\; \\Rightarrow \\;\\;\\; \\hat{\\mathbf{w}}=(\\Phi^T\\Phi)^{-1}\\Phi^Ty$$\n",
    "\n",
    "* **Overfitting**\n",
    "\n",
    "><img src=\"images/image1_02.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Likelihood and Noise\n",
    "\n",
    "><img src=\"images/image1_03.png\" width=400>\n",
    "\n",
    "* **Gaussian Noise Assumption**\n",
    "\n",
    ">$$\\boldsymbol{\\epsilon}\\; \\text{~} \\; \\mathcal{N}(\\boldsymbol{\\epsilon}_n;\\textbf{0},\\sigma^2_{noise} \\textbf{I})$$\n",
    "\n",
    "* **Maximum Likelihood Estimate**\n",
    "\n",
    ">$$p(\\mathbf{y}|\\mathbf{f}, {\\sigma}^2_{noise})=\\mathcal{N}(\\mathbf{y};\n",
    "\\mathbf{f},\\sigma^2_{noise})=\\left( \\frac{1}{\\sqrt{2\\pi \\sigma^2_{noise}}} \\right)^N \\exp{\\left( -\\frac{||\\mathbf{y}-\\mathbf{f}||^2}{2\\sigma^2_{noise}}\\right)}$$\n",
    "\n",
    ">* **ML solution = Least squares solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Probability Basics\n",
    "* **Sum Rule**\n",
    "\n",
    ">$$p(A)=\\sum_B{p(A,B)}\\;\\;\\;\\text{or}\\;\\;\\;p(A)=\\int_B{p(A,B)dB}$$\n",
    "\n",
    "* **Product Rule**\n",
    "\n",
    ">$$p(A,B)=p(A|B)p(B)$$\n",
    "\n",
    "* **Bayes' Rule**\n",
    "\n",
    ">$$p(A|B)=\\frac{p(A,B)}{p(B)}=\\frac{p(B|A)p(A)}{p(B)}$$\n",
    ">\n",
    ">* $p(A)$: **marginal**\n",
    ">* $p(B|A)$: **conditional**\n",
    ">* $p(A,B)$: **joint**\n",
    ">* If $A$ and $B$ are independent, $p(A,B)=p(A)p(B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Bayesian Inference and Prediction in Finite Regression Models\n",
    "\n",
    "* **Posterior**\n",
    "\n",
    ">$$p(\\mathbf{w}|\\mathbf{x},\\mathbf{y},\\mathcal{M})=\\frac{p(\\mathbf{w}|\\mathcal{M})p(\\mathbf{y}|\\mathbf{x},\\mathbf{w},\\mathcal{M})}{p(\\mathbf{y}|\\mathbf{x},\\mathcal{M})}=\\mathcal{N}(\\mathbf{w;μ,\\Sigma})$$\n",
    ">$$\\;$$\n",
    ">* **Gaussian Prior**: $p(\\mathbf{w}|\\mathcal{M})=\\mathcal{N}(\\mathbf{w};\\textbf{0},\\sigma^2_\\mathbf{w}\\mathbf{I})$\n",
    ">$$\\;$$\n",
    ">* **Gaussian Likelihood**: $p(\\mathbf{y}|\\mathbf{x},\\mathbf{w},\\mathcal{M})=\\mathcal{N}(\\mathbf{y};\\mathbf{Φw},\\sigma^2_{noise}\\mathbf{I})$\n",
    ">$$\\;$$\n",
    ">* **Marginal Likelihood**: $p(\\mathbf{y}|\\mathbf{x},\\mathcal{M})=\\int{p(\\mathbf{w}|\\mathbf{x},\\mathcal{M})p(\\mathbf{y}|\\mathbf{x},\\mathbf{w},\\mathcal{M})\\text{d}\\mathbf{w}}$\n",
    ">$$\\;$$\n",
    ">$$\\mathbf{μ}=\\left( \\mathbf{Φ}^T\\mathbf{Φ} + \\frac{\\sigma^2_{noise}}{\\sigma^2_{\\mathbf{w}}}\\mathbf{I} \\right)^{-1}\\mathbf{Φ}^T\\mathbf{y} \\;\\;\\;,\\;\\;\\; \\mathbf{Σ}=\\left( \\sigma^{-2}_{noise}\\mathbf{Φ}^T\\mathbf{Φ}+\\sigma^{-2}_{\\mathbf{w}}\\mathbf{I}\\right)^{-1}$$\n",
    "\n",
    "\n",
    "* **Bayesian Inference**\n",
    "\n",
    ">\\begin{align}\n",
    "p(y_*|x_*,\\mathbf{x},\\mathbf{y},\\mathcal{M})&=\\int{p(y_*,\\mathbf{w}|\\mathbf{x},\\mathbf{y},x_*,\\mathcal{M})\\text{d}\\mathbf{w}}\\\\\n",
    "&= \\int{p(y_*|\\mathbf{w},x_*,\\mathcal{M})p(\\mathbf{w}|\\mathbf{x},\\mathbf{y},\\mathcal{M})\\text{d}\\mathbf{w}}\\\\\n",
    "&=\\mathcal{N}(y_*;\\mathbf{Φ}(x_*)^T\\mathbf{μ},\\mathbf{Φ}(x_*)^T\\mathbf{Σ}\\mathbf{Φ}(x_*)+\\sigma^2_{noise}\\mathbf{I})\n",
    "\\end{align}\n",
    "\n",
    "* **Evidence** (marginal likelihood, used to select between models)\n",
    "\n",
    ">\\begin{align}\n",
    "p(\\mathcal{M}|\\mathbf{x,y}) \\propto p(\\mathbf{y|\\mathcal{M},x}) &= \\int{p(\\mathbf{w}|\\mathbf{x},\\mathcal{M})p(\\mathbf{y}|\\mathbf{x},\\mathbf{w},\\mathcal{M})\\text{d}\\mathbf{w}}\\\\\n",
    "&=\\mathcal{N}(\\mathbf{y;0,\\sigma^2_{w}ΦΦ}^T+\\sigma^2_{noise}\\mathbf{I})\n",
    "\\end{align}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
