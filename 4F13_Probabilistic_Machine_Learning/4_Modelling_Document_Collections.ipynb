{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modelling Document Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Introduction\n",
    "\n",
    "* **Bag of Words:** frequency of occurrence of every distinct word\n",
    "* **Zipf's Law:** the frequency of any word is inversely proportional to its rank in the frequency table\n",
    "\n",
    "><img src = 'images/image4_01.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Discrete Binary Distributions\n",
    "\n",
    "* **Discrete Distributions**\n",
    "\n",
    ">||binary|multi-valued|\n",
    "|-|-|-|\n",
    "|**sequence**|binary categorical, $\\pi^k(1-\\pi)^{n-k}$|categorical, $\\prod^k_{i=1} \\pi_i^{k_i}$|\n",
    "|**counts**|binomial|multinomial|\n",
    "\n",
    "* **Bernoulli Distribution**\n",
    "\n",
    ">$$p(X=1)=\\pi \\;\\;\\;,\\;\\;\\; p(X=0)=1-\\pi$$\n",
    ">$$p(X=x|\\pi) = \\pi^x (1-\\pi)^{1-x}$$\n",
    "\n",
    "* **Binomial Distribution** ($k$ heads out of $n$ tosses)\n",
    "\n",
    ">$$p(k|\\pi, n) = \\left( \\begin{matrix} n \\\\ k \\end{matrix} \\right) \\pi^k (1-\\pi)^{n-k} \\;\\;\\;\\rightarrow\\;\\;\\; \\textbf{MLE: } \\pi = \\frac{k}{n}$$\n",
    "\n",
    ">$$\\left( \\begin{matrix} n \\\\ k \\end{matrix} \\right) = \\frac{n!}{k!(n-k)!}$$\n",
    "\n",
    "* **Beta Distribution**\n",
    "\n",
    ">$$\\text{Beta}(\\pi|\\alpha,\\beta) = \\frac{1}{B(\\alpha,\\beta)} \\pi^{\\alpha-1} (1-\\pi)^{\\beta-1} \\;\\;\\;\\rightarrow\\;\\;\\; E(\\pi) = \\frac{\\alpha}{\\alpha+\\beta}$$\n",
    "\n",
    ">$$\\frac{1}{B(\\alpha,\\beta)} = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\;\\;\\;,\\;\\;\\; \\Gamma(\\alpha)=\\int^\\infty_0 x^{\\alpha-1} e^{-x} dx$$\n",
    "\n",
    "* **Posterior** (Prior: Beta, Likelihood: Binomial)\n",
    "\n",
    ">\\begin{align}\n",
    "p(\\pi|\\mathcal{D}) &= \\frac{p(\\pi|\\alpha,\\beta)p(\\mathcal{D}|\\pi)}{p(\\mathcal{D})} \\\\\n",
    "&\\propto \\text{Beta}(\\pi|\\alpha,\\beta) \\pi^k (1-\\pi)^{n-k} \\\\\n",
    "&\\propto \\text{Beta}(\\pi|\\alpha+k,\\beta+(n-k))\n",
    "\\end{align}\n",
    "\n",
    "* **Making Predictions**\n",
    "\n",
    ">$$p(x_{next}=1 | \\mathcal{D}) = \\int p(x=1|\\pi) p(\\pi|\\mathcal{D}) d\\pi$$\n",
    "\n",
    ">$$p(\\pi > 0.5 | \\mathcal{D}) = \\int^1_{0.5} p(\\pi'|\\mathcal{D}) d\\pi'$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Discrete Categorical Distribution\n",
    "\n",
    "* **Categorical Distribution** (Multinomial with one trial)\n",
    "* **Multinomial Distribution**\n",
    "\n",
    ">$$p(\\mathbf{k}|\\boldsymbol{\\pi},n) = \\frac{n!}{k_1!...k_m!} \\prod^m_{i=1} \\pi_i^{k_i}$$\n",
    "\n",
    ">$$\\sum^m_{i=1} k_i = n \\;\\;\\;,\\;\\;\\; \\sum^m_{i=1} \\pi_i = 1$$\n",
    "\n",
    "* **Dirichlet Distribution**\n",
    "\n",
    ">$$\\text{Dir}(\\boldsymbol{\\pi}|\\alpha_1,...\\alpha_m) = \\frac{\\Gamma (\\sum^m_{i=1} \\alpha_i)}{\\prod^m_{i=1} \\Gamma(\\alpha_i)} \\prod^m_{i=1} \\pi_i^{\\alpha_i - 1} = \\frac{1}{B(\\boldsymbol{\\alpha})} \\prod^m_{i=1} \\pi_i^{\\alpha_i - 1}$$\n",
    "\n",
    ">$$E(\\pi_j) = \\frac{\\alpha_j}{\\sum^m_{i=1} \\alpha_i}$$\n",
    "\n",
    ">* $\\boldsymbol{\\alpha}=[\\alpha_1,...,\\alpha_m]^T$: shape parameters\n",
    ">* $B(\\boldsymbol{\\alpha})$: multivariate Beta fn.\n",
    ">* Symmetric Dirichlet distribution: $\\alpha_i = \\alpha, \\forall i$ \n",
    ">  * `w=randg(alpha,D,1); bar(w/sum(w));`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Document Models\n",
    "\n",
    "* **Simple Model**\n",
    "\n",
    "><img src='images/image4_02.png' width = 200>\n",
    "\n",
    ">* $w_{nd} \\sim \\text{Cat}(\\boldsymbol{\\beta})$, where $\\boldsymbol{\\beta}=[\\beta_1,...,\\beta_M]^T$\n",
    ">* $n$-th word in $d$-th document, $M$-words\n",
    "\n",
    "* **Simple Model - MLE**\n",
    "\n",
    ">$$\\log p(\\mathbf{w}|\\boldsymbol{\\beta}) = \\sum^M_{m=1} c_m \\log \\beta_m$$\n",
    "\n",
    ">$$\\text{Cost: } F = \\sum^M_{m=1} c_m \\log \\beta_m + \\lambda \\left( 1-\\sum^M_{m=1} \\beta_m \\right) \\;\\;\\rightarrow\\;\\; \\beta_m = \\frac{c_m}{n}$$\n",
    "\n",
    "* **Mixture of Categoricals Model**\n",
    "\n",
    "><img src='images/image4_03.png' width = 300>\n",
    "\n",
    ">\\begin{align}\n",
    "p(\\mathbf{w}|\\boldsymbol{\\theta},\\boldsymbol{\\beta}) &= \\prod^D_{d=1} p(\\mathbf{w}_d | \\boldsymbol{\\theta},\\boldsymbol{\\beta}) \\\\\n",
    "&= \\prod^D_{d=1} \\sum^K_{k=1} p(\\mathbf{w}_d,z_d=k|\\boldsymbol{\\theta},\\boldsymbol{\\beta}) \\\\\n",
    "&= \\prod^D_{d=1} \\sum^K_{k=1} p(z_d=k|\\boldsymbol{\\theta})p(\\mathbf{w}_d|z_d=k,\\boldsymbol{\\beta}_k) \\\\\n",
    "&= \\prod^D_{d=1} \\sum^K_{k=1} p(z_d=k|\\boldsymbol{\\theta}) \\sum^{N_d}_{n=1} p(w_{nd}|z_d=k,\\boldsymbol{\\beta}_k) \\\\\n",
    "\\end{align}\n",
    "\n",
    "* **Mixture of Categoricals Model - EM**\n",
    "\n",
    ">$$F(\\mathbf{R},\\boldsymbol{\\theta},\\boldsymbol{\\beta}) = \\sum_{k,d} r_{kd} \\left( \\sum^M_{m=1} c_{md} \\log \\beta_{km} + \\log \\theta_k \\right)$$\n",
    "\n",
    ">$$\\hat{\\theta}_k = \\underset{\\theta_k}{\\text{argmax}} \\left[ F(\\mathbf{R},\\boldsymbol{\\theta},\\boldsymbol{\\beta}) + \\lambda \\left( 1-\\sum^K_{k'=1} \\theta_k' \\right) \\right] = \\frac{\\sum^D_{d=1} r_{kd}}{D}$$\n",
    "\n",
    ">$$\\hat{\\beta}_{km} = \\underset{\\beta_{km}}{\\text{argmax}} \\left[ F(\\mathbf{R},\\boldsymbol{\\theta},\\boldsymbol{\\beta}) + \\sum^K_{k'=1} \\lambda_{k'} \\left( 1-\\sum^M_{m'=1} \\beta_k'm' \\right) \\right] = \\frac{\\sum^D_{d=1} r_{kd} c_{md} }{\\sum^M_{m'=1} \\sum^D_{d=1} r_{kd} c_{m'd}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Gibbs Sampling for Bayesian Mixture\n",
    "\n",
    "* **Bayesian Mixture of Categoricals Model**\n",
    "  * Returns **posterior** distributions of $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\beta}$\n",
    "  \n",
    "><img src='images/image4_04.png' width = 550>\n",
    "\n",
    "\n",
    "* **Latent Posterior**\n",
    "\n",
    ">$$p(y_n|z_n=k,\\boldsymbol{\\beta}) = p(y_n|\\beta_k) = p(y_n|\\beta_{z_n}) \\;\\;\\;,\\;\\;\\; p(\\boldsymbol{\\beta}_k|\\gamma) = \\text{Dir}(\\gamma)$$\n",
    "\n",
    ">$$p(z_n = k | \\boldsymbol{\\theta}) = \\theta_k \\;\\;\\;,\\;\\;\\; p(\\boldsymbol{\\theta}|\\alpha) = \\text{Dir}(\\alpha)$$\n",
    "\n",
    ">$$\\therefore \\;\\;\\; p(z_n = k | y_n, \\boldsymbol{\\theta}, \\boldsymbol{\\beta}) \\propto p(z_n=k | \\boldsymbol{\\theta}) p(y_n | z_n=k, \\boldsymbol{\\beta}) \\propto \\theta_k p(y_n | \\beta_{z_n})$$\n",
    "\n",
    ">* $\\rightarrow$ Discrete distribution with $K$ possible outcomes\n",
    "\n",
    "* **Gibbs Sampling**\n",
    "\n",
    ">* Component parameters (the mixture aspect eliminated)\n",
    "\n",
    ">$$p(\\beta_k | \\mathbf{y}, \\mathbf{z}) \\propto p(\\beta_k) \\prod_{n:z_n=k} p(y_n|\\beta_k)$$\n",
    "\n",
    ">* Latent allocations\n",
    "\n",
    ">$$p(z_n = k | y_n, \\boldsymbol{\\theta}, \\boldsymbol{\\beta}) \\propto \\theta_k p(y_n|\\beta_{z_n})$$\n",
    "\n",
    ">* Mixing proportions ($c_k$: counts for mixture $k$)\n",
    "\n",
    ">$$p(\\boldsymbol{\\theta}|\\mathbf{z}, \\alpha) \\propto p(\\boldsymbol{\\theta}|\\alpha) p(\\mathbf{z}|\\boldsymbol{\\theta}) = \\text{Dir} \\left( \\frac{c_k + \\alpha_k}{\\sum^K_{j=1} c_j + \\alpha_j} \\right)$$\n",
    "\n",
    "* **Collapsed Gibbs Sampler**\n",
    "\n",
    ">* Marginalisaiton over $\\theta$ ($-n$: all except $n$)\n",
    "\n",
    ">$$p(z_n=k|\\mathbf{z}_{-n},\\alpha) = \\frac{\\alpha+c_{-n,k}}{\\sum^K_{j=1} \\alpha + c_{-n,j}}$$\n",
    "\n",
    ">* Collapsed Gibbs Sampler for the latent assignments\n",
    "\n",
    ">$$p(z_n=k|y_n,z_{-n},\\boldsymbol{\\theta},\\alpha) \\propto p(y_n|\\beta_k) \\frac{\\alpha + c_{-n,k}}{\\sum^K_{j=1} \\alpha + c_{-n,j}}$$\n",
    "\n",
    ">* $z_n$: cond. independent given $theta$ $\\rightarrow$ **dependent**\n",
    ">* ***Rich get Richer*** property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. Latent Dirichlet Allocation for Topic Modelling\n",
    "\n",
    "* **MoC - Generative Model**\n",
    "\n",
    ">1. Draw a distribution $\\boldsymbol{\\theta}$ over $K$ topics from $\\text{Dir}(\\alpha)$\n",
    ">2. For each topic $k$, draw a distribution $\\boldsymbol{\\beta}_k$ over words from $\\text{Dir}(\\gamma)$\n",
    ">3. For each document $d$, draw a topic $z_d$ from $\\text{Cat}(\\boldsymbol{\\theta})$\n",
    ">4. For each document $d$, draw $N_d$ words $w_{nd}$ from $\\text{Cat}(\\boldsymbol{\\beta}_{z_d})$\n",
    ">  * $\\rightarrow$ **However, the documents may span more than one topic**\n",
    "\n",
    "* **LDA - Generative Model** (Now, every document has $\\boldsymbol{\\theta}_d$)\n",
    "\n",
    ">1. For each document $d$, draw a distribution $\\boldsymbol{\\theta}_d$ over topics from $\\text{Dir}(\\alpha)$\n",
    ">2. For each topic $k$, draw a distribution $\\boldsymbol{\\beta}_k$ over words from $\\text{Dir}(\\gamma)$\n",
    ">3. Draw a topic $z_{nd}$ for the $n$-th word in document $d$ from $\\text{Cat}(\\boldsymbol{\\theta}_d)$\n",
    ">4. Draw word $w_{nd}$ from $\\text{Cat}(\\boldsymbol{\\beta}_{z_{nd}})$\n",
    "\n",
    "* **LDA - Graphical Model**\n",
    "\n",
    "><img src='images/image4_05.png' width=400>\n",
    "\n",
    ">* Each **topic**: distribution over words\n",
    ">* Each **document**: mixture of corpus-wide topics\n",
    ">* Each **word**: drawn from one of these topics\n",
    "\n",
    ">  * **Goal:** infer hidden variables from documents\n",
    ">  * **Hidden Variables:** topics, proportions, assignments\n",
    "\n",
    "* **LDA - Inference Problem** $\\rightarrow$ **Intractable**\n",
    "\n",
    ">$$p(\\boldsymbol{\\beta}_{1:K},\\boldsymbol{\\beta}_{1:D},\\{z_{nd}\\},\\{w_{nd}\\}|\\gamma,\\alpha) $$\n",
    "$$\\;$$\n",
    "$$= \\prod^K_{k=1} \\; p(\\boldsymbol{\\beta}_k|\\gamma) \\; \\prod^D_{d=1} \\; \\bigg[ \\; p(\\boldsymbol{\\theta}_d|\\alpha) \\prod^{N_d}_{n=1} \\; \\big[ \\; p(z_{nd}|\\boldsymbol{\\theta}_d) \\; p(w_{nd}|\\boldsymbol{\\beta}_{1:K},z_{nd}) \\; \\big] \\; \\bigg]$$\n",
    "\n",
    ">* Normalising constant of the posterior (evidence): Average over all possible $z_{nd}$\n",
    "\n",
    ">$$p(\\{w_{id}\\}) = \\int \\int \\sum_{z_{id}} \\prod^D_{d=1} \\prod^K_{k=1} \\prod^{N_d}_{n=1} p(z_{nd}|\\boldsymbol{\\theta}_d) p(\\boldsymbol{\\theta}_d|\\alpha) p(w_{nd}|\\boldsymbol{\\beta}_{1:K}, z_{nd}) p(\\boldsymbol{\\beta}_k|\\gamma) d\\boldsymbol{\\beta}_k d\\boldsymbol{\\theta}_d$$\n",
    "\n",
    "* **Collapsed Gibbs Sampler for LDA**\n",
    "\n",
    ">\\begin{align}\n",
    "p(z_{nd}=k|\\{z_{-nd}\\},\\{w\\},\\gamma,\\alpha) &\\propto p(z_{nd}=k|\\{z_{-nd}\\},\\alpha) \\; p(w_{nd}|z_{nd}=k,\\{w_{-nd}\\},\\{z_{-nd}\\},\\gamma) \\\\\n",
    "\\;\\\\\n",
    "&= \\frac{\\alpha + c^k_{-nd}}{\\sum^K_{j=1} \\left( \\alpha+c^j_{-nd} \\right)} \\frac{\\gamma + \\tilde{c}^k_{-w_{nd}}}{\\sum^M_{m=1} \\left( \\gamma + \\tilde{c}^k_{-m} \\right)}\n",
    "\\end{align}\n",
    "\n",
    ">* $c^k_{-nd}$: no. of words from document $d$, excluding $n$, assigned to topic $k$\n",
    ">* $\\tilde{c}^k_{-m}$: no. of times word $m$ was generated from topic $k$, excluding $nd$\n",
    "\n",
    "* **Per Word Perplexity**\n",
    "\n",
    ">$$\\text{perplexity} = \\exp \\left( -\\frac{\\text{log joint prob. over the words}}{\\text{number of words}} \\right)$$\n",
    "\n",
    ">* Perplexity of $g$: uncertainty associated with a dice with $g$ sides"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
