{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Introduction\n",
    "\n",
    "* **Posterior Distribution over Functions:**\n",
    "\n",
    ">* Parametric model: represented using **parameters** (nuisance)\n",
    ">* The aim is to work directly in the **space of functions**\n",
    ">  * **Step 1.** Set up a model in terms of parameters\n",
    ">  * **Step 2.** Marginalise out the parameters\n",
    "\n",
    ">$$p(\\mathbf{f}|\\mathbf{y})=\\frac{p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f})}{p(\\mathbf{y})}$$\n",
    "\n",
    ">* $p(\\mathbf{f})$: Of all the functions generated from the prior,\n",
    ">* $p(\\mathbf{y}|\\mathbf{f})$: Keep those that fit the data\n",
    "\n",
    "* **Gaussian Distribution - PDF**\n",
    "\n",
    ">\\begin{align}\n",
    "p(x|\\mu,\\sigma^2)&=(2\\pi\\sigma^2)^{-1/2}\\exp{\\left( -\\frac{1}{2\\sigma^2} (x-\\mu)^2 \\right)}\\\\\n",
    "p(\\mathbf{x|μ,Σ})&=\\det{(2\\pi\\mathbf{Σ})}^{-1/2}\\exp{ \\left( -\\frac{1}{2}(\\mathbf{x-μ})^T\\mathbf{Σ}^{-1}(\\mathbf{x-μ})\\right) }\n",
    "\\end{align}\n",
    "\n",
    ">* **Conditionals and Marginals of joint Gaussian are also Gaussian**\n",
    "\n",
    "* **Algebra**\n",
    "\n",
    ">$$p(\\mathbf{x,y})=p \\left( \\begin{bmatrix} \\mathbf{x} \\\\ \\mathbf{y} \\end{bmatrix} \\right) = \\mathcal{N} \\left( \\begin{bmatrix} \\mathbf{a} \\\\ \\mathbf{b} \\end{bmatrix} , \\begin{bmatrix} \\mathbf{A} & \\mathbf{B} \\\\ \\mathbf{B}^T & \\mathbf{C} \\end{bmatrix} \\right)$$\n",
    ">$$\\;$$\n",
    ">$$\\Rightarrow p(\\mathbf{x})=\\mathcal{N}(\\mathbf{a,A})$$\n",
    ">$$\\;$$\n",
    ">$$\\Rightarrow p(\\mathbf{x}|\\mathbf{y})=\\mathcal{N}(\\mathbf{a+BC}^{-1}(\\mathbf{y-b}),\\mathbf{A-BC}^{-1}\\mathbf{B}^T)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Gaussian Process\n",
    "\n",
    "* **Definition**\n",
    "\n",
    ">* A **Gaussian Process** is a collection of random variables, any finite number of which have (consistent) Gaussian distributions\n",
    ">* It is fully specified by the **mean function** $\\mathcal{m}(\\mathcal{x})$ and **covariance function** $\\mathcal{k}(x,x')$\n",
    ">\n",
    ">$$f \\; \\text{~} \\; \\mathcal{GP}(m,k)$$\n",
    ">\n",
    ">* $m(x)$: mean function, function on $\\mathcal{X}$\n",
    ">* $k(x,x')$: covariance function, function on $\\mathcal{X}\\times\\mathcal{X}$\n",
    "\n",
    "* **Marginalisation Property**\n",
    "\n",
    ">$$p(\\mathbf{x,y}) = \\mathcal{N} \\left( \\begin{bmatrix} \\mathbf{a} \\\\ \\mathbf{b} \\end{bmatrix} , \\begin{bmatrix} \\mathbf{A} & \\mathbf{B} \\\\ \\mathbf{B}^T & \\mathbf{C} \\end{bmatrix} \\right) \\;\\Rightarrow\\;\n",
    "p(\\mathbf{x})=\\mathcal{N}(\\mathbf{a,A})$$\n",
    "\n",
    "* **Joint Generation**\n",
    "\n",
    ">* Generate a random sample from a $D$-dim joint Gaussian with $\\mathbf{m}$ and $\\mathbf{K}$\n",
    "\n",
    ">>`z=randn(D,1);`\n",
    "\n",
    ">>`y=chol(K)'*z + m;`\n",
    "\n",
    ">* `chol(K)`: ***Cholesky factor***, $\\mathbf{R}$ such that $\\mathbf{R}^T \\mathbf{R} = \\mathbf{K}$\n",
    ">* $\\mathbb{E}[(\\mathbf{y-m})(\\mathbf{y-m})^T]=\\mathbb{E}[\\mathbf{R}^T\\mathbf{zz}^T\\mathbf{R}]=\\mathbf{R}^T\\mathbb{E}[\\mathbf{zz}^T]\\mathbf{R}=\\mathbf{R}^T\\mathbf{IR}=\\mathbf{K}$\n",
    "\n",
    "* **Sequential Generation**\n",
    "\n",
    ">$$p(f_n,f_{<n})= \\mathcal{N} \\left( \\begin{bmatrix} \\mathbf{a} \\\\ \\mathbf{b} \\end{bmatrix} , \\begin{bmatrix} \\mathbf{A} & \\mathbf{B} \\\\ \\mathbf{B}^T & \\mathbf{C} \\end{bmatrix} \\right)$$\n",
    ">$$\\;$$\n",
    "\n",
    ">$$p(f_n|f_{<n})=\\mathcal{N}(\\mathbf{a+BC}^{-1}(f_{<n}-\\mathbf{b}),\\mathbf{A-BC}^{-1}\\mathbf{B}^T)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Non-Parametric Gaussian Process Models\n",
    "\n",
    "* In non-parametric model, the **parameters** are the function itself\n",
    "* **Gaussian Likelihood**\n",
    "\n",
    ">$$p(\\mathbf{y}|\\mathbf{x},f,\\mathcal{M}_i) \\;\\text{~}\\; \\mathcal{N}(\\mathbf{f},\\sigma^2_{noise}\\mathbf{I})$$\n",
    "\n",
    "* **Gaussian Process Prior**\n",
    "\n",
    ">$$p(f|\\mathcal{M}_i) \\;\\text{~}\\; \\mathcal{GP}(m \\equiv 0,k)$$\n",
    "\n",
    "* **Gaussian Process Posterior**\n",
    "\n",
    ">$$p(f|\\mathbf{x},\\mathbf{y},\\mathcal{M}_i) \\;\\text{~}\\; \\mathcal{GP}(m_{post},k_{post})$$\n",
    ">$$\\;$$\n",
    ">\\begin{align}\n",
    "m_{post}(x)&=\\mathbf{k}(x,\\mathbf{x})[\\mathbf{K}(\\mathbf{x},\\mathbf{x})+\\sigma^2_{noise}\\mathbf{I}]^{-1}\\mathbf{y}\\\\\n",
    "k_{post}(x,x')&=k(x,x')-\\mathbf{k}(x,\\mathbf{x})[\\mathbf{K}(\\mathbf{x},\\mathbf{x})+\\sigma^2_{noise}\\mathbf{I}]^{-1}\\mathbf{k}(\\mathbf{x},x')\n",
    "\\end{align}\n",
    "\n",
    "* **Gaussian Predictive**\n",
    "\n",
    "><img src=\"images/image2_01.png\" width=550>\n",
    "\n",
    "* **Mean** (linear in 2 ways)\n",
    "\n",
    "><img src=\"images/image2_02.png\" width=600>\n",
    "\n",
    "* **Variance** (difference between 2 terms)\n",
    "\n",
    "><img src=\"images/image2_03.png\" width=430>\n",
    "\n",
    ">* 1st term: **prior variance**\n",
    ">* 2nd term: **how much the data $x$ has explained**\n",
    ">* **NOTE:** the variance is independent of the observed outputs $\\mathbf{y}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. GP Marginal Likelihood and Hyperparameters\n",
    "\n",
    "* **Log Marginal Likelihood:**\n",
    "\n",
    "><img src=\"images/image2_04.png\" width=550>\n",
    "\n",
    ">* First term: ***data fit***\n",
    ">* Second term: ***complexity penalty*** $\\rightarrow$ Occam's Razor\n",
    "\n",
    "* **Hyperparameters:**\n",
    "\n",
    ">$$k(x,x')=\\exp \\left( -\\frac{(x-x')^2}{2l^2} \\right)$$\n",
    "\n",
    ">* $l$: characteristic lengthscales\n",
    "\n",
    "* **Learning in GP:**\n",
    "\n",
    ">1. Find the form of the covariance function\n",
    ">2. Find any unknown (hyper)parameters $\\theta$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Linear in the Parameters Models and GP\n",
    "\n",
    "* **Q.** Does every GP corresponds to a linear in the parameters model?\n",
    "\n",
    ">* **A.** Yes, but not necessarily a finite one (Mercer's theorem)\n",
    "\n",
    "* **Computational complexity:**\n",
    "\n",
    ">* **GP:** $\\mathcal{O}(N^3)$ vs **Linear model:** $\\mathcal{O}(NM^2)$\n",
    ">* $N$: no. of training data\n",
    ">* $M$: no. of basis functions\n",
    "\n",
    "* **Linear Model with Gaussian Random Parameters:**\n",
    "\n",
    "><img src=\"images/image2_05.png\" width=380>\n",
    "\n",
    ">* **Mean fn.**\n",
    "\n",
    "><img src=\"images/image2_06.png\" width=500>\n",
    "\n",
    ">* **Covariance fn.**\n",
    "\n",
    "><img src=\"images/image2_07.png\" width=500>\n",
    "\n",
    "* **Finite Linear Model with Gaussian Priors on the Weights:**\n",
    "\n",
    "><img src=\"images/image2_08.png\" width=380>\n",
    "\n",
    ">* **Mean fn.**\n",
    "\n",
    "><img src=\"images/image2_09.png\" width=500>\n",
    "\n",
    ">* **Covariance fn.**\n",
    "\n",
    "><img src=\"images/image2_10.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Finite and Infinite Basis GPs\n",
    "\n",
    "* **GP with Squared Exponential Covariance fn.:** \n",
    "* $\\rightarrow$ corresponds to an infinite linear in the parameters model with Gaussian bumps everywhere\n",
    "\n",
    "><img src=\"images/image2_11.png\" width=500>\n",
    "\n",
    ">* **Mean fn.**\n",
    "\n",
    "><img src=\"images/image2_12.png\" width=500>\n",
    "\n",
    ">* **Covariance fn.**\n",
    "\n",
    "><img src=\"images/image2_13.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Covariance Functions\n",
    "\n",
    "* **ARD(Automatic Relevance Determination):**\n",
    "  * Used for feature/variable selection\n",
    "\n",
    "><img src=\"images/image2_14.png\" width=500>\n",
    "\n",
    "* **RQ(Rational Quadratic):**\n",
    "\n",
    "><img src=\"images/image2_15.png\" width=430>\n",
    "\n",
    ">* $r=x-x'$\n",
    ">* $l>0$\n",
    ">* $\\alpha \\rightarrow \\infty$: RQ becomes SE\n",
    "\n",
    "* **Matérn:**\n",
    "\n",
    "><img src=\"images/image2_16.png\" width=400>\n",
    "\n",
    ">* $\\text{K}_{\\nu}$: modified Bessel fn. of second kind of order $\\nu$\n",
    ">* $l$: characteristic length scale\n",
    ">* $\\lfloor \\nu - 1 \\rfloor$ times differentiable (degree of smoothness)\n",
    "\n",
    "><img src=\"images/image2_17.png\" width=430>\n",
    "\n",
    "* **Periodic:**\n",
    "\n",
    "><img src=\"images/image2_18.png\" width=330>\n",
    "\n",
    ">* Map the inputs: $u=(\\sin(x),\\cos(x))^T$\n",
    ">* & Measure the distance in $u$ space\n",
    ">* & Combine with SE covariance fn.\n",
    "\n",
    "* **Composite Covariance fn.:**\n",
    "\n",
    ">* Covariance fn. have to be positive definite\n",
    ">* Compose by **sum**, **products**, or **other combinations** (e.g. $g(x)k(x,x')g(x')$)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
