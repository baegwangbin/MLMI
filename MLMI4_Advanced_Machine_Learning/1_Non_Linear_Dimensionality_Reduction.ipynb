{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Non-liner Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Applications of Dimensionality Reduction**\n",
    "\n",
    ">* Modelling data on/near manifolds\n",
    ">* Visualization of high-dimensional data\n",
    ">* Simple building blocks for complex models (e.g. FA $\\rightarrow$ LGSSMs, GPLVM $\\rightarrow$ GPSSMs)\n",
    "\n",
    "* **Dimensionality Reduction: Conceptual Space**\n",
    "\n",
    "><img src = 'images/image1_01.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. DR via Distance\n",
    "\n",
    "* **Goal:** find a mapping that preserves the distance\n",
    "\n",
    ">$$d^{(y)}_{nm} = ||y^{(n)}-y^{(m)}|| \\approx d^{(x)}_{nm} = ||x^{(n)}-x^{(m)}||$$\n",
    "\n",
    "* **PCA:** Linear DR\n",
    "\n",
    ">* Data\n",
    "\n",
    ">$$\\mathcal{D} = \\{ \\mathbf{y}_{1}, \\cdots , \\mathbf{y}_{N} \\} \\;\\;\\;,\\;\\;\\; \\mathbf{y}_{n} \\in \\mathbb{R}^D \\;\\;\\;,\\;\\;\\; \\text{w.l.g. assume} \\;\\;\\; \\frac{1}{N} \\sum_n \\mathbf{y}_{n} = \\mathbf{0}$$\n",
    "\n",
    ">* Linear projection\n",
    "\n",
    ">$$\\mathbf{x}_n = \\mathbf{w}^T \\mathbf{y}_n \\;\\;\\;,\\;\\;\\; \\mathbf{x}_{n} \\in \\mathbb{R}^N$$\n",
    "\n",
    ">* Variance\n",
    "\n",
    ">$$\\text{Var}(x) = \\frac{1}{N} \\sum_n \\mathbf{x}_n \\mathbf{x}_n^T = \\frac{1}{N} \\sum_n \\mathbf{w}^T \\mathbf{y}_n \\mathbf{y}_n^T \\mathbf{w} = \\mathbf{w}^T \\left( \\frac{1}{N} \\sum_n \\mathbf{y}_n \\mathbf{y}_n^T \\right) \\mathbf{w} = \\mathbf{w}^T \\mathbf{\\Sigma}_y \\mathbf{w}$$\n",
    "\n",
    ">* Objective Function (regularization by setting $\\mathbf{w}^T \\mathbf{w} = 1$)\n",
    "\n",
    ">$$\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmax}} \\; \\mathbf{w}^T \\mathbf{\\Sigma}_y \\mathbf{w} - \\lambda(\\mathbf{w}^T \\mathbf{w} - 1)$$\n",
    "\n",
    ">* Solution\n",
    "\n",
    ">$$\\mathbf{\\Sigma}_y \\mathbf{w}^* = \\lambda \\mathbf{w}^*$$\n",
    "\n",
    ">$$\\mathbf{w}^*:  N \\text{ eigenvectors in the order of decreasing eigenvalues}$$\n",
    "\n",
    "* **ISOMAP:** Non-linear DR / geodesic distance via neighbourhood graph\n",
    "\n",
    ">1. Determine the **neighbors** of each point (e.g. kNN)\n",
    ">2. Construct a **neighborhood graph** (connect each point to its kNNs / edge length: Euclidean distance)\n",
    ">3. Compute **shortest path** between two nodes (e.g. Dijkstra's algorithm, Floyd-Warshall algorithm, ...)\n",
    ">4. Compute **lower-dimensional embedding** (MDS - multidimensional scaling)\n",
    "\n",
    "* **Dijkstra's Algorithm**\n",
    "\n",
    ">1. Create the **unvisited set** (containing all nodes)\n",
    ">1. Assign a **tentative distance** to every node ($0$ for initial node, $\\infty$ for others)\n",
    ">1. For every **unvisited neighbour** of the current node,\n",
    ">  1. calculate the tentative distance through the current node\n",
    ">  1. update if it is smaller than the current value\n",
    ">1. **Remove** the current node from the unvisited set\n",
    ">1. **Stop if:**\n",
    ">  1. destination node is visited (when planning a route between two specific nodes)\n",
    ">  1. smallest tentative distance in the unvisited set is $\\infty$ (when planning a complete traversal)\n",
    ">1. **Otherwise:**\n",
    ">  1. Current node $\\leftarrow$ unvisited node with the smallest tentative distance\n",
    ">  1. Go back to **Step 3**\n",
    "\n",
    "* **MDS** (a.k.a. **PCoA** - Principal Coordinates Analysis)\n",
    "\n",
    ">1. Set up the **squared proximity matrix**\n",
    ">$$$$\n",
    ">$$D^{(2)} = [d^2_{ij}]$$\n",
    ">$$$$\n",
    ">2. Apply **double centering** ($n$: no. of objects) \n",
    ">$$$$\n",
    ">$$B = -\\frac{1}{2} JD^{(2)}J \\;\\;\\;,\\;\\;\\; J=I-\\frac{1}{n} \\mathbf{1} \\mathbf{1}^T$$\n",
    ">$$$$\n",
    ">3. Determine $m$ largest **eigenvalues and corresponding eigenvectors** of $B$ ($m$: desired dimension)\n",
    ">4. $X=E_m \\Lambda_m^{1/2}$ ($E_m$: matrix of eigenvectors / $\\Lambda_m$: diagonal matrix of eigenvalues)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
