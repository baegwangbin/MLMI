{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Non-liner Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Applications of Dimensionality Reduction**\n",
    "\n",
    ">* Modelling data on/near manifolds\n",
    ">* Visualization of high-dimensional data\n",
    ">* Simple building blocks for complex models (e.g. FA $\\rightarrow$ LGSSMs, GPLVM $\\rightarrow$ GPSSMs)\n",
    "\n",
    "* **Dimensionality Reduction: Conceptual Space**\n",
    "\n",
    "><img src = 'images/image1_01.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. DR via Distance\n",
    "\n",
    "* **Goal:** find a mapping that preserves the distance\n",
    "\n",
    ">$$d^{(y)}_{nm} = ||y^{(n)}-y^{(m)}|| \\approx d^{(x)}_{nm} = ||x^{(n)}-x^{(m)}||$$\n",
    "\n",
    "* **PCA:** Linear DR\n",
    "\n",
    ">* Data\n",
    "\n",
    ">$$\\mathcal{D} = \\{ \\mathbf{y}_{1}, \\cdots , \\mathbf{y}_{N} \\} \\;\\;\\;,\\;\\;\\; \\mathbf{y}_{n} \\in \\mathbb{R}^D \\;\\;\\;,\\;\\;\\; \\text{w.l.g. assume} \\;\\;\\; \\frac{1}{N} \\sum_n \\mathbf{y}_{n} = \\mathbf{0}$$\n",
    "\n",
    ">* Linear projection\n",
    "\n",
    ">$$\\mathbf{x}_n = \\mathbf{w}^T \\mathbf{y}_n \\;\\;\\;,\\;\\;\\; \\mathbf{x}_{n} \\in \\mathbb{R}^K$$\n",
    "\n",
    ">* Variance\n",
    "\n",
    ">$$\\text{Var}(x) = \\frac{1}{N} \\sum_n \\mathbf{x}_n \\mathbf{x}_n^T = \\frac{1}{N} \\sum_n \\mathbf{w}^T \\mathbf{y}_n \\mathbf{y}_n^T \\mathbf{w} = \\mathbf{w}^T \\left( \\frac{1}{N} \\sum_n \\mathbf{y}_n \\mathbf{y}_n^T \\right) \\mathbf{w} = \\mathbf{w}^T \\mathbf{\\Sigma}_y \\mathbf{w}$$\n",
    "\n",
    ">* Objective Function (regularization by setting $\\mathbf{w}^T \\mathbf{w} = 1$)\n",
    "\n",
    ">$$\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmax}} \\; \\mathbf{w}^T \\mathbf{\\Sigma}_y \\mathbf{w} - \\lambda(\\mathbf{w}^T \\mathbf{w} - 1)$$\n",
    "\n",
    ">* Solution\n",
    "\n",
    ">$$\\mathbf{\\Sigma}_y \\mathbf{w}^* = \\lambda \\mathbf{w}^*$$\n",
    "\n",
    ">$$\\mathbf{w}^*:  N \\text{ eigenvectors in the order of decreasing eigenvalues}$$\n",
    "\n",
    "* **ISOMAP:** Non-linear DR / geodesic distance via neighbourhood graph\n",
    "\n",
    ">1. Determine the **neighbors** of each point (e.g. kNN)\n",
    ">2. Construct a **neighborhood graph** (connect each point to its kNNs / edge length: Euclidean distance)\n",
    ">3. Compute **shortest path** between two nodes (e.g. Dijkstra's algorithm, Floyd-Warshall algorithm, ...)\n",
    ">4. Compute **lower-dimensional embedding** (MDS - multidimensional scaling)\n",
    "\n",
    "* **Dijkstra's Algorithm**\n",
    "\n",
    ">1. Create the **unvisited set** (containing all nodes)\n",
    ">1. Assign a **tentative distance** to every node ($0$ for initial node, $\\infty$ for others)\n",
    ">1. For every **unvisited neighbour** of the current node,\n",
    ">  1. calculate the tentative distance through the current node\n",
    ">  1. update if it is smaller than the current value\n",
    ">1. **Remove** the current node from the unvisited set\n",
    ">1. **Stop if:**\n",
    ">  1. destination node is visited (when planning a route between two specific nodes)\n",
    ">  1. smallest tentative distance in the unvisited set is $\\infty$ (when planning a complete traversal)\n",
    ">1. **Otherwise:**\n",
    ">  1. Current node $\\leftarrow$ unvisited node with the smallest tentative distance\n",
    ">  1. Go back to **Step 3**\n",
    "\n",
    "* **MDS** (a.k.a. **PCoA** - Principal Coordinates Analysis)\n",
    "\n",
    ">1. Set up the **squared proximity matrix**\n",
    ">$$$$\n",
    ">$$D^{(2)} = [d^2_{ij}]$$\n",
    ">$$$$\n",
    ">2. Apply **double centering** ($n$: no. of objects) \n",
    ">$$$$\n",
    ">$$B = -\\frac{1}{2} JD^{(2)}J \\;\\;\\;,\\;\\;\\; J=I-\\frac{1}{n} \\mathbf{1} \\mathbf{1}^T$$\n",
    ">$$$$\n",
    ">3. Determine $K$ largest **eigenvalues and corresponding eigenvectors** of $B$ ($K$: desired dimension)\n",
    ">4. $X=E_K \\Lambda_K^{1/2}$ ($E_K$: matrix of eigenvectors / $\\Lambda_K$: diagonal matrix of eigenvalues)\n",
    "\n",
    "* **Limitations**\n",
    "\n",
    ">1. Non-linear embedding-based methods require optimisation of new representation $x$\n",
    ">2. Works well for small $K$, but slow for higher dimensions\n",
    ">3. Does not provide quick way to $y^{(new)} \\rightarrow x^{(new)}$\n",
    ">4. Does not proviee a way for $x^{(new)} \\rightarrow y^{(new)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. DR using Auto-Encoders\n",
    "\n",
    "* **Framework**\n",
    "\n",
    "><img src = 'images/image1_02.png' width=250>\n",
    "\n",
    ">* **Cost function**\n",
    "\n",
    ">$$\\underset{\\theta,\\phi}{\\text{argmin}} \\sum^N_{n=1} ||y^{(n)} - \\hat{y}^{(n)}||^2 + \\text{constraints}$$\n",
    "\n",
    ">* **Examples for Encoding/Decoding Fn.**\n",
    ">  * **Linear PCA:** use linear matrix multiplication - $\\Phi$ and $\\Theta$\n",
    ">  * **Deep Neural AE:** use DNN\n",
    "\n",
    "* **Objective:** to find **Interesting Embedding** (not identity mapping)\n",
    "\n",
    ">* **Achieve via constraint**\n",
    ">  * Dimensionality of $x$\n",
    ">  * Sparsity - only a subset of $x$ is non-zero\n",
    ">  * Function complexity\n",
    "\n",
    ">* **Achieve via data corruption**\n",
    ">  * Add noise in $y$\n",
    ">  * Drop-out\n",
    ">  * Transform(e.g. $y$: image from one angle $\\rightarrow$ $\\hat{y}$: from other angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. DR using Probabilistic Models\n",
    "\n",
    "* **Family of Models**\n",
    "\n",
    ">|Model Class|<img src = 'images/image1_03.png' width=67>|<img src = 'images/image1_04.png' width=210>|<img src = 'images/image1_05.png' width=210>|\n",
    "|-|-|-|-|\n",
    "|**Full Linear**|　　Factor Analysis (FA)<br/><br/>$p(x)=\\mathcal{G}(x;0,I)$<br/><br/>$p(y|x)=\\mathcal{G}(y;\\theta x,D)$|Inter-battery FA<br/><br/>$p(x),p(x_i)=\\mathcal{G}(x;0,I)$<br/><br/>$p(y_i|x)=\\mathcal{G}(y_i;\\theta^{\\text{sh}}_i x + \\theta^{\\text{pri}}_i x_i, D)$|LGSSM<br/><br/>$p(x_t|x_{t-1})=\\mathcal{G}(x_t;\\Psi x_{t-1},\\Sigma)$<br/><br/>$p(y|x)=\\mathcal{G}(y;\\theta x,D)$|\n",
    "|||||\n",
    "|**Special Linear**|PCA<br/><br/>$D=\\sigma^2 I$|Canonical Correlation Analysis<br/><br/>$D=\\sigma^2 I$|Slow Feature Analysis<br/><br/>$D=\\sigma^2 I \\;\\;,\\;\\; \\Sigma=1-\\Psi^2$<br/><br/>$\\Psi=\\text{diag}(\\psi_1,...,\\psi_K)$|\n",
    "|||||\n",
    "|**GP maps $y=f(x)$**|GP-LVM|Multi-view GP-LVM|GP-dynamical System|\n",
    "|||||\n",
    "|**Others**|e.g. ICA|Information Bottleneck<br/>style and content|e.g. GP-SSM|\n",
    "\n",
    "* **Probabilistic PCA**\n",
    "\n",
    ">* **Generative Model**\n",
    "\n",
    ">$$x_n \\text{ ~ } \\mathcal{N}(0,I) \\;\\;\\;,\\;\\;\\;\n",
    "y_n = {\\theta} x_n + \\sigma \\epsilon_n \\;\\;\\;,\\;\\;\\;\n",
    "\\epsilon_n \\text{ ~ } \\mathcal{N}(0,I)$$\n",
    "\n",
    ">\\begin{align}\n",
    "p(y_n) &= \\int p(x_n, y_n) dx_n = \\mathcal{N} (y_n|\\mathbf{\\mu}_y, \\Sigma_y) \\\\\n",
    "\\mathbf{\\mu}_y &= \\mathbb{E}_{p(x,\\mathbf{\\epsilon})} [\\theta x + \\sigma \\mathbf{\\epsilon}] = \\theta \\cdot 0 + 0 = 0 \\\\\n",
    "\\Sigma_y &= \\text{Cov}(y) = \\mathbb{E}(y y^T) = \\mathbb{E}_{p(x,\\epsilon)} [(\\Theta x + \\sigma \\epsilon)(\\Theta x + \\sigma \\epsilon)^T] = \\theta \\theta^T + \\sigma^2 I \\\\\n",
    "p(y_n|\\theta,\\sigma) &= \\mathcal{N} (y_n | 0, \\theta \\theta^T + \\sigma^2 I)\n",
    "\\end{align}\n",
    "\n",
    ">* **ML Solution**\n",
    "\n",
    ">\\begin{align}\n",
    "\\Sigma_y e_k &= \\lambda_k e_k \\;\\;\\;\\rightarrow\\;\\;\\; E_K = [e_1,...,e_K] \\text{ and } \\Lambda_K = \\text{diag}(\\lambda_1,...,\\lambda_K)\\\\\n",
    "\\sigma^{ML} &= \\frac{1}{D-K} \\sum^D_{k=K+1} \\lambda_k \\\\\n",
    "\\theta^{ML} &= E_K (\\Lambda_K - \\sigma^2 I)^{1/2}R \\;\\;\\;\\rightarrow\\;\\;\\; R: \\text{arbitrary rotation}\n",
    "\\end{align}\n",
    "\n",
    "* **GP-LVM**\n",
    "\n",
    ">* **Gaussian Process**\n",
    "\n",
    ">\\begin{align}\n",
    "p(f(x)|\\theta) &= \\mathcal{GP}(0,K(x,x')) \\\\\n",
    "K(x,x') &= \\sigma^2 \\exp \\left( -\\frac{1}{2l^2} (x-x')^2 \\right) \\\\\n",
    "p(y(x)|\\theta) &= \\mathcal{GP}(0,K(x,x')+\\sigma_y^2 I)\n",
    "\\end{align}\n",
    "\n",
    ">* **GP-LVM** (set $C(x,x')=\\sum_k x_k x_k'$ to recover PCA)\n",
    "\n",
    ">\\begin{align}\n",
    "p(f_d) &= \\mathcal{GP} (f;0,C(x,x'))\\\\\n",
    "p(x) &= \\mathcal{G} (x;0,I) \\\\\n",
    "p(y_d|x,f_d) &= \\mathcal{G}(y;f_d(x),\\sigma^2)\n",
    "\\end{align}\n",
    "\n",
    ">* **MAP Inference**\n",
    ">  * Optimise positions of data in latent space\n",
    ">  * No explicit mappings (ISOMAP-like)\n",
    "\n",
    ">\\begin{align}\n",
    "p(y|x) &= \\int p(y|x,f)p(f)df = \\mathcal{G}(y_{1:N};0,\\Sigma(x_{1:N})) \\\\\n",
    "\\\\\n",
    "x_{\\text{MAP}} &= \\underset{x}{\\text{argmax}} p(y|x)p(x) \\\\\n",
    "&= \\underset{x}{\\text{argmax}} \\log p(x) - \\frac{1}{2} \\log \\det \\Sigma (x_{1:N}) - \\frac{1}{2} \\text{trace} \\left( \\Sigma (x_{1:N})^{-1} y_{1:N} y_{1:N}^T \\right)\n",
    "\\end{align}\n",
    "\n",
    ">* **Back-constrained Inference**\n",
    ">  * Accelerates inference/learning via recognition model\n",
    ">  * $\\phi$ and $\\theta$ optimised using same objective\n",
    "\n",
    ">\\begin{align}\n",
    "x_{\\text{MAP}}(y_{1:N}) &\\approx g_\\phi (y_{1:N}) \\\\\n",
    "\\phi &= \\underset{\\phi}{\\text{argmax}} \\; p(x=g_\\phi(y_{1:N})|y,\\theta) = \\underset{\\phi}{\\text{argmax}} \\log p(x=g_\\phi(y_{1:N}),y|\\theta) \\\\\n",
    "\\theta &= \\underset{\\theta}{\\text{argmax}} \\log p(y|\\theta) \\approx \\underset{\\theta}{\\text{argmax}} \\log p(x=g_\\phi(y_{1:N}),y|\\theta)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Probabilistic Inference as an Auto-encoder\n",
    "\n",
    "* **Variational Inference**\n",
    "\n",
    ">* **Goal:** maximise the **approximate likelihood**\n",
    "\n",
    "><img src = 'images/image1_06.png' width=300>\n",
    "\n",
    ">\\begin{align}\n",
    "\\mathcal{L}(\\theta) &= \\log p(y|\\theta) = \\log \\int p(y,x|\\theta) dx = \\log \\int \\frac{q(x)}{q(x)} p(y,x|\\theta)dx \\\\\n",
    "&\\geq \\int q(x) \\log \\frac{p(y,x|\\theta)}{q(x)}  dx = \\mathcal{F}(q,\\theta) \\\\\n",
    "\\mathcal{F}(q,\\theta) &= \\underset{\\text{(reconstruction cost)}}{ \\int q(x) \\log p(y|x,\\theta)dx} + \\underset{\\text{(soft-constraint)}}{ \\int q(x) \\log \\frac{p(x|\\theta)}{q(x)}dx} \\\\\n",
    "&= - \\frac{1}{2\\sigma^2} \\langle ||y - f_\\theta(x)||^2 \\rangle_{q(x)} - \\frac{D}{2} \\log \\sigma^2 - \\text{KL}(q(x)||p(x))\n",
    "\\end{align}\n",
    "\n",
    ">* **Reconstruction Cost:** $f_\\theta(x)$ should be similar to $y$ over $q(x)$\n",
    ">* **Self-constraint:** $q(x)$ should be similar to $p(x)$\n",
    "\n",
    "* **Flavours of Variational Inference**\n",
    "\n",
    ">|Flavour|　　　　　　　　　　　　$q(x)$|　　　　　Parameter Learning|\n",
    "|-|-|-|\n",
    "|Fixed Family|$q(x)=\\mathcal{G}(x;\\mu_q,\\Sigma_q)$|$\\underset{\\theta,\\mu_q,\\Sigma_q}{\\text{argmax}} \\mathcal{F}(\\theta,\\mu_q,\\Sigma_q)$|\n",
    "|Structured|$q(x)=\\prod^K_{k=1} q_k(x_k)$|$\\underset{\\{q_k(x)\\}^K_{k=1},\\theta}{\\text{argmax}} \\mathcal{F}(\\{q_k(x)\\}^K_{k=1},\\theta)$|\n",
    "|Recognition Model<br/>(Variational Auto-encoder)|$q_\\phi(x)=\\mathcal{G}(x;\\mu_\\phi(y),\\Sigma_\\phi(y)$|$\\underset{\\phi,\\theta}{\\text{argmax}} \\mathcal{F}(\\phi,\\theta)$|\n",
    "|GP-LVM|$q_\\phi(x) = \\delta(x-g_\\phi(y))$||\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
