{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLMI1: Introduction to Machine Learning\n",
    "\n",
    "Lecturer: Dr. Richard Turner\n",
    "\n",
    "----\n",
    "\n",
    "# Table of Contents\n",
    "## 1. Introduction\n",
    "* 1.1. Radioactive Decay - Heuristic Approach\n",
    "* 1.2. Radioactive Decay - Probabilistic Approach\n",
    "* 1.3. Inference and Decision Making\n",
    "\n",
    "## 2. Regression\n",
    "* 2.1. Linear Regression\n",
    "* 2.2. Non-linear Regression\n",
    "* 2.3. Regularisation\n",
    "* 2.4. Bayesian Linear Regression\n",
    "\n",
    "## 3. Classification\n",
    "* 3.1. Binary Logistic Classification\n",
    "* 3.2. kNN Classification\n",
    "* 3.3. Multi-class Softmax Classification\n",
    "* 3.4. Overfitting in Classification\n",
    "* 3.5. Overfitting in Classification\n",
    "* 3.6. Bayesian Classification\n",
    "* 3.7. Bayesian Logistic Regression\n",
    "\n",
    "## 4. Clustering\n",
    "* 4.1. K-means Algorithm\n",
    "* 4.2. K-means as Optimisation\n",
    "* 4.3. K++ means\n",
    "\n",
    "## 5. EM Method\n",
    "* 5.1. Mixture of Gaussian: Generative Model\n",
    "* 5.2. KL Divergence\n",
    "* 5.3. EM Algorithm\n",
    "* 5.4. EM - Application to 1D data\n",
    "\n",
    "## 6. Sequence Modelling\n",
    "* 6.1. Markov Models\n",
    "* 6.2. N-gram Models (discrete data)\n",
    "* 6.3. AR Gaussian Models (continuous data)\n",
    "* 6.4. HMM (discrete hidden state)\n",
    "* 6.5. HMM (continuous hidden state)\n",
    "* 6.6. Kalman Filter\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Inference\n",
    "## 1.1. Radioactive Decay - Heuristic Approach\n",
    "\n",
    "* **Problem**\n",
    "\n",
    ">$$\\begin{align}\n",
    "p(x|\\lambda) = \\frac{1}{Z(\\lambda)} \\text{exp}\\bigg(-\\frac{x}{\\lambda}\\bigg)\n",
    "\\end{align} \\Rightarrow \\text{decay events in } (x_{min}, x_{max}) \\Rightarrow \\text{estimate } \\lambda$$\n",
    "\n",
    "* **Histogram-based**\n",
    "\n",
    ">* $C_x$: No. of events in $[x-w/2,x+w/2]$\n",
    ">\n",
    ">\\begin{align}\n",
    "\\mathbb{E}(C_x) &= N \\int^{x+w/2}_{x-w/2}p(x|\\lambda) dx\n",
    "~\\approx N w p(x|\\lambda) = \\frac{N w}{Z(\\lambda)} \\exp\\bigg(-\\frac{x}{\\lambda}\\bigg) \\\\\n",
    "\\log(\\mathbb{E}(C_{x})) &= -\\frac{x}{\\lambda} + \\text{const.} \\rightarrow \\text{find } \\lambda \\text{ from least squares}\n",
    "\\end{align}\n",
    "\n",
    ">* **Problems** $\\lambda$ depends on choice of bins / No uncertainty estimate / Why least squares? (this can be justified)\n",
    "\n",
    "* **Statistic-based (e.g. mean)**\n",
    "\n",
    ">* **Calculate mean**\n",
    "\n",
    ">\\begin{align}\n",
    "\\mu = \\mathbb{E}(x) = \\int x \\; p(x|\\lambda) \\; \\text{d}x = \\int x \\; \\frac{1}{Z(\\lambda)} \\text{exp}\\bigg(-\\frac{x}{\\lambda}\\bigg) \\; \\text{d}x\n",
    "\\end{align}\n",
    "\n",
    ">* **Calculate $Z(\\lambda)$**\n",
    "\n",
    ">\\begin{align}\n",
    "Z(\\lambda) = \\int^{x_{\\text{min}}}_{x_{\\text{max}}} \\exp(-x/\\lambda) \\; \\text{d}x = \\lambda \\left [ \\exp(-x_{\\text{min}}/\\lambda) - \\exp(-x_{\\text{max}}/\\lambda) \\right]\n",
    "\\end{align}\n",
    "\n",
    ">* **Apply it to $\\mu$**\n",
    "\n",
    ">$$\n",
    "\\frac{d}{d(1/\\lambda)} \\left( \\int^{x_{min}}_{x_{max}} \\text{exp}(-x/\\lambda) dx \\right)= \\int^{x_{min}}_{x_{max}} \\frac{\\partial}{\\partial (1/\\lambda)} \\text{exp}(-x/\\lambda) dx = -x \\int^{x_{min}}_{x_{max}} \\text{exp}(-x/\\lambda) dx\n",
    "$$\n",
    ">\n",
    ">$$\n",
    "\\begin{align}\n",
    "\\mu  &= - \\frac{1}{Z(\\lambda)} \\frac{d}{d(1/\\lambda)} \\int^{x_{\\text{min}}}_{x_{\\text{max}}} \\exp(-x/\\lambda) \\; dx = - \\frac{d}{d(1/\\lambda)} \\log Z(\\lambda) \\\\\n",
    "&= \\lambda +  \\frac{  \n",
    "x_{\\text{min}} \\exp(-x_{\\text{min}}/\\lambda) - x_{\\text{max}} \\exp(-x_{\\text{max}}/\\lambda) \n",
    "}{\\exp(-x_{\\text{min}}/\\lambda) - \\exp(-x_{\\text{max}}/\\lambda)} \\rightarrow \\text{find } \\lambda\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    ">* **Sanity Check**\n",
    ">  * **1.** $\\lambda \\rightarrow 0 \\;\\; \\Rightarrow \\;\\; \\mu \\rightarrow x_{min}$\n",
    ">  * **2.** $\\lambda \\rightarrow \\infty \\;\\; \\Rightarrow \\;\\; \\mu \\rightarrow (x_{max}+x_{min})/2$\n",
    ">  * **3.** $x_{min}=0 \\; \\text{and} \\; x_{max} \\rightarrow \\infty \\;\\; \\Rightarrow \\;\\; Z(\\lambda) \\rightarrow \\lambda$\n",
    "\n",
    ">* **Problems** Why $\\mathbb{E}(x)$? / What if $\\hat{\\mu}$ is higher than $(x_{\\text{max}} - x_{\\text{min}} )/2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Radioactive Decay - Probabilistic Approach\n",
    "\n",
    "* **Bayes' Rule**\n",
    "\n",
    ">$$p(\\lambda | \\mathcal{D}) = \\frac{p(\\lambda) p(\\mathcal{D} | \\lambda )}{p(\\mathcal{D})} \\propto p(\\lambda) \\prod_{n=1}^N p( x_n | \\lambda )$$\n",
    "\n",
    "* **Likelihood** (Mean: Sufficient Statistics)\n",
    "\n",
    ">$$p(\\{ x_n \\}_{n=1}^N | \\lambda) = \\frac{1}{Z(\\lambda)^N} \\exp\\left(-\\frac{1}{\\lambda} \\sum_{n = 1}^{N} x_n \\right)\n",
    "$$\n",
    "\n",
    ">* Small $\\lambda$: inconsistent with the observation\n",
    ">* Large $\\lambda$: flat likelihood\n",
    "\n",
    "\n",
    "* **Prior** (assume uniform)\n",
    "\n",
    ">$$\\begin{align}\n",
    "p(\\lambda) = \\mathcal{U}(\\lambda; 0, 100)\n",
    "\\end{align}$$\n",
    "\n",
    "* **Posterior Distribution**\n",
    "\n",
    ">* MAP (Maximum a Posteriori): mode\n",
    ">* Uncertainty: RMS variation around the MAP value\n",
    "\n",
    "* **Posterior Predictive** (less confident than the MAP predictive)\n",
    "\n",
    ">$$p(x^\\star \\lvert \\{x_n\\}_{n=1}^N) = \\int p(x^\\star \\lvert  \\lambda) p(\\lambda | \\{x_n\\}_{n=1}^N) \\text{d} \\lambda$$\n",
    "\n",
    "* **MLE** (alternative to full Bayesian)\n",
    "\n",
    ">$$\\lambda_{\\text{ML}} = \\underset{\\lambda}{\\mathrm{arg\\min}} \\;\\; p(\\{ x_n \\}_{n=1}^N | \\lambda ) = \\underset{\\lambda}{\\mathrm{arg\\min}} \\;\\;\\prod_{n=1}^N p( x_n | \\lambda )$$\n",
    "\n",
    ">* Comparison to MAP (uniform prior)\n",
    "\n",
    ">$$\\lambda_{\\text{MAP}} = \\underset{\\lambda<\\lambda_{\\text{max}}}{\\mathrm{arg\\min}} \\;\\; p(\\lambda) p(\\{ x_n \\}_{n=1}^N | \\lambda )  = \\underset{\\lambda<\\lambda_{\\text{max}}}{\\mathrm{arg\\min}} \\;\\; \\frac{1}{\\lambda_{\\text{max}}} \\prod_{n=1}^N p( x_n | \\lambda ) $$\n",
    "\n",
    ">* MLE is recovered in the limit $\\lambda_{\\text{max}} \\rightarrow \\infty$\n",
    "\n",
    "* **Alternative**\n",
    "\n",
    ">* Draw samples $\\lambda \\sim p(\\lambda | \\{ x_n\\}_{n=1}^N)$\n",
    ">* Typical values of the decay constant that are consistent with the observed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Inference and Decision Making\n",
    "\n",
    "* **Medical Diagnosis** (a: disease / b: test result)\n",
    "\n",
    ">$$p(b=1|a=1)=0.95 \\;\\;\\;,\\;\\;\\; p(b=0|a=0)=0.95 \\;\\;\\;,\\;\\;\\; p(a=1)=0.05$$\n",
    "\n",
    ">\\begin{align}\n",
    "p(a=1|b=1)&=\\frac{p(b=1|a=1)p(a=1)}{p(b=1)} \\\\\n",
    "&=\\frac{p(b=1|a=1)p(a=1)}{p(b=1|a=1)p(a=1)+p(b=1|a=2)p(a=2)} = \\frac{1}{2}\n",
    "\\end{align}\n",
    "\n",
    "* **Medical Treatment**\n",
    "\n",
    ">* **Reward:**\n",
    "\n",
    ">$$\n",
    "\\begin{bmatrix}\n",
    "R(a = 0, t = 0) & R(a = 0, t = 1) \\\\\n",
    "R(a = 1, t = 0) &R(a = 1, t = 1) \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "10 & 7\\\\\n",
    "3 &5\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    ">* **Conditional Reward:**\n",
    "\n",
    ">$$R(t)=\\sum_a R(a,t)p(a|b=1)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
