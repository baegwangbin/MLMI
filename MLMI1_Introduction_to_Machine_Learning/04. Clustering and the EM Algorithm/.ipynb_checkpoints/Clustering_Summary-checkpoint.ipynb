{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clustering\n",
    "\n",
    "## 4.1. K-means Algorithm\n",
    "\n",
    "* **Algorithm**\n",
    "\n",
    ">* **Step 1:** Initialise: $\\boldsymbol{m}_k \\in \\mathbb{R}^D$\n",
    ">* **Step 2:** $s_n={\\text{argmin}_k} ||\\boldsymbol{x}_n - \\boldsymbol{m}_k|| $\n",
    ">* **Step 3:** $\\boldsymbol{m}_k = \\text{mean}(\\boldsymbol{x}_n : s_n=k)$\n",
    ">* Continue until $s_n$ converge\n",
    "\n",
    "## 4.2. K-means as Optimisation\n",
    "\n",
    "* **Cost fn.** (Lyupanov function - converges but hard to find **global optimum**)\n",
    "\n",
    ">$$\\mathcal{C}(\\{s_{n,k}\\},\\{\\boldsymbol{m}_k\\})=\\sum^N_{n=1} \\sum^K_{k=1} s_{n,k} \\;||\\boldsymbol{x}_n-\\boldsymbol{m}_k||^2$$\n",
    "\n",
    ">$$\\text{where}\\;\\;\\;\\sum^K_{k=1} s_{n,k} = 1 \\;\\;\\;\\text{and}\\;\\;\\; s_{n,k}\\in\\{0,1\\}$$\n",
    "\n",
    "* **K-means as Optimisation**\n",
    "\n",
    ">1. Minimise $\\mathcal{C}$ w.r.t. $\\{s_{n,k}\\}$, holding $\\{\\boldsymbol{m}_k\\}$ fixed\n",
    ">2. Minimise $\\mathcal{C}$ w.r.t. $\\{\\boldsymbol{m}_k\\}$, holding $\\{s_{n,k}\\}$ fixed\n",
    "\n",
    "## 4.3. K++ means\n",
    "\n",
    "* **Algorithm to select initial centroids** (set $M$: store centroid)\n",
    "\n",
    ">1. Randomly select $\\mu_0$, and put it in $M$\n",
    ">2. For $x_i \\notin M$, calculate $d(M, x_i)$ ($\\mu_k$ that minimizes $d(\\mu_k, x_i)$ is the class of $x_i$)\n",
    ">3. Select the next $\\mu$, based on pmf proportional to $d(M, x_i)$\n",
    ">4. Repeat until $K$ centroids are selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "b868be8d070d43819ab213fde0a8c2c8"
   },
   "source": [
    "# 5. EM Algorithm\n",
    "\n",
    "## 5.1. Mixture of Gaussian: Generative Model\n",
    "\n",
    "* **Sample cluster membership $\\rightarrow$ Sample data-value**\n",
    "\n",
    ">$$p(s_n=k|\\theta)=\\pi_k \\;\\;\\;\\text{where}\\;\\;\\; \\sum^K_{k=1}\\pi_k=1$$\n",
    "\n",
    ">$$p(\\mathbf{x}_n|s_n=k,\\theta)=\\mathcal{N}(\\mathbf{x}_n;\\mathbf{m}_k,\\Sigma_k)$$\n",
    "\n",
    "## 5.2. KL Divergence\n",
    "\n",
    ">$$\\mathcal{KL}(p_1(z)||p_2(z))=\\sum_z p_1(z)\\log{\\frac{p_1(z)}{p_2(z)}}$$\n",
    "\n",
    ">* **Gibb's Inequality:** $\\mathcal{KL}(p_1(z)||p_2(z))\\geq0\\;\\;\\;\\text{equality at } p_1(z)=p_2(z)$\n",
    ">* **Non-Symmetric:** $\\mathcal{KL}(p_1(z)||p_2(z))\\neq\\mathcal{KL}(p_2(z)||p_1(z))$\n",
    "\n",
    "## 5.3. EM Algorithm\n",
    "\n",
    "* **Free Energy**: Lower bound on LL\n",
    "\n",
    ">$$\\mathcal{F}(q(s),\\theta)=\\log p(x|\\theta) - \\sum_s q(s)\\log{\\frac{q(s)}{p(s|x,\\theta)}}$$\n",
    "\n",
    "* **E Step:** For fixed $\\theta{t-1}$, maximise lower bound $\\mathcal{F}(q(s),\\theta_{t-1})$ wrt $q(s)$\n",
    "\n",
    ">$$q_t(s)=p(s|x,\\theta_{t-1})$$\n",
    "\n",
    "* **M Step:** For fixed $q_t(s)$, maximise lower bound $\\mathcal{F}(q_t(s),\\theta)$ wrt $\\theta$\n",
    "\n",
    ">$$\\mathcal{F}(q(s),\\theta)=\\sum_s q(s)\\log(p(x|s,\\theta)p(s|\\theta))-\\sum_s q(s)\\log q(s)$$\n",
    "\n",
    ">$$\\theta_t=\\underset{\\theta}{\\text{argmax}}\\sum_s q_t(s) \\log (p(x|s,\\theta)p(s|\\theta)) $$\n",
    "\n",
    "* **LL cannot decrease**\n",
    "\n",
    "><img src=\"images\\image04.png\" width=550>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. EM - Application to 1D data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Probability of the observations given the latent variables and the parameters:**\n",
    "\n",
    ">$$p(x_n|s_n=k,\\theta)=\\frac{1}{\\sqrt{2\\pi\\sigma^2_k}}\\exp\\left(-\\frac{1}{2\\sigma_k^2}(x_n-\\mu_k)^2\\right)$$\n",
    "\n",
    "* **Prior on latent variables**\n",
    "\n",
    ">$$p(s_n=k)=\\pi_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **E Step: fills in the values of the hidden variables** \n",
    "\n",
    ">\\begin{align}\n",
    "q(s_n=k)=p(s_n=k|x_n,\\theta)&\\propto p(x_n,s_n=k|\\theta)\\\\\n",
    "&=\\frac{\\pi_k}{\\sqrt{2\\pi\\sigma^2_k}}\\exp\\left(-\\frac{1}{2\\sigma_k^2}(x_n-\\mu_k)^2\\right)=u_{nk}\n",
    "\\end{align}\n",
    "\n",
    ">$$q(s_n=k)=r_{nk}=\\frac{u_{nk}}{u_n} \\;\\;\\;\\text{where}\\;\\;\\; u_n=\\sum^K_{k=1}u_{nk}$$\n",
    "\n",
    ">* $r_{nk}$: ***Responsibility*** that component $k$ takes for datapoint $n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **M Step: performs supervised learning with known (soft) cluster assignments**\n",
    "\n",
    ">$$\\mathcal{F}(q(s),\\theta)=\\sum^N_{n=1} \\sum^K_{k=1} q(s_n=k) \\left[ \\log(\\pi_k)-\\frac{1}{2\\sigma_k^2}(x_n-\\mu_k)^2-\\frac{1}{2}\\log(\\sigma_k^2) \\right] + \\text{const.}$$\n",
    "\n",
    "><img src=\"images\\image05.png\" width=550>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
