{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Inference\n",
    "## 1.1. Radioactive Decay - Heuristic Approach\n",
    "\n",
    "* **Problem**\n",
    "\n",
    ">$$p(x|\\lambda) = \\frac{1}{Z(\\lambda)} \\text{exp}\\bigg(-\\frac{x}{\\lambda}\\bigg)$$\n",
    "\n",
    "* **Histogram-based**\n",
    "\n",
    ">\\begin{align}\n",
    "\\mathbb{E}(C_x) &= N \\int^{x+w/2}_{x-w/2}p(x|\\lambda) dx\n",
    "~\\approx N w p(x|\\lambda) = \\frac{N w}{Z(\\lambda)} \\exp\\bigg(-\\frac{x}{\\lambda}\\bigg) \\\\\n",
    "\\log(\\mathbb{E}(C_{x})) &= -\\frac{x}{\\lambda} + \\text{const.} \\rightarrow \\text{find } \\lambda \\text{ from least squares}\n",
    "\\end{align}\n",
    "\n",
    "* **Statistic-based (e.g. mean)**\n",
    "\n",
    ">\\begin{align}\n",
    "Z(\\lambda) = \\int^{x_{\\text{max}}}_{x_{\\text{min}}} \\exp(-x/\\lambda) \\; \\text{d}x = \\lambda \\left [ \\exp(-x_{\\text{min}}/\\lambda) - \\exp(-x_{\\text{max}}/\\lambda) \\right]\n",
    "\\end{align}\n",
    "\n",
    ">$$\n",
    "\\mu  = - \\frac{d}{d(1/\\lambda)} \\log Z(\\lambda) = \\lambda +  \\frac{  \n",
    "x_{\\text{min}} \\exp(-x_{\\text{min}}/\\lambda) - x_{\\text{max}} \\exp(-x_{\\text{max}}/\\lambda) \n",
    "}{\\exp(-x_{\\text{min}}/\\lambda) - \\exp(-x_{\\text{max}}/\\lambda)}\n",
    "$$\n",
    "\n",
    "## 1.2. Radioactive Decay - Probabilistic Approach\n",
    "\n",
    "* **Likelihood** (Mean: Sufficient Statistics)\n",
    "\n",
    ">$$p(\\{ x_n \\}_{n=1}^N | \\lambda) = \\frac{1}{Z(\\lambda)^N} \\exp\\left(-\\frac{1}{\\lambda} \\sum_{n = 1}^{N} x_n \\right)\n",
    "$$\n",
    "\n",
    "* **Posterior Predictive** (less confident than the MAP predictive)\n",
    "\n",
    ">$$p(x^\\star \\lvert \\{x_n\\}_{n=1}^N) = \\int p(x^\\star \\lvert  \\lambda) p(\\lambda | \\{x_n\\}_{n=1}^N) \\text{d} \\lambda$$\n",
    "\n",
    "* **MLE & MAP**\n",
    "\n",
    ">$$\\lambda_{\\text{ML}} = \\underset{\\lambda}{\\mathrm{arg\\max}} \\;\\; p(\\{ x_n \\}_{n=1}^N | \\lambda ) = \\underset{\\lambda}{\\mathrm{arg\\max}} \\;\\;\\prod_{n=1}^N p( x_n | \\lambda )$$\n",
    "\n",
    ">$$\\lambda_{\\text{MAP}} = \\underset{\\lambda<\\lambda_{\\text{max}}}{\\mathrm{arg\\max}} \\;\\; p(\\lambda) p(\\{ x_n \\}_{n=1}^N | \\lambda )  = \\underset{\\lambda<\\lambda_{\\text{max}}}{\\mathrm{arg\\max}} \\;\\; \\frac{1}{\\lambda_{\\text{max}}} \\prod_{n=1}^N p( x_n | \\lambda ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regression\n",
    "\n",
    "## 2.1. Linear Regression\n",
    "\n",
    "* **Least Squares Fit - Cost fn.**\n",
    "\n",
    ">$$C_2 = \\big|\\big|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\big|\\big|^2 = \\big(\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\big)^\\top \\big(\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\big)$$\n",
    "\n",
    "* **Solution** (has no uncertainty)\n",
    "\n",
    ">\\begin{align}\n",
    "\\frac{\\partial C_2}{\\partial \\mathbf{w}} &= -2\\mathbf{X}^\\top\\big(\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\big)=0\\\\\n",
    "\\implies &\\boxed{\\mathbf{w} = \\big( \\mathbf{X}^\\top\\mathbf{X}\\big)^{-1}\\mathbf{X}^\\top \\mathbf{y}}\n",
    "\\end{align}\n",
    "\n",
    "* **MLE Fit** ($\\mathbf{y} = \\mathbf{Xw}+\\epsilon_n$)\n",
    "\n",
    ">\\\\[p(\\mathbf{y}\\mid\\mathbf{X}, \\mathbf{w}, \\sigma_y^2) = \\frac{1}{(2\\pi \\sigma_y^2)^{N/2}}\\text{exp}\\left(-\\frac{1}{2\\sigma_y^2}(\\mathbf{y} - \\mathbf{X}\\mathbf{w})^\\top (\\mathbf{y} - \\mathbf{X}\\mathbf{w})\\right)\\\\]\n",
    "\n",
    ">\\\\[-\\mathcal{L}(\\mathbf{w}) = \\frac{N}{2}\\log(2\\pi \\sigma_y^2) +\\frac{1}{2\\sigma_y^2}(\\mathbf{y} - \\mathbf{X}\\mathbf{w})^\\top (\\mathbf{y} - \\mathbf{X}\\mathbf{w})\\\\]\n",
    "\n",
    ">\\\\[\\boxed{\\text{Least squares} \\equiv \\text{minimize}~ (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^\\top (\\mathbf{y} - \\mathbf{X}\\mathbf{w}) \\Leftrightarrow \\text{Maximum-likelihood}}\\\\]\n",
    "\n",
    "## 2.2. Non-linear Regression\n",
    "\n",
    "* **MLE Fit**\n",
    "\n",
    ">\\\\[y_n = w_0 + w_1 \\phi_{1}(x_n) + w_2 \\phi_{2}(x_n) + ... w_D \\phi_{D}(x_n) + \\epsilon_n = \\boldsymbol{\\phi}(x_n)^\\top \\mathbf{w} + \\epsilon_n.\\\\]\n",
    "\n",
    ">$$ \\Rightarrow \\mathbf{y} = \\boldsymbol{\\Phi}\\mathbf{w} + \\boldsymbol{\\epsilon}$$\n",
    "\n",
    ">\\begin{equation}\n",
    "\\text{design matrix: }\\boldsymbol{\\Phi} =  \\begin{pmatrix}\n",
    "1 & \\phi_1(x_1) & \\cdots & \\phi_D(x_1)\\\\\\\n",
    "1 & \\phi_1(x_2) & \\cdots & \\phi_D(x_2)\\\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\\\n",
    "1 & \\phi_1(x_N) & \\cdots & \\phi_D(x_N)\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    ">\\\\[C_2 = \\big|\\big| \\mathbf{y} - \\boldsymbol{\\Phi}\\mathbf{w}\\big|\\big|^2 = \\big(\\mathbf{y} - \\boldsymbol{\\Phi}\\mathbf{w}\\big)^\\top \\big(\\mathbf{y} - \\boldsymbol{\\Phi}\\mathbf{w}\\big)\\\\]\n",
    "\n",
    ">\\\\[- \\mathcal{L}(\\mathbf{w}) = - \\text{log}~ p(\\mathbf{y}|\\boldsymbol{\\Phi}, \\mathbf{w}, \\sigma_y^2) = \\frac{N}{2}\\text{log}(2\\pi \\sigma^2) + \\frac{1}{2\\sigma^2}(\\mathbf{y} - \\boldsymbol{\\Phi}\\mathbf{w})^\\top (\\mathbf{y} - \\boldsymbol{\\Phi}\\mathbf{w})\\\\]\n",
    "\n",
    ">\\begin{align}\n",
    "\\boxed{\\mathbf{w} = \\big( \\boldsymbol{\\Phi}^\\top\\boldsymbol{\\Phi}\\big)^{-1}\\boldsymbol{\\Phi}^\\top \\mathbf{y}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Regularisation\n",
    "\n",
    "* **$Lp$ regularisation**: $||\\mathbf{w}||^p$\n",
    "\n",
    ">\\\\[C_2^{(\\text{reg})} = \\big|\\big|\\mathbf{y} - \\boldsymbol{\\Phi}\\mathbf{w}\\big|\\big|^2 +\\frac{\\alpha}{2}\\mathbf{w}^\\top\\mathbf{w} \\\\]\n",
    "\n",
    "* **MAP Fit** - Gaussian prior: $p(\\mathbf{w}|\\sigma_\\mathbf{w}^2)=\\mathcal{N}(\\mathbf{w};\\mathbf{0},\\sigma^2_\\mathbf{w}\\mathbf{I})$\n",
    "\n",
    ">\\begin{align}\n",
    "\\mathbf{w}^{\\text{MAP}} & = \\underset{\\mathbf{w}}{\\mathrm{arg\\,max}} \\; p(\\mathbf{w} | \\{x_n,y_n\\}_{n=1}^N,\\sigma_y^2,\\sigma_{\\mathbf{w}}^2)\\\\\n",
    "& = \\underset{\\mathbf{w}}{\\mathrm{arg\\,max}} \\frac{1}{(2\\pi \\sigma_\\mathbf{w}^2)}\\text{exp}\\big(-\\frac{1}{2\\sigma_\\mathbf{w}^2}\\mathbf{w}^\\top \\mathbf{w} \\big) \\times \\frac{1}{(2\\pi \\sigma_y^2)^{N/2}}\\text{exp}\\big(-\\frac{1}{2\\sigma_y^2}(\\mathbf{y} - \\mathbf{X}\\mathbf{w})^\\top (\\mathbf{y} - \\mathbf{X}\\mathbf{w})\\big) \\\\\n",
    "& = \\underset{\\mathbf{w}}{\\mathrm{arg\\,min}} \\;   (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^\\top (\\mathbf{y} - \\mathbf{X}\\mathbf{w}) - \\alpha \\mathbf{w}^\\top \\mathbf{w}  \\;\\; \\text{where} \\;\\;\\alpha = \\frac{\\sigma_y^2}{\\sigma_\\mathbf{w}^2}\n",
    "\\end{align}\n",
    "\n",
    "* **Optimisation**\n",
    "\n",
    ">\\begin{align}\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial \\mathbf{w}} &= - \\boldsymbol{\\Phi}^\\top(\\mathbf{y} - \\boldsymbol{\\Phi}\\mathbf{w}) + \\alpha\\mathbf{w} \\\\\n",
    "&= -\\boldsymbol{\\Phi}^\\top\\mathbf{y} + \\boldsymbol{\\Phi}^\\top\\boldsymbol{\\Phi}\\mathbf{w} + \\alpha \\mathbf{I}\\mathbf{w} = 0 \\\\\n",
    "\\end{align}\n",
    "\n",
    ">$$\\implies \\boxed{\\mathbf{w} = (\\boldsymbol{\\Phi}^\\top\\boldsymbol{\\Phi} + \\alpha\\mathbf{I})^{-1}\\boldsymbol{\\Phi}^\\top\\mathbf{y}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Bayesian Linear Regression\n",
    "* **Inference**\n",
    "\n",
    ">* **Prior and Likelihood**\n",
    "\n",
    ">$$\\begin{align}\n",
    "p(\\mathbf{w}| \\sigma_{\\mathbf{w}}^2) &= \\frac{1}{(2\\pi \\sigma_{\\mathbf{w}}^2)^{D/2}}\\text{exp}\\big(-\\frac{1}{2\\sigma_w^2}\\mathbf{w}^\\top \\mathbf{w}\\big)\\\\\n",
    "p(\\mathbf{y}|\\mathbf{X}, \\mathbf{w}, \\sigma_y^2) &= \\frac{1}{(2\\pi \\sigma_y^2)^{N/2}}\\text{exp}\\left(-\\frac{1}{2\\sigma_y^2}(\\mathbf{y} - \\boldsymbol{\\Phi}\\mathbf{w})^\\top (\\mathbf{y} - \\boldsymbol{\\Phi}\\mathbf{w})\\right)\n",
    "\\end{align}$$\n",
    "\n",
    ">* **Posterior Distribution**\n",
    "\n",
    ">\\begin{align}\n",
    "p(\\mathbf{w}|\\mathbf{y}, \\mathbf{X}, \\sigma_{\\mathbf{w}}^2, \\sigma_{y}^2) = \\mathcal{N}(\\mathbf{w}; \\mathbf{\\mu}_{\\mathbf{w} | \\mathbf{y}, \\mathbf{X} },\\Sigma_{\\mathbf{w} | \\mathbf{y}, \\mathbf{X} }).\n",
    "\\end{align} \n",
    ">\n",
    ">\\begin{align}\n",
    "\\Sigma_{\\mathbf{w} | \\mathbf{y}, \\mathbf{X} } & = \\left( \\frac{1}{\\sigma_y^2} \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Phi} + \\frac{1}{\\sigma_{\\mathbf{w}}^2} \\mathrm{I} \\right)^{-1} \\;\\;\\; \\text{and} \\;\\;\\;\n",
    "\\mathbf{\\mu}_{\\mathbf{w} | \\mathbf{y}, \\mathbf{X} } =  \\Sigma_{\\mathbf{w} | \\mathbf{y}, \\mathbf{X} } \\frac{1}{\\sigma_y^2}  \\boldsymbol{\\Phi}^\\top \\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    ">* **MAP Setting:** Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification\n",
    "\n",
    "## 3.1. Binary Logistic Classification\n",
    "\n",
    "* **Algorithm**\n",
    "\n",
    ">$$p(y_n = 1 | \\mathbf{x}_n, \\mathbf{w}) = \\sigma(a_n)  = \\frac{1}{1 + \\text{exp}({-\\mathbf{w}_n^\\top \\mathbf{x}})}$$\n",
    "\n",
    "* **Optimisation Method**\n",
    "\n",
    ">$$\n",
    "p(\\{ y_n \\}_{n=1}^N|\\{\\mathbf{x}_n\\}_{n=1}^N, \\mathbf{w}) = \\prod^N_{n = 1} \\sigma(\\mathbf{w}^\\top\\mathbf{x}_n)^{y_n} \\big(1 - \\sigma(\\mathbf{w}^\\top\\mathbf{x}_n)\\big)^{1-y_n}\n",
    "$$\n",
    "\n",
    ">$$\n",
    "\\mathcal{L}(\\mathbf{w}) =\\text{log}~p(\\{ y_n \\}_{n=1}^N|\\{\\mathbf{x}_n\\}_{n=1}^N, \\mathbf{w}) = \\sum^N_{n = 1} \\left[ y_n\\text{log}~\\sigma(\\mathbf{w}^\\top\\mathbf{x}_n)+(1-y_n)\\text{log}~\\big(1 - \\sigma(\\mathbf{w}^\\top\\mathbf{x}_n)\\big) \\right]\n",
    "$$\n",
    "\n",
    ">$$\\frac{\\partial \\mathcal{L}(\\mathbf{w})}{\\partial \\mathbf{w}} = \\sum^N_{n = 1} \\big(y_n - \\sigma(\\mathbf{w}^\\top\\mathbf{x}_n)\\big)\\mathbf{x}_n \n",
    "\\;\\;\\;\\Rightarrow\\;\\;\\;\n",
    "\\mathbf{w}_{i+1} = \\mathbf{w}_{i} + \\eta \\frac{\\partial \\mathcal{L}(\\mathbf{w})}{\\partial \\mathbf{w}}\\bigg|_{\\mathbf{w}_{i}}$$\n",
    "\n",
    "## 3.2. Multi-class Softmax Classification\n",
    "\n",
    "* **Compute $k$ activations, using $\\mathbf{w}$ of each class**\n",
    "* **Prob. contours: not linear / Decision boundaries: linear**\n",
    "* **Algorithm**\n",
    "\n",
    ">$$\n",
    "p(y_{n} = k |\\mathbf{x}_n, \\{\\mathbf{w}_k\\}_{k=1}^K) = \\frac{\\exp(a_{n,k})}{\\sum_{k'=1}^K \\text{exp}(a_{n,k'})} = \\frac{\\text{exp}(\\mathbf{w}_k^\\top \\mathbf{x}_n)}{\\sum_{k'=1}^K \\exp(\\mathbf{w}_{k'}^\\top \\mathbf{x}_n)}\n",
    "$$\n",
    "\n",
    "* **MLE** (one hot encoding)\n",
    "\n",
    ">\\begin{align}\n",
    "p(\\{y_{n}\\}_{n=1}^N|\\{\\mathbf{x}_n\\}_{n=1}^N, \\{\\mathbf{w}_k\\}_{k=1}^K) &= \\prod_{n = 1}^N \\prod_{k = 1}^K s_{n,k}^{y_{n,k}}\n",
    "\\end{align}\n",
    ">\n",
    ">$$\\mathcal{L}(\\{\\mathbf{w}\\}_{k=1}^K) = \\sum_{n = 1}^N \\sum_{k = 1}^K y_{n,k} \\log s_{n,k}\n",
    "\\;\\;\\;\\Rightarrow\\;\\;\\;\n",
    "\\frac{\\partial \\mathcal{L}(\\{\\mathbf{w}\\}_{k=1}^K)}{\\partial \\mathbf{w}_j} = \\sum^N_{n = 1} (y_{n,j} - s_{n,j}) \\mathbf{x}_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Non-linear Classification\n",
    "\n",
    "* **Non-linear binary logistic classification**\n",
    "\n",
    ">$$ a_n = w_0 + w_1 \\phi_{1}(\\mathbf{x}_n) + w_2 \\phi_{2}(\\mathbf{x}_n) + ... w_D \\phi_{D}(\\mathbf{x}_n) = \\mathbf{w}^\\top \\boldsymbol{\\Phi}(\\mathbf{x}_n) $$\n",
    "\n",
    ">\\begin{equation}\n",
    "\\boldsymbol{\\Phi} =  \\begin{pmatrix}\n",
    "1 & \\phi_1(x_1) & \\cdots & \\phi_D(x_1)\\\\\\\n",
    "1 & \\phi_1(x_2) & \\cdots & \\phi_D(x_2)\\\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\\\n",
    "1 & \\phi_1(x_N) & \\cdots & \\phi_D(x_N)\\\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    ">\\begin{align}\n",
    "p(y_n = 1 | \\mathbf{x}_n, \\mathbf{w}) = \\sigma(\\mathbf{w}^\\top \\boldsymbol{\\Phi}(\\mathbf{x}_n))\n",
    "\\end{align}\n",
    "\n",
    "* **Example:** **isotropic Gaussian basis fn.** a.k.a. **radial basis fn.**\n",
    "\n",
    ">$$\\phi_{d}(\\mathbf{x}) = \\exp \\left( -\\frac{1}{2 l^2} | \\mathbf{x} - \\mu_{d}|^2 \\right)$$\n",
    "\n",
    "## 3.4. Bayesian Classification\n",
    "\n",
    "* **Taylor Expansion of $\\log p(z)$ ($z_0$: mode)**\n",
    "\n",
    ">\\begin{align}\n",
    "\\text{log}~p(z) \\approx \\text{log}~p(z_0) + \\frac{1}{2}(z - z_0)^2\\frac{d^2}{dz^2}\\text{log}~p(z)\n",
    "\\end{align}\n",
    "\n",
    "* **Laplace approximation of $p(z)$**\n",
    "\n",
    ">\\begin{align}\n",
    "\\text{log}~\\mathcal{N}(z; z_0, \\sigma^2) = \\text{const. } - \\frac{1}{2\\sigma^2}(z - z_0)^2\n",
    "\\end{align}\n",
    "\n",
    ">$$\\frac{1}{\\sigma^2} = - \\frac{d^2}{dz^2}\\text{log}~p(z)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
