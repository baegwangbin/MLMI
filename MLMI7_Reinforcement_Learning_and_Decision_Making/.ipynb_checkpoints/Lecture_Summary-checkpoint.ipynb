{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLMI7: Reinforcement Learning and Decision Making\n",
    "\n",
    "Lecturer: Dr. José Miguel Hernández-Lobato & Prof. Carl Edward Rasmussen\n",
    "\n",
    "----\n",
    "\n",
    "# Table of Contents\n",
    ">## 1. Introduction\n",
    "* 1.1. What is Reinforcement Learning?\n",
    "* 1.2. Markov Property\n",
    "* 1.3. Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. What is Reinforcement Learning?\n",
    "\n",
    "* **Agent-Environment Interface**\n",
    "\n",
    "><img src='images/image01.png' width=400>\n",
    "\n",
    ">* **Goal: (1)** For a given state, **(2)** make actions to **(3)** maximise the numerical reward\n",
    "\n",
    "* **Tasks & Rewards**\n",
    "\n",
    ">* **Episodic tasks:** interaction terminates after finite no. of steps\n",
    "\n",
    ">$$R_t = r_{t+1} + \\cdots + r_T$$\n",
    "\n",
    ">* **Continuing tasks:** interaction has no limit (e.g. trading, driving)\n",
    ">  * $\\gamma$: discount factor / $\\gamma=0$: the agent is **miopic** / $\\gamma \\rightarrow 1$: the agent is **farsighted**\n",
    "\n",
    ">$$R_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots$$\n",
    "\n",
    "\n",
    "* **Main Elements in RL**\n",
    "\n",
    ">|Element|Explanation|\n",
    "|-|-|\n",
    "|**Policy**|defines behaviour of the agent, $\\pi_t (a|s) = p(a_t=a | s_t=s)$|\n",
    "|**Reward**|defines the goal of the agent (to maximise $E[$cumulative reward$]$|\n",
    "|**Value Function**|defines what is the goal in the long run (prediction of future reward)|\n",
    "|**Model**|explains how the environment behaves (e.g. state transition prob.)|\n",
    "\n",
    "* **Exploration and exploitation dilemma**\n",
    "\n",
    ">* The agent must simultaneously **exploit** current knowledge and **explore** new actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Markov Property\n",
    "\n",
    "* **Markov Property** of the State\n",
    "\n",
    ">$$p(s_{t+1}, r_{t+1} | s_{0:t}, a_{0:t}, r_{1:t}) = p(s_{t+1},r_{t+1}|s_t,a_t)$$\n",
    "\n",
    ">* The best policy for choosing actions as a function of a Markov state is just as good as the best policy for choosing actions as a function of complete history\n",
    "\n",
    "* **Markov Decision Process**\n",
    "\n",
    "><img src='images/image02.png' width=200>\n",
    "\n",
    ">* **State-transition Probability**\n",
    "\n",
    ">$$p(s_{t+1}|s_t,a_t) = \\sum_{r \\in \\mathbb{R}} p(s_{t+1},r_{t+1}|s_t,a_t)$$\n",
    "\n",
    ">* **Expected Reward**\n",
    "\n",
    ">$$E[r_{t+1}|s_{t+1},s_t,a_t] = \\frac{\\sum_{r \\in \\mathbb{R}} r p(s_{t+1}, r_{t+1} | s_t,a_t)}{p(s_{t+1}|s_t,a_t)}$$\n",
    "\n",
    "* **Value Functions**\n",
    "\n",
    ">* **State-value** fn. for policy $\\pi$: $V_\\pi (s) = \\mathbb{E}_\\pi [r_t | s_t=s]$\n",
    ">* **Action-value** fn. for policy $\\pi$: $Q_\\pi (s,a) = \\mathbb{E}_\\pi [r_t | s_t=s,a_t=a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Bellman Equation\n",
    "\n",
    "* **Value Function has Recursive Relation**\n",
    "\n",
    ">$$V_{\\pi}(s) = \\sum_a \\pi(a,s) \\sum_{s'} \\sum_r p(s',r|s,a)(r + \\gamma V_{\\pi}(s'))$$\n",
    "\n",
    "* **Optimal Value Functions**\n",
    "\n",
    ">\\begin{align}\n",
    "V_*(s) &= \\underset{\\pi}{\\max} V_\\pi (s) \\\\\n",
    "Q_*(s,a) &= \\underset{\\pi}{\\max} Q_\\pi (s,a) \\\\\n",
    "&= \\mathbb{E} [r_{t+1} + \\gamma V_* (s_{t+1}) | s_t=s,a_t=a]\n",
    "\\end{align}\n",
    "\n",
    "* **Bellman Optimality Equation**\n",
    "\n",
    ">\\begin{align}\n",
    "V_*(s) &= \\underset{a}{\\max} \\sum_{s',r} p(s',r|s,a)[r_{t+1}+\\gamma V_*(s')] \\\\\n",
    "Q_*(s,a) &= \\sum_{s',r} p(s',r|s,a) [r_{t+1} + \\gamma \\underset{a'}{\\max} Q_* (s',a')]\n",
    "\\end{align}\n",
    "\n",
    "* **Optimality and Approximation**\n",
    "\n",
    ">* **Tabular case:** value fn. can be expressed as table values\n",
    ">* **Non-tabular case:** value fn. must be approximated using fn. approximations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
