{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLMI7: Reinforcement Learning and Decision Making\n",
    "\n",
    "Lecturer: Dr. José Miguel Hernández-Lobato & Prof. Carl Edward Rasmussen\n",
    "\n",
    "----\n",
    "\n",
    "# Table of Contents\n",
    ">## 1. Introduction\n",
    "* 1.1. What is Reinforcement Learning?\n",
    "* 1.2. Markov Property\n",
    "* 1.3. Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. What is Reinforcement Learning?\n",
    "\n",
    "* **Agent-Environment Interface**\n",
    "\n",
    "><img src='images/image01.png' width=400>\n",
    "\n",
    ">* **Goal: (1)** For a given state, **(2)** make actions to **(3)** maximise the numerical reward\n",
    "\n",
    "* **Tasks & Rewards**\n",
    "\n",
    ">* **Episodic tasks:** interaction terminates after finite no. of steps\n",
    "\n",
    ">$$R_t = r_{t+1} + \\cdots + r_T$$\n",
    "\n",
    ">* **Continuing tasks:** interaction has no limit (e.g. trading, driving)\n",
    ">  * $\\gamma$: discount factor / $\\gamma=0$: the agent is **miopic** / $\\gamma \\rightarrow 1$: the agent is **farsighted**\n",
    "\n",
    ">$$R_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots$$\n",
    "\n",
    "\n",
    "* **Main Elements in RL**\n",
    "\n",
    ">|Element|Explanation|\n",
    "|-|-|\n",
    "|**Policy**|defines behaviour of the agent, $\\pi_t (a|s) = p(a_t=a | s_t=s)$|\n",
    "|**Reward**|defines the goal of the agent (to maximise $E[$cumulative reward$]$|\n",
    "|**Value Function**|defines what is the goal in the long run (prediction of future reward)|\n",
    "|**Model**|explains how the environment behaves (e.g. state transition prob.)|\n",
    "\n",
    "* **Exploration and exploitation dilemma**\n",
    "\n",
    ">* The agent must simultaneously **exploit** current knowledge and **explore** new actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Markov Property\n",
    "\n",
    "* **Markov Property** of the State\n",
    "\n",
    ">$$p(s_{t+1}, r_{t+1} | s_{0:t}, a_{0:t}, r_{1:t}) = p(s_{t+1},r_{t+1}|s_t,a_t)$$\n",
    "\n",
    ">* The best policy for choosing actions as a function of a Markov state is just as good as the best policy for choosing actions as a function of complete history\n",
    "\n",
    "* **Markov Decision Process**\n",
    "\n",
    "><img src='images/image02.png' width=200>\n",
    "\n",
    ">* **State-transition Probability**\n",
    "\n",
    ">$$p(s_{t+1}|s_t,a_t) = \\sum_{r \\in \\mathbb{R}} p(s_{t+1},r_{t+1}|s_t,a_t)$$\n",
    "\n",
    ">* **Expected Reward**\n",
    "\n",
    ">$$E[r_{t+1}|s_{t+1},s_t,a_t] = \\frac{\\sum_{r \\in \\mathbb{R}} r p(s_{t+1}, r_{t+1} | s_t,a_t)}{p(s_{t+1}|s_t,a_t)}$$\n",
    "\n",
    "* **Value Functions**\n",
    "\n",
    ">* **State-value** fn. for policy $\\pi$: $V_\\pi (s) = \\mathbb{E}_\\pi [r_t | s_t=s]$\n",
    ">* **Action-value** fn. for policy $\\pi$: $Q_\\pi (s,a) = \\mathbb{E}_\\pi [r_t | s_t=s,a_t=a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Bellman Equation\n",
    "\n",
    "* **Value Functions - Recursive Relations**\n",
    "\n",
    ">\\begin{align}\n",
    "V_{\\pi}(s) &= \\mathbb{E}_\\pi [R_t |s_t=s] \\\\\n",
    "&= \\sum_a \\pi(a,s) \\sum_{s'} \\sum_r p(s',r|s,a)(r + \\gamma V_{\\pi}(s')) \\\\\n",
    "\\\\\n",
    "Q_{\\pi}(s) &= \\mathbb{E}_\\pi [R_t |s_t=s,a_t=a] \\\\\n",
    "&= \\bar{r}(s,a) + \\sum_{s'} p(s'|s,a) \\gamma \\sum_{a'} \\pi(s',a') Q^\\pi(s',a')\n",
    "\\end{align}\n",
    "\n",
    "* **Bellman Optimality Equations**\n",
    "\n",
    ">\\begin{align}\n",
    "V_*(s) &= \\underset{\\pi}{\\max} V_\\pi (s) \\\\\n",
    "&= \\underset{a}{\\max} \\sum_{s',r} p(s',r|s,a) \\left[ r_{t+1}+\\gamma V_*(s') \\right] \\\\\n",
    "\\\\\n",
    "Q_*(s,a) &= \\underset{a}{\\max} \\mathbb{E}_{\\pi^*} \\left[ r_{t+1} + \\gamma \\underset{a' \\in \\mathcal{A}}{\\max} Q_* (s_{t+1},a') \\bigg| s_t=s, a_t=a \\right] \\\\\n",
    "&= \\sum_{s',r} p(s',r|s,a) \\left[ r_{t+1} + \\gamma \\underset{a'}{\\max} Q_*(s',a') \\right]\n",
    "\\end{align}\n",
    "\n",
    ">* **Optimal policy:** policy associated with optimal value fn. or optimal Q-fn.\n",
    "\n",
    "* **Optimality and Approximation**\n",
    "\n",
    ">* **Tabular case:** value fn. can be expressed as table values\n",
    ">* **Non-tabular case:** value fn. must be approximated using approximations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dynamic Programming\n",
    "\n",
    "* **DP algorithms** can solve an MDP RL task given the model of the environment (state-transition prob. and reward fn.)\n",
    "\n",
    "* **Limitations**\n",
    "\n",
    ">* Tasks with continuous states and actions $\\rightarrow$ quantize and solve **finite-state MDP**\n",
    ">* Involves operations over the entire set, which can be very large $\\rightarrow$ **asynchronous algorithms**\n",
    ">* **Curse of dimensionality** $\\rightarrow$ use **model-free RL** (next chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Policy Iteration\n",
    "\n",
    "><img src='images/image03.png' width=400>\n",
    "\n",
    ">* **Eq 1: Policy Evaluation**\n",
    ">  * **Stop when:** value fn. stops changing, $|V_{k+1}(s) - V_k(s)| < \\theta$\n",
    "\n",
    ">$$V_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) \\left( r+\\gamma V_k(s') \\right)$$\n",
    "\n",
    ">* **Eq 2: Policy Improvement** (greedy)\n",
    ">  * **Step when:** new policy is not better than the old policy\n",
    "\n",
    ">$$\\pi'(s) = \\underset{a}{\\text{argmax}} \\sum_{s,r} p(s',r|s,a) \\left( r+\\gamma V_\\pi(s') \\right)$$\n",
    "\n",
    ">* The two processes **compete** in the short term and **cooperate** in the long term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Value Iteration\n",
    "\n",
    "><img src='images/image04.png' width=400>\n",
    "\n",
    ">* **Eq 3: Value Iteration** (combines policy evaluation & improvement)\n",
    ">  * **Stop when:** value fn. stops changing, $|V_{k+1}(s) - V_k(s)| < \\theta$\n",
    "\n",
    ">$$V_{k+1}(s) = \\max_a \\sum_{s',r} p(s',r|s,a) \\left( r+\\gamma V_k(s') \\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model-free Reinforcement Learning\n",
    "\n",
    "* Learn from **experience**, which is obtained from **interaction** with **simulated** or **real** environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Monte Carlo Prediction ($V$)\n",
    "\n",
    "><img src='images/image05.png' width=400>\n",
    "\n",
    ">* Average: unbiased estimate, $\\sigma$ falls as $\\frac{1}{\\sqrt{n}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. On-policy Monte Carlo Control ($Q$)\n",
    "\n",
    "><img src='images/image06.png' width=400>\n",
    "\n",
    ">* Evaluate or improve the policy that is used to make decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Off-policy Monte Carlo Control\n",
    "\n",
    "><img src='images/image07.png' width=400>\n",
    "\n",
    ">* **Target policy:** greedy policy wrt $Q$\n",
    ">* **Behaviour policy:** soft policy that generates behaviour\n",
    ">  * **Soft policy:** non-zero prob. for all actions (coverage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
