{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLMI7: Reinforcement Learning and Decision Making\n",
    "\n",
    "Lecturer: Dr. José Miguel Hernández-Lobato & Prof. Carl Edward Rasmussen\n",
    "\n",
    "----\n",
    "\n",
    "# Table of Contents\n",
    ">## 1. Introduction\n",
    "* 1.1. What is Reinforcement Learning?\n",
    "* 1.2. Markov Property\n",
    "* 1.3. Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. What is Reinforcement Learning?\n",
    "\n",
    "* **Agent-Environment Interface**\n",
    "\n",
    "><img src='images/image01.png' width=400>\n",
    "\n",
    ">* **Goal: (1)** For a given state, **(2)** make an action to **(3)** maximise the numerical reward\n",
    "\n",
    "* **Tasks & Rewards**\n",
    "\n",
    ">* **Episodic tasks:** interaction terminates after finite no. of steps\n",
    "\n",
    ">$$R_t = r_{t+1} + \\cdots + r_T$$\n",
    "\n",
    ">* **Continuing tasks:** interaction has no limit (e.g. trading, driving)\n",
    ">  * $\\gamma$: discount factor / $\\gamma=0$: the agent is **miopic** / $\\gamma \\rightarrow 1$: the agent is **farsighted**\n",
    "\n",
    ">$$R_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots$$\n",
    "\n",
    "\n",
    "* **Main Elements in RL**\n",
    "\n",
    ">|Element|Explanation|\n",
    "|-|-|\n",
    "|**Policy**|defines behaviour of the agent, $\\pi_t (a|s) = p(a_t=a | s_t=s)$|\n",
    "|**Reward**|defines the goal of the agent (to maximise $E[$cumulative reward$]$|\n",
    "|**Value Function**|defines what is the goal in the long run (prediction of future reward)|\n",
    "|**Model**|explains how the environment behaves (e.g. state transition prob.)|\n",
    "\n",
    "* **Exploration and exploitation dilemma**\n",
    "\n",
    ">* The agent must simultaneously **exploit** current knowledge and **explore** new actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Markov Property\n",
    "\n",
    "* **Markov Property** of the State\n",
    "\n",
    ">$$p(s_{t+1}, r_{t+1} | s_{0:t}, a_{0:t}, r_{1:t}) = p(s_{t+1},r_{t+1}|s_t,a_t)$$\n",
    "\n",
    ">* The best policy for choosing actions as a function of a Markov state is just as good as the best policy for choosing actions as a function of complete history\n",
    "\n",
    "* **Markov Decision Process**\n",
    "\n",
    "><img src='images/image02.png' width=200>\n",
    "\n",
    ">* **State-transition Probability**\n",
    "\n",
    ">$$p(s'|s,a) = \\sum_{r \\in \\mathbb{R}} p(s',r|s,a)$$\n",
    "\n",
    ">* **Expected Reward**\n",
    "\n",
    ">$$r(s,a,s') = \\sum_{r \\in \\mathbb{R}} p(r|s,a,s') = \\frac{\\sum_{r \\in \\mathbb{R}} r p(s',r|s,a)}{p(s'|s,a)}$$\n",
    "\n",
    "* **Value Functions**\n",
    "\n",
    ">* **State-value** fn. for policy $\\pi$: $V_\\pi (s) = \\mathbb{E}_\\pi [R_t | s_t=s]$\n",
    ">* **Action-value** fn. for policy $\\pi$: $Q_\\pi (s,a) = \\mathbb{E}_\\pi [R_t | s_t=s,a_t=a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Bellman Equation\n",
    "\n",
    "* **Bellman Equation:** recursive relation of the value function\n",
    "\n",
    ">\\begin{align}\n",
    "V_{\\pi}(s) &= \\mathbb{E}_\\pi [R_t |s_t=s] \\\\\n",
    "&= \\sum_a \\pi(a,s) \\sum_{s'} \\sum_r p(s',r|s,a) \\left( r + \\gamma V_{\\pi}(s') \\right) \\\\\n",
    "\\\\\n",
    "Q_{\\pi}(s,a) &= \\mathbb{E}_\\pi [R_t |s_t=s,a_t=a] \\\\\n",
    "&= \\sum_{s'} \\sum_r p(s',r|s,a) \\left( r + \\gamma \\sum_{a'} \\pi(s',a') Q_{\\pi}(s',a') \\right)\n",
    "\\end{align}\n",
    "\n",
    "* **Bellman Optimality Equations**\n",
    "\n",
    ">\\begin{align}\n",
    "V_*(s) &= \\underset{\\pi}{\\max} V_\\pi (s) \\\\\n",
    "&= \\underset{a}{\\max} \\sum_{s',r} p(s',r|s,a) \\left( r +\\gamma V_*(s') \\right) \\\\\n",
    "\\\\\n",
    "Q_*(s,a) &= \\sum_{s',r} p(s',r|s,a) \\left( r + \\gamma \\underset{a'}{\\max} Q_*(s',a') \\right)\n",
    "\\end{align}\n",
    "\n",
    ">* **Optimal policy:** policy associated with optimal $V$ or $Q$ function\n",
    "\n",
    "* **Optimality and Approximation**\n",
    "\n",
    ">* **Tabular case:** value fn. can be expressed as table values\n",
    ">* **Non-tabular case:** value fn. must be approximated using approximations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dynamic Programming\n",
    "\n",
    "* **DP algorithms** can solve an MDP RL task given the model of the environment (state-transition prob. and reward fn.)\n",
    "\n",
    "* **Limitations**\n",
    "\n",
    ">* Tasks with continuous states and actions $\\rightarrow$ quantize and solve **finite-state MDP**\n",
    ">* Involves operations over the entire set, which can be very large $\\rightarrow$ **asynchronous algorithms**\n",
    ">* **Curse of dimensionality** $\\rightarrow$ use **model-free RL** (next chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Policy Iteration\n",
    "\n",
    "><img src='images/image03.png' width=400>\n",
    "\n",
    ">* **Eq 1: Policy Evaluation**\n",
    ">  * **Stop when:** value fn. stops changing, $|V_{k+1}(s) - V_k(s)| < \\theta$\n",
    "\n",
    ">$$V_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) \\left( r+\\gamma V_k(s') \\right)$$\n",
    "\n",
    ">* **Eq 2: Policy Improvement** (greedy)\n",
    ">  * **Step when:** new policy is not better than the old policy\n",
    "\n",
    ">$$\\pi'(s) = \\underset{a}{\\text{argmax}} \\sum_{s,r} p(s',r|s,a) \\left( r+\\gamma V_\\pi(s') \\right)$$\n",
    "\n",
    ">* The two processes **compete** in the short term and **cooperate** in the long term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Value Iteration\n",
    "\n",
    "><img src='images/image04.png' width=400>\n",
    "\n",
    ">* **Eq 3: Value Iteration** (combines policy evaluation & improvement)\n",
    ">  * **Stop when:** value fn. stops changing, $|V_{k+1}(s) - V_k(s)| < \\theta$\n",
    "\n",
    ">$$V_{k+1}(s) = \\max_a \\sum_{s',r} p(s',r|s,a) \\left( r+\\gamma V_k(s') \\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model-free Reinforcement Learning\n",
    "\n",
    "* Learn from **experience**, which is obtained from **interaction** with **simulated** or **real** environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Monte Carlo Prediction ($V$)\n",
    "\n",
    "><img src='images/image05.png' width=400>\n",
    "\n",
    ">* Average: unbiased estimate, $\\sigma$ falls as $\\frac{1}{\\sqrt{n}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. On-policy Monte Carlo Control ($Q$)\n",
    "\n",
    "><img src='images/image06.png' width=400>\n",
    "\n",
    ">* Evaluate or improve the policy that is used to make decisions\n",
    ">* $\\epsilon$-greedy: assign $1 - \\epsilon$ to best action, and distribute $\\epsilon$ to others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Off-policy Monte Carlo Control\n",
    "\n",
    "><img src='images/image07.png' width=400>\n",
    "\n",
    ">* **Target policy:** greedy policy wrt $Q$\n",
    ">* **Behaviour policy:** soft policy that generates behaviour\n",
    ">  * **Soft policy:** non-zero prob. for all actions (coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Temporal Difference Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. TD Prediction\n",
    "\n",
    "* **TD Method:** online average (update using TD error: **backup**)\n",
    "\n",
    ">$$ V(s_t) \\leftarrow V(s_t) + \\alpha \\times \\underbrace{ \\big( r_{t+1} + \\gamma V(s_{t+1}) - V(s_t) \\big)}_{\\rm TD\\;Error}$$\n",
    "\n",
    ">* $\\alpha>0$: step-size parameter\n",
    ">* Only wait until the next time step to update $V$ (unlike MC)\n",
    ">* Update based on existing estimate (bootstrapping, similar to DP)\n",
    "\n",
    "* **TD Error** \n",
    "\n",
    ">$$\\delta_t = r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    ">* **Error in the estimate made at $t$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. SARSA: On-policy TD Control\n",
    "\n",
    "><img src='images/image08.png' width=400>\n",
    "\n",
    ">* Learns tabular Q-function\n",
    "\n",
    ">$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\times \\big( r_{t+1} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t) \\big)$$\n",
    "\n",
    ">* Require quintuple of events - $(s_t,a_t,r_{t+1},s_{t+1},a_{t+1})$\n",
    ">* If $s_{t+1}$ is terminal $\\rightarrow$ $Q(s_{t+1},a_{t+1}) = 0$\n",
    ">* **Converges to an optimal policy** as long as all state-action pairs are visited an infinite no. of times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Q-learning: Off-policy TD Control\n",
    "\n",
    "><img src='images/image09.png' width=400>\n",
    "\n",
    ">$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\times \\big( r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1},a') - Q(s_t,a_t) \\big)$$ \n",
    "\n",
    ">* **Directly approximates the optimal $Q$ fn.**, independent of the policy being followed\n",
    ">* **Dramatically simplifies the analysis of the **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Expected Sarsa\n",
    "\n",
    ">\\begin{align}\n",
    "Q(s_t,a_t) \\leftarrow &Q(s_t,a_t) + \\alpha \\times \\big( \\mathbb{E}[Q(s_{t+1},a_{t+1})|s_{t+1}] - Q(s_t,a_t) \\big) \\\\\n",
    "= &Q(s_t,a_t) + \\alpha \\times \\left( r_{t+1} + \\gamma \\sum_{a'} \\pi (a'|s_{t+1}) Q(s_{t+1},a') - Q(s_t,a_t) \\right)\n",
    "\\end{align}\n",
    "\n",
    ">* Instead of taking random action, use the **expected value** of $Q$\n",
    ">* **Computationally more complex** but **lower variance**\n",
    ">* Generally performs better / can be either on-policy or off-policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Tabular Dyna-Q\n",
    "\n",
    "* **Planning:** based on simulated experience (from model)\n",
    "* **Learning:** based on real experience\n",
    "* **Dyna Architecture:** includes both\n",
    "\n",
    "><img src='images/image10.png' width=450>\n",
    "\n",
    "* **Tabular Dyna-Q**\n",
    "\n",
    "><img src='images/image11.png' width=450>\n",
    "\n",
    ">* **Model-Learning:** table-based / assume deterministic model\n",
    ">* **Planning and Learning:** Q-learning\n",
    ">  * In planning, $(s,a)$ is sampled from the ones that have been experienced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Prioritised Sweeping\n",
    "\n",
    "* **Prioritised Sweeping** (deterministic environment)\n",
    "\n",
    "><img src='images/image12.png' width=450>\n",
    "\n",
    ">* Prioritise the backups based on **TD-error (urgency)**\n",
    ">* If a $(s,a)$ pair is updated, all preceding pairs are also updated\n",
    ">* **Benefit:** can reach optimal solution with **less updates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
